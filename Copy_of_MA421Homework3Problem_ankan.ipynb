{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnzRlaQaW6y1"
   },
   "source": [
    "In this homework, you will write a python implementation of logistic regression. You will test it on two datasets. \n",
    "First we import some libraries that we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Szla9qyoPuqg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0byO4vq0Xcxz"
   },
   "source": [
    "We define some functions involved. Use the formulations that avoid overflows.  \n",
    "1. sigmoid function sigmoid(t)\n",
    "2. log of sigmoid(t), called log_sig(t)\n",
    "3. log of 1-sigmoid = 1/(1+e^t), called log_one_sig(t)\n",
    "4. cross-entropy loss function given the inputs of label y and prediction y_hat = sigmoid(z), where y, y_hat, and z are vectors of dimension N. (N = # of data points.) You should implement this function with z, rather than y_hat, as the input; namely, the loss function should be\n",
    "\n",
    "    loss = -y log(sigmoid(z)) - (1-y) log (1-sigmoid(z)) \n",
    "\n",
    "  where log(sigmoid(z)) and log (1-sigmoid(z)) should be computed by the functions log_sig(z) and log_one_sig(z) in parts 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "kuzmD54GT9yb"
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "  return 1/(1+np.exp(-t))\n",
    "\n",
    "def customloss(y, z): \n",
    "  # loss function for y and yhat = sigmoid(z)\n",
    "  return ((-y*log_sig(z)) - ((1-y)*log_one_sig(z)))\n",
    "\n",
    "def log_sig(t):\n",
    "  return np.log(sigmoid(t))\n",
    "\n",
    "def log_one_sig(t):\n",
    "  return 1./(1+np.exp(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLulJqXcbEpw"
   },
   "source": [
    "Define the model output z=w^T x + b, or z = x^Tw + B, given the data input X (an N-by-n array containing N data points) and the model parameters w (n-dimensional weigth vector) and b (bias).\n",
    "\n",
    "Note that mathematically it's easier to write the data matrix as an n-by-N matrix, with each column being a data point. In python, the data is more commonly represented as as an N-by-n array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "eI9PNMZnhy0d"
   },
   "outputs": [],
   "source": [
    "def model(w,b,X):\n",
    "  # using X as Nxn\n",
    "  print(f'In model, X: {X.shape}, b: {b}, w: {w.shape}')\n",
    "  return (X @ w)+b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv_xi0ajaEEY"
   },
   "source": [
    "Define the function that computes the gradient of the cross-entropy loss given the label y (N-vector), the model prediction y_hat = sigmoid(z) (N-vector), and the dataset X (an n-by-N or N-by-n array). It's probably easier to return the gradients with respect w and b separately, which can be used to update w and b later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "I8KJF8lrZlFi"
   },
   "outputs": [],
   "source": [
    "def gradients(X, y, y_hat):\n",
    "  # Using X as Nxn\n",
    "  # print(f'grad: y shape: {y.shape}, X shape: {X.shape}')\n",
    "  return (np.transpose(X) @ (y_hat - y))/X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgqML9T2cOqd"
   },
   "source": [
    "Write the function that minimizes the loss (i.e. training) by the gradient descent algorithm using a fixed number of iteration (*iter*) and learning rate (*lr*). Your function should take *iter* and *lr* as well as the initial weight w, initial bias b, the input data X and the label y as the inputs. It produces new w and b as output. Also compute the loss value at each iteration and output the sequence of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "6bIdE16li086"
   },
   "outputs": [],
   "source": [
    "def train(w, b, X, y, iter, lr):\n",
    "  print(f'>> {X.shape}')\n",
    "  losslist=list()\n",
    "  for k in range(iter):\n",
    "    z = model(w, b, X)\n",
    "    y_hat = sigmoid(z)\n",
    "    grad = gradients(X, y, y_hat)\n",
    "    print(f'gradient shape: {grad.shape}')\n",
    "    w = w - (lr * grad)\n",
    "    b = np.mean((b*np.ones(y_hat.shape)) - (lr * (y_hat - y)))\n",
    "    myloss = customloss(y, y_hat)\n",
    "    losslist.append(np.mean(myloss))\n",
    "    print(f'Iter: {k} Loss: {losslist[-1]}')\n",
    "  return w, b, losslist\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGC-EjrzeHyU"
   },
   "source": [
    "1. Write the function that uses a trained model to produce class prediction (0 or 1) for an input dataset X, i.e. turn the model output z = model(w,b,X) into predicted label y_label (N-vector of 0 or 1). \n",
    "2. For an input dataset X with a known label y (e.g. a training or testing dataset) and a predicted label y_label, compute the accuracy of prediction (i.e. # correct predictions/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "HChwCsuWf07D"
   },
   "outputs": [],
   "source": [
    "def predict(z):\n",
    "  ypred = sigmoid(z)\n",
    "  ypred[ypred<=0.5]=0\n",
    "  ypred[ypred>0.5]=1\n",
    "  ypred = ypred.astype(int)\n",
    "  ypred = np.squeeze(ypred)\n",
    "  # print(f'In pred, {ypred.shape}')\n",
    "  return ypred\n",
    "\n",
    "def accuracy(y, y_label):\n",
    "  diff_bool = (y == y_label)\n",
    "  diff_true = diff_bool[diff_bool==True]\n",
    "  total_sample = len(diff_bool)\n",
    "  return (len(diff_true)/total_sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icfCmavagcp5"
   },
   "source": [
    "We are ready to test your programs on some datasets. First, we use a synthetic dataset generated using [scikit-learn](https://scikit-learn.org/stable/datasets.html) package. We generate a dataset for training and simultaneously a dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "EXJOlxH2nYw3",
    "outputId": "b23ae813-b042-4735-9ce4-5307d503e662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba09bc9a30>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X_train, y_train = make_moons(n_samples=500, noise=0.1)\n",
    "X_test, y_test = make_moons(n_samples=1000, noise=0.1)\n",
    "\n",
    "print(X_train.shape)\n",
    "plt.figure()\n",
    "plt.plot(X_train[:, 0][y_train==0], X_train[:, 1][y_train==0], \"g^\")\n",
    "plt.plot(X_train[:, 0][y_train==1], X_train[:, 1][y_train==1], \"bs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2_uUM_Zufoz"
   },
   "source": [
    "Here is another toy test example you may try but not part of homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "EJbVzPfLdOvC",
    "outputId": "8044a53a-075d-4011-f61b-86f453165282"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba099fdb20>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X_train, y_train = make_classification(n_samples=1000, n_features=4)\n",
    "X_test=X_train[500:,]\n",
    "y_test=y_train[500:,]\n",
    "X_train=X_train[:500,]\n",
    "y_train=y_train[:500,]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_train[:, 0][y_train==0], X_train[:, 1][y_train==0], \"g^\")\n",
    "plt.plot(X_train[:, 0][y_train==1], X_train[:, 1][y_train==1], \"bs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMm__k_AjOFp"
   },
   "source": [
    "We now train the model using (X_train, y_train). We initialize weight as a random vector, and b=0. We plot the loss convergence history. You should get the loss down to about 0.2.\n",
    "We compute the prediction accuracy on (X_train, y_train). You should get an accuracy in the 80s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mi868jT_mpnq",
    "outputId": "6f86543c-7563-4f43-9302-df162b0e1986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(500, 4)\n",
      "(500, 1)\n",
      ">> (500, 4)\n",
      "In model, X: (500, 4), b: 0, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 0 Loss: 0.09642068969870123\n",
      "In model, X: (500, 4), b: 0.004516336131446229, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 1 Loss: 0.09300772183682958\n",
      "In model, X: (500, 4), b: 0.009058838179995159, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 2 Loss: 0.08958298001921632\n",
      "In model, X: (500, 4), b: 0.013611840871196362, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 3 Loss: 0.08614720644825798\n",
      "In model, X: (500, 4), b: 0.018158285311683082, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 4 Loss: 0.08270336055321845\n",
      "In model, X: (500, 4), b: 0.02268027719930199, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 5 Loss: 0.07925640734336492\n",
      "In model, X: (500, 4), b: 0.02715961336740236, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 6 Loss: 0.07581309362926403\n",
      "In model, X: (500, 4), b: 0.031578272392160336, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 7 Loss: 0.07238169593165003\n",
      "In model, X: (500, 4), b: 0.0359188629813421, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 8 Loss: 0.0689717294258024\n",
      "In model, X: (500, 4), b: 0.04016502268775039, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 9 Loss: 0.06559361506921975\n",
      "In model, X: (500, 4), b: 0.04430175919074736, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 10 Loss: 0.06225831082903621\n",
      "In model, X: (500, 4), b: 0.04831572702676348, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 11 Loss: 0.058976921133572766\n",
      "In model, X: (500, 4), b: 0.05219543425624192, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 12 Loss: 0.055760304850658644\n",
      "In model, X: (500, 4), b: 0.05593137605657901, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 13 Loss: 0.05261870515026428\n",
      "In model, X: (500, 4), b: 0.05951609540408376, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 14 Loss: 0.049561424066552284\n",
      "In model, X: (500, 4), b: 0.06294417448753366, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 15 Loss: 0.04659656067419839\n",
      "In model, X: (500, 4), b: 0.06621216383848143, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 16 Loss: 0.0437308254030543\n",
      "In model, X: (500, 4), b: 0.06931845894071724, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 17 Loss: 0.0409694353928241\n",
      "In model, X: (500, 4), b: 0.07226313596838639, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 18 Loss: 0.0383160882743556\n",
      "In model, X: (500, 4), b: 0.0750477591354927, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 19 Loss: 0.03577300548184975\n",
      "In model, X: (500, 4), b: 0.07767517192983302, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 20 Loss: 0.03334103186537008\n",
      "In model, X: (500, 4), b: 0.08014928341021059, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 21 Loss: 0.031019776225531332\n",
      "In model, X: (500, 4), b: 0.08247485901960586, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 22 Loss: 0.028807777257109562\n",
      "In model, X: (500, 4), b: 0.08465732329468309, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 23 Loss: 0.02670268081305302\n",
      "In model, X: (500, 4), b: 0.08670257970013209, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 24 Loss: 0.024701416814753678\n",
      "In model, X: (500, 4), b: 0.08861685079701659, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 25 Loss: 0.022800366986547402\n",
      "In model, X: (500, 4), b: 0.09040654020971024, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 26 Loss: 0.0209955174367134\n",
      "In model, X: (500, 4), b: 0.09207811645931502, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 27 Loss: 0.019282592641617715\n",
      "In model, X: (500, 4), b: 0.09363801769849038, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 28 Loss: 0.0176571694523018\n",
      "In model, X: (500, 4), b: 0.09509257568831277, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 29 Loss: 0.016114771285425947\n",
      "In model, X: (500, 4), b: 0.096447956952825, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 30 Loss: 0.014650943712530064\n",
      "In model, X: (500, 4), b: 0.09771011887173665, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 31 Loss: 0.013261313296444614\n",
      "In model, X: (500, 4), b: 0.09888477846653691, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 32 Loss: 0.0119416318318867\n",
      "In model, X: (500, 4), b: 0.09997739174642721, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 33 Loss: 0.010687808218411638\n",
      "In model, X: (500, 4), b: 0.10099314166350436, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 34 Loss: 0.00949593010643028\n",
      "In model, X: (500, 4), b: 0.10193693294706362, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 35 Loss: 0.008362277273981129\n",
      "In model, X: (500, 4), b: 0.10281339231965442, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 36 Loss: 0.0072833284605257\n",
      "In model, X: (500, 4), b: 0.10362687282553375, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 37 Loss: 0.006255763137212462\n",
      "In model, X: (500, 4), b: 0.10438146121472906, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 38 Loss: 0.005276459452289622\n",
      "In model, X: (500, 4), b: 0.10508098751715775, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 39 Loss: 0.004342489368330695\n",
      "In model, X: (500, 4), b: 0.10572903610868496, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 40 Loss: 0.0034511118110845834\n",
      "In model, X: (500, 4), b: 0.10632895771446062, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 41 Loss: 0.002599764480251583\n",
      "In model, X: (500, 4), b: 0.10688388191562125, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 42 Loss: 0.0017860548298275134\n",
      "In model, X: (500, 4), b: 0.1073967298255359, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 43 Loss: 0.0010077506078162344\n",
      "In model, X: (500, 4), b: 0.10787022668364693, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 44 Loss: 0.0002627702492941939\n",
      "In model, X: (500, 4), b: 0.1083069141811141, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 45 Loss: -0.00045082666002666373\n",
      "In model, X: (500, 4), b: 0.10870916238531927, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 46 Loss: -0.0011348486934346892\n",
      "In model, X: (500, 4), b: 0.10907918117203326, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 47 Loss: -0.001790981557353941\n",
      "In model, X: (500, 4), b: 0.10941903110663873, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 48 Loss: -0.0024207966580811\n",
      "In model, X: (500, 4), b: 0.10973063374093732, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 49 Loss: -0.0030257592173206314\n",
      "In model, X: (500, 4), b: 0.11001578131118091, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 50 Loss: -0.003607235913867408\n",
      "In model, X: (500, 4), b: 0.11027614583726214, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 51 Loss: -0.004166502044458505\n",
      "In model, X: (500, 4), b: 0.11051328763347315, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 52 Loss: -0.00470474820819825\n",
      "In model, X: (500, 4), b: 0.11072866324871027, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 53 Loss: -0.005223086527018498\n",
      "In model, X: (500, 4), b: 0.11092363285912853, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 54 Loss: -0.005722556420138732\n",
      "In model, X: (500, 4), b: 0.11109946713956774, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 55 Loss: -0.006204129954049163\n",
      "In model, X: (500, 4), b: 0.11125735364200655, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 56 Loss: -0.006668716791636056\n",
      "In model, X: (500, 4), b: 0.1113984027101942, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 57 Loss: -0.00711716876507868\n",
      "In model, X: (500, 4), b: 0.1115236529597269, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 58 Loss: -0.0075502840973627715\n",
      "In model, X: (500, 4), b: 0.11163407635238858, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 59 Loss: -0.007968811296902549\n",
      "In model, X: (500, 4), b: 0.1117305828927272, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 60 Loss: -0.008373452749014739\n",
      "In model, X: (500, 4), b: 0.11181402497371398, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 61 Loss: -0.008764868026973389\n",
      "In model, X: (500, 4), b: 0.1118852013970315, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 62 Loss: -0.009143676944194188\n",
      "In model, X: (500, 4), b: 0.11194486109213136, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 63 Loss: -0.009510462367823545\n",
      "In model, X: (500, 4), b: 0.1119937065567469, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 64 Loss: -0.009865772812694861\n",
      "In model, X: (500, 4), b: 0.11203239704008379, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 65 Loss: -0.010210124833302412\n",
      "In model, X: (500, 4), b: 0.11206155148847091, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 66 Loss: -0.01054400523015804\n",
      "In model, X: (500, 4), b: 0.11208175127185441, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 67 Loss: -0.010867873085657266\n",
      "In model, X: (500, 4), b: 0.11209354270817706, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 68 Loss: -0.011182161643401847\n",
      "In model, X: (500, 4), b: 0.11209743940140957, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 69 Loss: -0.011487280043811745\n",
      "In model, X: (500, 4), b: 0.11209392440779616, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 70 Loss: -0.011783614927815794\n",
      "In model, X: (500, 4), b: 0.11208345224374781, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 71 Loss: -0.012071531919437781\n",
      "In model, X: (500, 4), b: 0.11206645074776049, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 72 Loss: -0.012351376997191743\n",
      "In model, X: (500, 4), b: 0.11204332280775402, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 73 Loss: -0.012623477763367074\n",
      "In model, X: (500, 4), b: 0.11201444796431659, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 74 Loss: -0.012888144619515141\n",
      "In model, X: (500, 4), b: 0.11198018389949604, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 75 Loss: -0.01314567185574275\n",
      "In model, X: (500, 4), b: 0.11194086782000195, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 76 Loss: -0.01339633866077014\n",
      "In model, X: (500, 4), b: 0.1118968177429642, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 77 Loss: -0.013640410059117532\n",
      "In model, X: (500, 4), b: 0.11184833369173401, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 78 Loss: -0.013878137781240966\n",
      "In model, X: (500, 4), b: 0.11179569880860639, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 79 Loss: -0.014109761071942901\n",
      "In model, X: (500, 4), b: 0.11173918039078498, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 80 Loss: -0.014335507441928816\n",
      "In model, X: (500, 4), b: 0.11167903085539901, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 81 Loss: -0.014555593366968673\n",
      "In model, X: (500, 4), b: 0.11161548863891249, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 82 Loss: -0.01477022493874408\n",
      "In model, X: (500, 4), b: 0.11154877903583514, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 83 Loss: -0.01497959847111837\n",
      "In model, X: (500, 4), b: 0.11147911498125032, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 84 Loss: -0.015183901065253042\n",
      "In model, X: (500, 4), b: 0.11140669778131367, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 85 Loss: -0.015383311136707982\n",
      "In model, X: (500, 4), b: 0.11133171779554422, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 86 Loss: -0.015577998907401878\n",
      "In model, X: (500, 4), b: 0.11125435507442707, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 87 Loss: -0.01576812686507181\n",
      "In model, X: (500, 4), b: 0.11117477995556756, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 88 Loss: -0.015953850192653368\n",
      "In model, X: (500, 4), b: 0.11109315362138242, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 89 Loss: -0.016135317169805402\n",
      "In model, X: (500, 4), b: 0.11100962862107952, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 90 Loss: -0.016312669548622274\n",
      "In model, X: (500, 4), b: 0.11092434935946327, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 91 Loss: -0.016486042905411925\n",
      "In model, X: (500, 4), b: 0.1108374525549066, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 92 Loss: -0.016655566970267117\n",
      "In model, X: (500, 4), b: 0.11074906766864975, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 93 Loss: -0.01682136593601996\n",
      "In model, X: (500, 4), b: 0.11065931730742097, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 94 Loss: -0.01698355874804355\n",
      "In model, X: (500, 4), b: 0.110568317601222, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 95 Loss: -0.017142259376249944\n",
      "In model, X: (500, 4), b: 0.11047617855798231, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 96 Loss: -0.01729757707052817\n",
      "In model, X: (500, 4), b: 0.11038300439665724, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 97 Loss: -0.017449616600769536\n",
      "In model, X: (500, 4), b: 0.11028889386022811, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 98 Loss: -0.017598478482539568\n",
      "In model, X: (500, 4), b: 0.11019394050995365, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 99 Loss: -0.017744259189374743\n",
      "In model, X: (500, 4), b: 0.11009823300212275, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 100 Loss: -0.017887051352607986\n",
      "In model, X: (500, 4), b: 0.11000185534846679, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 101 Loss: -0.018026943949559216\n",
      "In model, X: (500, 4), b: 0.10990488716130567, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 102 Loss: -0.018164022480864427\n",
      "In model, X: (500, 4), b: 0.10980740388442349, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 103 Loss: -0.018298369137659483\n",
      "In model, X: (500, 4), b: 0.10970947701059879, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 104 Loss: -0.018430062959282414\n",
      "In model, X: (500, 4), b: 0.10961117428664775, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 105 Loss: -0.018559179982109087\n",
      "In model, X: (500, 4), b: 0.10951255990677823, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 106 Loss: -0.018685793380092866\n",
      "In model, X: (500, 4), b: 0.10941369469499597, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 107 Loss: -0.018809973597537763\n",
      "In model, X: (500, 4), b: 0.10931463627725267, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 108 Loss: -0.018931788474596624\n",
      "In model, X: (500, 4), b: 0.10921543924397756, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 109 Loss: -0.019051303365951313\n",
      "In model, X: (500, 4), b: 0.10911615530358983, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 110 Loss: -0.019168581253099507\n",
      "In model, X: (500, 4), b: 0.10901683342754823, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 111 Loss: -0.019283682850643143\n",
      "In model, X: (500, 4), b: 0.10891751998745638, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 112 Loss: -0.01939666670694634\n",
      "In model, X: (500, 4), b: 0.1088182588847071, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 113 Loss: -0.019507589299504982\n",
      "In model, X: (500, 4), b: 0.1087190916731166, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 114 Loss: -0.019616505125347173\n",
      "In model, X: (500, 4), b: 0.10862005767496932, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 115 Loss: -0.019723466786762026\n",
      "In model, X: (500, 4), b: 0.10852119409086632, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 116 Loss: -0.01982852507263393\n",
      "In model, X: (500, 4), b: 0.10842253610374404, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 117 Loss: -0.01993172903564171\n",
      "In model, X: (500, 4), b: 0.1083241169774065, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 118 Loss: -0.020033126065563914\n",
      "In model, X: (500, 4), b: 0.10822596814989136, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 119 Loss: -0.020132761958916515\n",
      "In model, X: (500, 4), b: 0.10812811932197004, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 120 Loss: -0.02023068098513406\n",
      "In model, X: (500, 4), b: 0.10803059854106231, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 121 Loss: -0.020326925949491485\n",
      "In model, X: (500, 4), b: 0.10793343228082816, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 122 Loss: -0.02042153825295184\n",
      "In model, X: (500, 4), b: 0.1078366455166832, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 123 Loss: -0.020514557949112375\n",
      "In model, X: (500, 4), b: 0.10774026179746801, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 124 Loss: -0.02060602379841143\n",
      "In model, X: (500, 4), b: 0.10764430331348783, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 125 Loss: -0.020695973319747638\n",
      "In model, X: (500, 4), b: 0.10754879096112521, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 126 Loss: -0.020784442839654017\n",
      "In model, X: (500, 4), b: 0.10745374440421594, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 127 Loss: -0.020871467539160155\n",
      "In model, X: (500, 4), b: 0.10735918213236684, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 128 Loss: -0.02095708149846823\n",
      "In model, X: (500, 4), b: 0.10726512151638301, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 129 Loss: -0.02104131773955997\n",
      "In model, X: (500, 4), b: 0.1071715788609622, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 130 Loss: -0.021124208266845478\n",
      "In model, X: (500, 4), b: 0.10707856945480422, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 131 Loss: -0.021205784105957477\n",
      "In model, X: (500, 4), b: 0.10698610761827469, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 132 Loss: -0.021286075340788767\n",
      "In model, X: (500, 4), b: 0.10689420674875387, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 133 Loss: -0.021365111148864604\n",
      "In model, X: (500, 4), b: 0.10680287936379404, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 134 Loss: -0.02144291983513638\n",
      "In model, X: (500, 4), b: 0.10671213714220106, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 135 Loss: -0.021519528864278135\n",
      "In model, X: (500, 4), b: 0.10662199096314942, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 136 Loss: -0.021594964891562124\n",
      "In model, X: (500, 4), b: 0.10653245094343372, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 137 Loss: -0.021669253792386114\n",
      "In model, X: (500, 4), b: 0.10644352647295323, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 138 Loss: -0.021742420690519956\n",
      "In model, X: (500, 4), b: 0.10635522624852117, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 139 Loss: -0.02181448998513587\n",
      "In model, X: (500, 4), b: 0.10626755830608468, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 140 Loss: -0.021885485376682917\n",
      "In model, X: (500, 4), b: 0.10618053005143677, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 141 Loss: -0.021955429891662655\n",
      "In model, X: (500, 4), b: 0.10609414828949693, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 142 Loss: -0.022024345906360064\n",
      "In model, X: (500, 4), b: 0.10600841925223292, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 143 Loss: -0.022092255169580476\n",
      "In model, X: (500, 4), b: 0.10592334862529194, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 144 Loss: -0.0221591788244409\n",
      "In model, X: (500, 4), b: 0.10583894157340583, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 145 Loss: -0.022225137429260834\n",
      "In model, X: (500, 4), b: 0.10575520276463127, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 146 Loss: -0.022290150977596034\n",
      "In model, X: (500, 4), b: 0.10567213639348284, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 147 Loss: -0.022354238917455492\n",
      "In model, X: (500, 4), b: 0.10558974620301334, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 148 Loss: -0.022417420169740417\n",
      "In model, X: (500, 4), b: 0.10550803550589281, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 149 Loss: -0.022479713145941535\n",
      "In model, X: (500, 4), b: 0.10542700720453556, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 150 Loss: -0.022541135765129193\n",
      "In model, X: (500, 4), b: 0.10534666381032082, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 151 Loss: -0.02260170547026909\n",
      "In model, X: (500, 4), b: 0.10526700746195121, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 152 Loss: -0.02266143924389445\n",
      "In model, X: (500, 4), b: 0.10518803994299032, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 153 Loss: -0.02272035362316409\n",
      "In model, X: (500, 4), b: 0.10510976269861842, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 154 Loss: -0.02277846471433427\n",
      "In model, X: (500, 4), b: 0.10503217685164394, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 155 Loss: -0.02283578820667064\n",
      "In model, X: (500, 4), b: 0.10495528321780559, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 156 Loss: -0.02289233938582547\n",
      "In model, X: (500, 4), b: 0.10487908232039878, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 157 Loss: -0.022948133146703818\n",
      "In model, X: (500, 4), b: 0.1048035744042579, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 158 Loss: -0.023003184005841375\n",
      "In model, X: (500, 4), b: 0.10472875944912467, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 159 Loss: -0.02305750611331536\n",
      "In model, X: (500, 4), b: 0.10465463718243094, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 160 Loss: -0.02311111326420893\n",
      "In model, X: (500, 4), b: 0.10458120709152331, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 161 Loss: -0.02316401890964838\n",
      "In model, X: (500, 4), b: 0.10450846843535497, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 162 Loss: -0.023216236167431698\n",
      "In model, X: (500, 4), b: 0.10443642025566945, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 163 Loss: -0.02326777783226586\n",
      "In model, X: (500, 4), b: 0.10436506138769937, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 164 Loss: -0.023318656385629712\n",
      "In model, X: (500, 4), b: 0.10429439047040232, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 165 Loss: -0.023368884005278166\n",
      "In model, X: (500, 4), b: 0.10422440595625479, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 166 Loss: -0.02341847257440282\n",
      "In model, X: (500, 4), b: 0.10415510612062402, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 167 Loss: -0.02346743369046351\n",
      "In model, X: (500, 4), b: 0.10408648907073706, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 168 Loss: -0.023515778673704378\n",
      "In model, X: (500, 4), b: 0.10401855275426444, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 169 Loss: -0.023563518575367565\n",
      "In model, X: (500, 4), b: 0.10395129496753638, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 170 Loss: -0.023610664185616907\n",
      "In model, X: (500, 4), b: 0.10388471336340724, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 171 Loss: -0.02365722604118368\n",
      "In model, X: (500, 4), b: 0.10381880545878411, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 172 Loss: -0.023703214432745333\n",
      "In model, X: (500, 4), b: 0.10375356864183419, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 173 Loss: -0.02374863941204849\n",
      "In model, X: (500, 4), b: 0.10368900017888512, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 174 Loss: -0.023793510798786034\n",
      "In model, X: (500, 4), b: 0.10362509722103162, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 175 Loss: -0.023837838187238516\n",
      "In model, X: (500, 4), b: 0.10356185681046134, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 176 Loss: -0.023881630952688887\n",
      "In model, X: (500, 4), b: 0.10349927588651199, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 177 Loss: -0.023924898257619794\n",
      "In model, X: (500, 4), b: 0.10343735129147145, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 178 Loss: -0.023967649057701858\n",
      "In model, X: (500, 4), b: 0.103376079776132, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 179 Loss: -0.02400989210758112\n",
      "In model, X: (500, 4), b: 0.10331545800510897, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 180 Loss: -0.02405163596647348\n",
      "In model, X: (500, 4), b: 0.10325548256193424, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 181 Loss: -0.024092889003573627\n",
      "In model, X: (500, 4), b: 0.10319614995393404, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 182 Loss: -0.024133659403285462\n",
      "In model, X: (500, 4), b: 0.10313745661690006, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 183 Loss: -0.024173955170280995\n",
      "In model, X: (500, 4), b: 0.10307939891956297, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 184 Loss: -0.024213784134394192\n",
      "In model, X: (500, 4), b: 0.10302197316787647, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 185 Loss: -0.024253153955355957\n",
      "In model, X: (500, 4), b: 0.1029651756091198, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 186 Loss: -0.02429207212737628\n",
      "In model, X: (500, 4), b: 0.10290900243582667, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 187 Loss: -0.024330545983579286\n",
      "In model, X: (500, 4), b: 0.10285344978954741, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 188 Loss: -0.02436858270029664\n",
      "In model, X: (500, 4), b: 0.10279851376445176, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 189 Loss: -0.024406189301224516\n",
      "In model, X: (500, 4), b: 0.10274419041077858, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 190 Loss: -0.02444337266144924\n",
      "In model, X: (500, 4), b: 0.10269047573813903, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 191 Loss: -0.024480139511346366\n",
      "In model, X: (500, 4), b: 0.1026373657186793, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 192 Loss: -0.02451649644035775\n",
      "In model, X: (500, 4), b: 0.10258485629010855, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 193 Loss: -0.024552449900651238\n",
      "In model, X: (500, 4), b: 0.10253294335859772, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 194 Loss: -0.024588006210666938\n",
      "In model, X: (500, 4), b: 0.10248162280155453, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 195 Loss: -0.02462317155855436\n",
      "In model, X: (500, 4), b: 0.10243089047027967, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 196 Loss: -0.0246579520055042\n",
      "In model, X: (500, 4), b: 0.10238074219250914, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 197 Loss: -0.02469235348897866\n",
      "In model, X: (500, 4), b: 0.1023311737748472, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 198 Loss: -0.024726381825843607\n",
      "In model, X: (500, 4), b: 0.10228218100509462, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 199 Loss: -0.02476004271540639\n",
      "In model, X: (500, 4), b: 0.1022337596544763, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 200 Loss: -0.02479334174236236\n",
      "In model, X: (500, 4), b: 0.1021859054797725, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 201 Loss: -0.02482628437965329\n",
      "In model, X: (500, 4), b: 0.10213861422535735, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 202 Loss: -0.024858875991240963\n",
      "In model, X: (500, 4), b: 0.10209188162514857, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 203 Loss: -0.024891121834798538\n",
      "In model, X: (500, 4), b: 0.10204570340447203, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 204 Loss: -0.024923027064322756\n",
      "In model, X: (500, 4), b: 0.10200007528184427, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 205 Loss: -0.024954596732669608\n",
      "In model, X: (500, 4), b: 0.10195499297067663, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 206 Loss: -0.024985835794016097\n",
      "In model, X: (500, 4), b: 0.10191045218090389, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 207 Loss: -0.025016749106250465\n",
      "In model, X: (500, 4), b: 0.10186644862054058, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 208 Loss: -0.025047341433293523\n",
      "In model, X: (500, 4), b: 0.10182297799716773, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 209 Loss: -0.0250776174473532\n",
      "In model, X: (500, 4), b: 0.10178003601935286, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 210 Loss: -0.02510758173111459\n",
      "In model, X: (500, 4), b: 0.10173761839800605, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 211 Loss: -0.025137238779867684\n",
      "In model, X: (500, 4), b: 0.10169572084767432, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 212 Loss: -0.025166593003574794\n",
      "In model, X: (500, 4), b: 0.10165433908777703, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 213 Loss: -0.025195648728879704\n",
      "In model, X: (500, 4), b: 0.10161346884378449, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 214 Loss: -0.025224410201060365\n",
      "In model, X: (500, 4), b: 0.1015731058483421, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 215 Loss: -0.02525288158592707\n",
      "In model, X: (500, 4), b: 0.10153324584234213, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 216 Loss: -0.025281066971667838\n",
      "In model, X: (500, 4), b: 0.10149388457594531, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 217 Loss: -0.025308970370642628\n",
      "In model, X: (500, 4), b: 0.10145501780955403, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 218 Loss: -0.025336595721128306\n",
      "In model, X: (500, 4), b: 0.10141664131473925, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 219 Loss: -0.02536394688901548\n",
      "In model, X: (500, 4), b: 0.10137875087512285, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 220 Loss: -0.02539102766945922\n",
      "In model, X: (500, 4), b: 0.10134134228721714, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 221 Loss: -0.02541784178848473\n",
      "In model, X: (500, 4), b: 0.1013044113612233, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 222 Loss: -0.025444392904549706\n",
      "In model, X: (500, 4), b: 0.10126795392179037, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 223 Loss: -0.02547068461006451\n",
      "In model, X: (500, 4), b: 0.10123196580873627, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 224 Loss: -0.02549672043287159\n",
      "In model, X: (500, 4), b: 0.1011964428777323, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 225 Loss: -0.025522503837685402\n",
      "In model, X: (500, 4), b: 0.1011613810009528, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 226 Loss: -0.025548038227494026\n",
      "In model, X: (500, 4), b: 0.10112677606769105, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 227 Loss: -0.02557332694492373\n",
      "In model, X: (500, 4), b: 0.10109262398494295, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 228 Loss: -0.02559837327356747\n",
      "In model, X: (500, 4), b: 0.10105892067795953, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 229 Loss: -0.025623180439278706\n",
      "In model, X: (500, 4), b: 0.10102566209076982, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 230 Loss: -0.025647751611431135\n",
      "In model, X: (500, 4), b: 0.10099284418667492, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 231 Loss: -0.025672089904146007\n",
      "In model, X: (500, 4), b: 0.10096046294871466, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 232 Loss: -0.025696198377487305\n",
      "In model, X: (500, 4), b: 0.1009285143801078, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 233 Loss: -0.02572008003862635\n",
      "In model, X: (500, 4), b: 0.10089699450466676, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 234 Loss: -0.025743737842976372\n",
      "In model, X: (500, 4), b: 0.10086589936718812, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 235 Loss: -0.025767174695298066\n",
      "In model, X: (500, 4), b: 0.10083522503381957, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 236 Loss: -0.025790393450777077\n",
      "In model, X: (500, 4), b: 0.10080496759240436, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 237 Loss: -0.02581339691607403\n",
      "In model, X: (500, 4), b: 0.10077512315280421, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 238 Loss: -0.02583618785034817\n",
      "In model, X: (500, 4), b: 0.10074568784720145, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 239 Loss: -0.025858768966255184\n",
      "In model, X: (500, 4), b: 0.10071665783038118, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 240 Loss: -0.025881142930920063\n",
      "In model, X: (500, 4), b: 0.1006880292799943, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 241 Loss: -0.02590331236688566\n",
      "In model, X: (500, 4), b: 0.10065979839680222, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 242 Loss: -0.02592527985303774\n",
      "In model, X: (500, 4), b: 0.10063196140490388, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 243 Loss: -0.025947047925507127\n",
      "In model, X: (500, 4), b: 0.10060451455194576, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 244 Loss: -0.025968619078549582\n",
      "In model, X: (500, 4), b: 0.10057745410931578, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 245 Loss: -0.02598999576540415\n",
      "In model, X: (500, 4), b: 0.10055077637232147, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 246 Loss: -0.026011180399130536\n",
      "In model, X: (500, 4), b: 0.1005244776603532, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 247 Loss: -0.02603217535342602\n",
      "In model, X: (500, 4), b: 0.10049855431703299, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 248 Loss: -0.02605298296342264\n",
      "In model, X: (500, 4), b: 0.10047300271034953, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 249 Loss: -0.026073605526465128\n",
      "In model, X: (500, 4), b: 0.10044781923278007, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 250 Loss: -0.0260940453028701\n",
      "In model, X: (500, 4), b: 0.10042300030139936, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 251 Loss: -0.02611430451666705\n",
      "In model, X: (500, 4), b: 0.10039854235797666, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 252 Loss: -0.02613438535632172\n",
      "In model, X: (500, 4), b: 0.10037444186906093, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 253 Loss: -0.026154289975442255\n",
      "In model, X: (500, 4), b: 0.10035069532605484, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 254 Loss: -0.026174020493468605\n",
      "In model, X: (500, 4), b: 0.10032729924527799, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 255 Loss: -0.02619357899634568\n",
      "In model, X: (500, 4), b: 0.10030425016801987, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 256 Loss: -0.026212967537180776\n",
      "In model, X: (500, 4), b: 0.10028154466058302, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 257 Loss: -0.02623218813688549\n",
      "In model, X: (500, 4), b: 0.10025917931431652, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 258 Loss: -0.026251242784802738\n",
      "In model, X: (500, 4), b: 0.10023715074564063, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 259 Loss: -0.026270133439319256\n",
      "In model, X: (500, 4), b: 0.10021545559606262, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 260 Loss: -0.02628886202846381\n",
      "In model, X: (500, 4), b: 0.10019409053218432, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 261 Loss: -0.026307430450491765\n",
      "In model, X: (500, 4), b: 0.10017305224570168, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 262 Loss: -0.026325840574456128\n",
      "In model, X: (500, 4), b: 0.10015233745339674, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 263 Loss: -0.02634409424076554\n",
      "In model, X: (500, 4), b: 0.10013194289712234, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 264 Loss: -0.0263621932617297\n",
      "In model, X: (500, 4), b: 0.10011186534377971, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 265 Loss: -0.026380139422092108\n",
      "In model, X: (500, 4), b: 0.10009210158528954, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 266 Loss: -0.026397934479551112\n",
      "In model, X: (500, 4), b: 0.10007264843855657, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 267 Loss: -0.026415580165268935\n",
      "In model, X: (500, 4), b: 0.10005350274542814, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 268 Loss: -0.02643307818436944\n",
      "In model, X: (500, 4), b: 0.10003466137264684, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 269 Loss: -0.026450430216424712\n",
      "In model, X: (500, 4), b: 0.10001612121179766, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 270 Loss: -0.02646763791593074\n",
      "In model, X: (500, 4), b: 0.09999787917924982, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 271 Loss: -0.026484702912772686\n",
      "In model, X: (500, 4), b: 0.09997993221609343, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 272 Loss: -0.026501626812679653\n",
      "In model, X: (500, 4), b: 0.09996227728807143, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 273 Loss: -0.02651841119766968\n",
      "In model, X: (500, 4), b: 0.09994491138550689, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 274 Loss: -0.026535057626484664\n",
      "In model, X: (500, 4), b: 0.09992783152322579, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 275 Loss: -0.02655156763501613\n",
      "In model, X: (500, 4), b: 0.0999110347404758, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 276 Loss: -0.02656794273672135\n",
      "In model, X: (500, 4), b: 0.0998945181008409, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 277 Loss: -0.026584184423030698\n",
      "In model, X: (500, 4), b: 0.09987827869215235, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 278 Loss: -0.026600294163746006\n",
      "In model, X: (500, 4), b: 0.09986231362639589, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 279 Loss: -0.026616273407430464\n",
      "In model, X: (500, 4), b: 0.09984662003961563, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 280 Loss: -0.02663212358179004\n",
      "In model, X: (500, 4), b: 0.09983119509181466, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 281 Loss: -0.026647846094046808\n",
      "In model, X: (500, 4), b: 0.09981603596685255, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 282 Loss: -0.026663442331304188\n",
      "In model, X: (500, 4), b: 0.09980113987233984, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 283 Loss: -0.026678913660904583\n",
      "In model, X: (500, 4), b: 0.09978650403952989, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 284 Loss: -0.02669426143077934\n",
      "In model, X: (500, 4), b: 0.09977212572320804, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 285 Loss: -0.026709486969791207\n",
      "In model, X: (500, 4), b: 0.09975800220157822, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 286 Loss: -0.02672459158806992\n",
      "In model, X: (500, 4), b: 0.09974413077614731, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 287 Loss: -0.026739576577340336\n",
      "In model, X: (500, 4), b: 0.09973050877160722, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 288 Loss: -0.026754443211244025\n",
      "In model, X: (500, 4), b: 0.09971713353571485, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 289 Loss: -0.02676919274565396\n",
      "In model, X: (500, 4), b: 0.0997040024391702, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 290 Loss: -0.026783826418982866\n",
      "In model, X: (500, 4), b: 0.09969111287549248, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 291 Loss: -0.026798345452484967\n",
      "In model, X: (500, 4), b: 0.09967846226089457, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 292 Loss: -0.02681275105055168\n",
      "In model, X: (500, 4), b: 0.09966604803415584, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 293 Loss: -0.02682704440100117\n",
      "In model, X: (500, 4), b: 0.09965386765649345, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 294 Loss: -0.026841226675361988\n",
      "In model, X: (500, 4), b: 0.09964191861143219, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 295 Loss: -0.026855299029150886\n",
      "In model, X: (500, 4), b: 0.09963019840467302, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 296 Loss: -0.02686926260214503\n",
      "In model, X: (500, 4), b: 0.09961870456396042, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 297 Loss: -0.026883118518648646\n",
      "In model, X: (500, 4), b: 0.09960743463894846, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 298 Loss: -0.026896867887754302\n",
      "In model, X: (500, 4), b: 0.09959638620106602, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 299 Loss: -0.02691051180359897\n",
      "In model, X: (500, 4), b: 0.09958555684338088, w: (4, 1)\n",
      "0.89\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.rand(X_train.shape[1],1)  # assuming X is N-by-n. \n",
    "                                        # if X is n-by-N, use X_train.shape[0]\n",
    "y_train = y_train.reshape((-1,1))\n",
    "y_test = y_test.reshape((-1,1))\n",
    "print(w.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "b = 0\n",
    "w, b, loss = train(w, b, X_train, y_train, iter=300, lr=0.1)\n",
    "plt.figure()\n",
    "plt.plot(loss)\n",
    "\n",
    "#training accuracy \n",
    "z = model(w,b,X_train)\n",
    "print(accuracy(np.squeeze(y_train), predict(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBm8ESACmrxe"
   },
   "source": [
    "To see how well our model performs, we compute its accuracy on the testing dataset (X_test, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pt9_Aiw-zqP6",
    "outputId": "b316bd15-9f8e-4380-fea3-05df6dcc77d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In model, X: (500, 4), b: 0.09958555684338088, w: (4, 1)\n",
      "[1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0\n",
      " 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1\n",
      " 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0\n",
      " 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1\n",
      " 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1\n",
      " 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1\n",
      " 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0\n",
      " 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1]\n",
      "[1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1\n",
      " 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0\n",
      " 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0\n",
      " 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1\n",
      " 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1\n",
      " 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0\n",
      " 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1\n",
      " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1\n",
      " 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1\n",
      " 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0\n",
      " 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1]\n",
      "0.888\n"
     ]
    }
   ],
   "source": [
    "z = model(w,b,X_test)\n",
    "y_test=np.squeeze(y_test)\n",
    "print(y_test)\n",
    "print(predict(z))\n",
    "print(accuracy(y_test, predict(z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "xLiBWzdBDCKM"
   },
   "outputs": [],
   "source": [
    "mycheck = (y_test==np.transpose(predict(z))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bKdIsqUDJlF",
    "outputId": "4c3e44fb-897b-40a7-9ba9-5aabc9d54b2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mycheck[mycheck==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQCklqW1DJiO",
    "outputId": "ffa45e03-6a6e-41b6-b9c0-f9f99a3ec01b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mycheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32lhiZG6DJfm",
    "outputId": "8221953b-f0ba-434d-e3fc-42a109ad0d52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.966"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "483/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "u-YnkECyDJXw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef5x5LENm7_s"
   },
   "source": [
    "Now, we look at a real-world dataset. [The Bank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#) is available at UCI's Machine Learning Repository. Colab can read this dataset directly from [GitHub](https://github.com/madmashup/targeted-marketing-predictive-engine) using pandas package: pd.read_csv. The data is in the DataFrame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5vKPwXfYgLV",
    "outputId": "48f6d747-ef7a-4ce9-f9b1-1e3b11bc1fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41188, 21)\n",
      "['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/madmashup/targeted-marketing-predictive-engine/master/banking.csv'\n",
    "data = pd.read_csv(url)\n",
    "print(data.shape)\n",
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG9UWfJ8n2Jr"
   },
   "source": [
    "This dataset is pretty large and cause my machine to crash. I remove some fileds. [This Webpage](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8) has a good description of this dataset. Note that you are not allowed to use any existing model such as those used in that Webpage for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGiNyyIsvUw-",
    "outputId": "a10b0f82-9654-4e3f-c7d1-10311232b6d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'job', 'marital', 'housing', 'loan', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']\n",
      "(41188, 16)\n"
     ]
    }
   ],
   "source": [
    "cat_vars=['default','education','contact','month','day_of_week',]\n",
    "data=data.drop(cat_vars, axis=1)\n",
    "print(list(data.columns))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVSJRTDeoHNj"
   },
   "source": [
    "Some data columns have k class labels. This is best represented as k columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "du0e-Dhyg2FV",
    "outputId": "d1bef8ca-4b24-4406-850c-5bed6d4b9e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job\n",
      "marital\n",
      "housing\n",
      "loan\n",
      "poutcome\n",
      "(41188, 36)\n",
      "['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y', 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown', 'divorced', 'married', 'single', 'unknown', 'no', 'unknown', 'yes', 'no', 'unknown', 'yes', 'failure', 'nonexistent', 'success']\n"
     ]
    }
   ],
   "source": [
    "cat_vars=['job','marital','housing','loan','poutcome']\n",
    "for va in cat_vars:\n",
    "    #cat_pre='var'+'_'+var\n",
    "    print(va)\n",
    "    #print(data[va])\n",
    "    cat_list = pd.get_dummies(data[va])\n",
    "    data1=pd.concat([data,cat_list], axis=1)\n",
    "    data=data1.drop(va, axis=1)\n",
    "    #print(list(cat_list.columns))\n",
    "    #print(list(data.columns))\n",
    "    #print(data.shape)\n",
    "\n",
    "print(data.shape)\n",
    "print(list(data.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NazsRFZmpIuD"
   },
   "source": [
    "We now split the data into input data X and the label y. We covert them to numpy and split them into training and testing datasets with 30% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2kDuXGHtBdB",
    "outputId": "4bcc89f5-aaf9-4f97-b846-251fc67bdd45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 35)\n",
      "(12357, 35)\n",
      "Index(['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate',\n",
      "       'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'admin.',\n",
      "       'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired',\n",
      "       'self-employed', 'services', 'student', 'technician', 'unemployed',\n",
      "       'unknown', 'divorced', 'married', 'single', 'unknown', 'no', 'unknown',\n",
      "       'yes', 'no', 'unknown', 'yes', 'failure', 'nonexistent', 'success'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = data.loc[:, data.columns != 'y']\n",
    "y = data.loc[:, data.columns == 'y']\n",
    "columns = X.columns\n",
    "X=X.to_numpy()\n",
    "y=y.to_numpy()\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train1.shape)\n",
    "print(X_test1.shape)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngDOmRz9pxyR"
   },
   "source": [
    "Now, train and test as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tHoIcRBXN_bG",
    "outputId": "3f2458ee-42ca-48ff-8b8e-e7024181d9a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1)\n",
      "(28831, 35)\n",
      "(28831, 1)\n",
      ">> (28831, 35)\n",
      "In model, X: (28831, 35), b: 0, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 0 Loss: -0.20302934938459252\n",
      "In model, X: (28831, 35), b: -0.08867885262391177, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 1 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.07735770524782351, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 2 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.06603655787173526, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 3 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.054715410495647016, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 4 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.04339426311955877, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 5 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.03207311574347053, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 6 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.02075196836738229, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 7 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.009430820991294052, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 8 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: 0.0018903263847941866, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 9 Loss: -0.20588241844216149\n",
      "In model, X: (28831, 35), b: -0.08555283801752073, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 10 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.07423169064143251, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 11 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.06291054326534427, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 12 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.05158939588925605, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 13 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.040268248513167806, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 14 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.02894710113707956, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 15 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.017625953760991315, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 16 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.006304806384903077, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 17 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: 0.005016340991185162, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 18 Loss: -0.20302934938459252\n",
      "In model, X: (28831, 35), b: -0.08366251163272663, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 19 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.0723413642566384, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 20 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.06102021688055017, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 21 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.049699069504461915, w: (35, 1)\n",
      "gradient shape: (35, 1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-84be012d15e9>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iter: 22 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.03837792212837367, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 23 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.027056774752285424, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 24 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.015735627376197183, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 25 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.004414480000108946, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 26 Loss: -0.36492150692790093\n",
      "In model, X: (28831, 35), b: 0.0068789194657437845, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 27 Loss: -0.20302934938459252\n",
      "In model, X: (28831, 35), b: -0.08179993315816796, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 28 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.07047878578207971, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 29 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.059157638405991454, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 30 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.04783649102990322, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 31 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.03651534365381497, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 32 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.02519419627772674, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 33 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.013873048901638495, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 34 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.002551901525550257, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 35 Loss: -0.3650475653928166\n",
      "In model, X: (28831, 35), b: 0.008686002119831454, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 36 Loss: -0.20302934938459252\n",
      "In model, X: (28831, 35), b: -0.07999285050408031, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 37 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.06867170312799208, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 38 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.05735055575190385, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 39 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.04602940837581561, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 40 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.03470826099972736, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 41 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.023387113623639117, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 42 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.012065966247550877, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 43 Loss: -0.36491403503429326\n",
      "In model, X: (28831, 35), b: -0.0007482873602420781, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 44 Loss: -0.36517770120802734\n",
      "In model, X: (28831, 35), b: 0.010371687666638714, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 45 Loss: -0.20302934938459252\n",
      "In model, X: (28831, 35), b: -0.07830716495727306, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 46 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.06698601758118483, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 47 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.0556648702050966, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 48 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.04434372282900836, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 49 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.03302257545292011, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 50 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.02170142807683187, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 51 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.010380280700743628, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 52 Loss: -0.36494554965052217\n",
      "In model, X: (28831, 35), b: 0.0009235242314474191, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 53 Loss: -0.36534967572912147\n",
      "In model, X: (28831, 35), b: 0.011897822729591766, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 54 Loss: -0.2030373636254662\n",
      "In model, X: (28831, 35), b: -0.07677756140554055, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 55 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.06545641402945232, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 56 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.054135266653364086, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 57 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.04281411927727585, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 58 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.03149297190118762, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 59 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.020171824525099375, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 60 Loss: -0.36491403503429326\n",
      "In model, X: (28831, 35), b: -0.008854145637790573, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 61 Loss: -0.36493984525842904\n",
      "In model, X: (28831, 35), b: 0.0024288483617238375, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 62 Loss: -0.36589805814344745\n",
      "In model, X: (28831, 35), b: 0.01313607322385141, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 63 Loss: -0.24117204368496178\n",
      "In model, X: (28831, 35), b: -0.05884216672449522, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 64 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.04752101934840699, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 65 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.036199871972318755, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 66 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.02487872459623052, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 67 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.013557577220142286, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 68 Loss: -0.36493237336482126\n",
      "In model, X: (28831, 35), b: -0.0022503037991717994, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 69 Loss: -0.3650390088046769\n",
      "In model, X: (28831, 35), b: 0.008956383447194962, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 70 Loss: -0.3745350489443406\n",
      "In model, X: (28831, 35), b: 0.01352438316440763, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 71 Loss: -0.3281654602726167\n",
      "In model, X: (28831, 35), b: -0.01684223517168023, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 72 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: -0.00553149326193031, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 73 Loss: -0.3649896982051859\n",
      "In model, X: (28831, 35), b: 0.005727221316128032, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 74 Loss: -0.3673255957285867\n",
      "In model, X: (28831, 35), b: 0.015612414337528609, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 75 Loss: -0.22125075922009318\n",
      "In model, X: (28831, 35), b: -0.06517079968380081, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 76 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.053849652307712575, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 77 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.04252850493162432, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 78 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.031207357555536074, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 79 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.019886210179447833, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 80 Loss: -0.36491403503429326\n",
      "In model, X: (28831, 35), b: -0.008568531292139032, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 81 Loss: -0.3649318310175553\n",
      "In model, X: (28831, 35), b: 0.002710994218595944, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 82 Loss: -0.36539952867587827\n",
      "In model, X: (28831, 35), b: 0.01366101329528423, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 83 Loss: -0.3700469840065887\n",
      "In model, X: (28831, 35), b: 0.007473229312765413, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 84 Loss: -0.3679114780706682\n",
      "In model, X: (28831, 35), b: 0.016841617506029607, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 85 Loss: -0.2770004128961331\n",
      "In model, X: (28831, 35), b: -0.03876853492229157, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 86 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.027447387546203336, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 87 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.016126240170115103, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 88 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: -0.0048154982603651805, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 89 Loss: -0.3649948602500131\n",
      "In model, X: (28831, 35), b: 0.006436279340134283, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 90 Loss: -0.36664707749904984\n",
      "In model, X: (28831, 35), b: 0.016706474616052566, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 91 Loss: -0.3416623500775442\n",
      "In model, X: (28831, 35), b: -0.00655320870815388, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 92 Loss: -0.3649238167766816\n",
      "In model, X: (28831, 35), b: 0.004722848313801657, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 93 Loss: -0.3657645277849241\n",
      "In model, X: (28831, 35), b: 0.01550984841785632, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 94 Loss: -0.3691902305339746\n",
      "In model, X: (28831, 35), b: 0.00872199469010472, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 95 Loss: -0.36779737073720487\n",
      "In model, X: (28831, 35), b: 0.01821524847942871, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 96 Loss: -0.3089593263427349\n",
      "In model, X: (28831, 35), b: -0.02193086428305777, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 97 Loss: -0.36491403503429326\n",
      "In model, X: (28831, 35), b: -0.010613185395748967, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 98 Loss: -0.36492952116877475\n",
      "In model, X: (28831, 35), b: 0.0006836825588831996, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 99 Loss: -0.3650625091800321\n",
      "In model, X: (28831, 35), b: 0.011873027361352763, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 100 Loss: -0.3717884781654057\n",
      "In model, X: (28831, 35), b: 0.01889324865093689, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 101 Loss: -0.3176904674807299\n",
      "In model, X: (28831, 35), b: -0.016942254622756347, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 102 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: -0.005631512713006425, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 103 Loss: -0.3649238167766816\n",
      "In model, X: (28831, 35), b: 0.005644544308949109, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 104 Loss: -0.3656126590958727\n",
      "In model, X: (28831, 35), b: 0.016521725121269183, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 105 Loss: -0.37320835237611744\n",
      "In model, X: (28831, 35), b: 0.012976929597652388, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 106 Loss: -0.3728624222452931\n",
      "In model, X: (28831, 35), b: 0.019379761066925568, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 107 Loss: -0.3327695713112807\n",
      "In model, X: (28831, 35), b: -0.008553923713866514, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 108 Loss: -0.3649134926870273\n",
      "In model, X: (28831, 35), b: 0.002736007263206776, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 109 Loss: -0.36519141984099424\n",
      "In model, X: (28831, 35), b: 0.013880261711543636, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 110 Loss: -0.37407506508959365\n",
      "In model, X: (28831, 35), b: 0.019578988776105963, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 111 Loss: -0.34432088537304556\n",
      "In model, X: (28831, 35), b: -0.0020990655941462402, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 112 Loss: -0.3649948602500131\n",
      "In model, X: (28831, 35), b: 0.00915271200635322, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 113 Loss: -0.36640582700613894\n",
      "In model, X: (28831, 35), b: 0.019547772878331298, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 114 Loss: -0.35500639976558396\n",
      "In model, X: (28831, 35), b: 0.0038424556850324178, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 115 Loss: -0.36520459612669504\n",
      "In model, X: (28831, 35), b: 0.01498324164458984, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 116 Loss: -0.3747528741246254\n",
      "In model, X: (28831, 35), b: 0.020068584739209278, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 117 Loss: -0.35221572477692337\n",
      "In model, X: (28831, 35), b: 0.0026881750673027147, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 118 Loss: -0.36503900880467693\n",
      "In model, X: (28831, 35), b: 0.013894862313669475, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 119 Loss: -0.3707797295266476\n",
      "In model, X: (28831, 35), b: 0.021615718336700243, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 120 Loss: -0.31576466490147365\n",
      "In model, X: (28831, 35), b: -0.01516325114430559, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 121 Loss: -0.3649060207934195\n",
      "In model, X: (28831, 35), b: -0.0038490407457762253, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 122 Loss: -0.3649318310175553\n",
      "In model, X: (28831, 35), b: 0.007430484764958748, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 123 Loss: -0.36535768996999507\n",
      "In model, X: (28831, 35), b: 0.018408251751882546, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 124 Loss: -0.37514392425292103\n",
      "In model, X: (28831, 35), b: 0.017672932130772596, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 125 Loss: -0.37554591442312774\n",
      "In model, X: (28831, 35), b: 0.019074198751027577, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 126 Loss: -0.3741381119343873\n",
      "In model, X: (28831, 35), b: 0.01656470255933973, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 127 Loss: -0.3755292553885296\n",
      "In model, X: (28831, 35), b: 0.02100360969671111, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 128 Loss: -0.35886285169569687\n",
      "In model, X: (28831, 35), b: 0.007490626714704012, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 129 Loss: -0.36523448370112604\n",
      "In model, X: (28831, 35), b: 0.01853429498843715, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 130 Loss: -0.37572267811059235\n",
      "In model, X: (28831, 35), b: 0.01908578470317929, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 131 Loss: -0.3750055305608895\n",
      "In model, X: (28831, 35), b: 0.018180505569726557, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 132 Loss: -0.37558888268788937\n",
      "In model, X: (28831, 35), b: 0.019893848603487228, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 133 Loss: -0.3732712411159856\n",
      "In model, X: (28831, 35), b: 0.016449639239550284, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 134 Loss: -0.3735206973634628\n",
      "In model, X: (28831, 35), b: 0.02265129717718686, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 135 Loss: -0.3304588347954887\n",
      "In model, X: (28831, 35), b: -0.006420009663680493, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 136 Loss: -0.36494554965052217\n",
      "In model, X: (28831, 35), b: 0.004883795268510552, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 137 Loss: -0.3650498752415971\n",
      "In model, X: (28831, 35), b: 0.016104356469995067, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 138 Loss: -0.370808934294096\n",
      "In model, X: (28831, 35), b: 0.023828680981805274, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 139 Loss: -0.303427280584125\n",
      "In model, X: (28831, 35), b: -0.01903264898723419, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 140 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.007711501611145952, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 141 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: 0.003599240298603972, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 142 Loss: -0.3650475653928166\n",
      "In model, X: (28831, 35), b: 0.014837143943985685, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 143 Loss: -0.3681633140810659\n",
      "In model, X: (28831, 35), b: 0.02435120866598631, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 144 Loss: -0.32239154771910267\n",
      "In model, X: (28831, 35), b: -0.008835292494020723, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 145 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: 0.0024754494157291996, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 146 Loss: -0.3649948602500131\n",
      "In model, X: (28831, 35), b: 0.013727227016228662, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 147 Loss: -0.3666556340871895\n",
      "In model, X: (28831, 35), b: 0.02402863869116189, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 148 Loss: -0.35254642909727585\n",
      "In model, X: (28831, 35), b: 0.006923967306106691, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 149 Loss: -0.36506536137607865\n",
      "In model, X: (28831, 35), b: 0.01812371757491457, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 150 Loss: -0.37221799306259185\n",
      "In model, X: (28831, 35), b: 0.025164749806991037, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 151 Loss: -0.3324119928334272\n",
      "In model, X: (28831, 35), b: -0.0028779358617152567, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 152 Loss: -0.36492952116877475\n",
      "In model, X: (28831, 35), b: 0.008418932092916913, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 153 Loss: -0.3651203763676627\n",
      "In model, X: (28831, 35), b: 0.01958746596270984, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 154 Loss: -0.3750228282678788\n",
      "In model, X: (28831, 35), b: 0.024852639475754872, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 155 Loss: -0.35478499861132107\n",
      "In model, X: (28831, 35), b: 0.00906068376291345, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 156 Loss: -0.36512553841248985\n",
      "In model, X: (28831, 35), b: 0.020222280655147504, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 157 Loss: -0.37539445345032946\n",
      "In model, X: (28831, 35), b: 0.024914764466630518, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 158 Loss: -0.3594844329607842\n",
      "In model, X: (28831, 35), b: 0.01186284118974092, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 159 Loss: -0.3652998227823645\n",
      "In model, X: (28831, 35), b: 0.02286141910934135, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 160 Loss: -0.37522555324348333\n",
      "In model, X: (28831, 35), b: 0.022372362191441425, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 161 Loss: -0.37561847349092364\n",
      "In model, X: (28831, 35), b: 0.02341984580283195, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 162 Loss: -0.3744533671096234\n",
      "In model, X: (28831, 35), b: 0.02150523816836348, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 163 Loss: -0.37517407727445984\n",
      "In model, X: (28831, 35), b: 0.024616472604040915, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 164 Loss: -0.3687507254249588\n",
      "In model, X: (28831, 35), b: 0.017565034925087313, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 165 Loss: -0.3681615465795512\n",
      "In model, X: (28831, 35), b: 0.02712419000122064, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 166 Loss: -0.310900039510952\n",
      "In model, X: (28831, 35), b: -0.011906946566421132, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 167 Loss: -0.36491403503429326\n",
      "In model, X: (28831, 35), b: -0.0005892676791123324, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 168 Loss: -0.36492952116877475\n",
      "In model, X: (28831, 35), b: 0.01070760027551984, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 169 Loss: -0.36513355265336356\n",
      "In model, X: (28831, 35), b: 0.021872665656533338, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 170 Loss: -0.37528107564186586\n",
      "In model, X: (28831, 35), b: 0.026121564423668246, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 171 Loss: -0.3625161451611452\n",
      "In model, X: (28831, 35), b: 0.014877013839889696, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 172 Loss: -0.3657152171854332\n",
      "In model, X: (28831, 35), b: 0.02571604127563595, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 173 Loss: -0.3696230111380977\n",
      "In model, X: (28831, 35), b: 0.019308181568572887, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 174 Loss: -0.36873818952651044\n",
      "In model, X: (28831, 35), b: 0.028419901592158608, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 175 Loss: -0.3060285951588651\n",
      "In model, X: (28831, 35), b: -0.013077243499770424, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 176 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: -0.0017560961236821853, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 177 Loss: -0.36494554965052217\n",
      "In model, X: (28831, 35), b: 0.00954770880850886, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 178 Loss: -0.3650498752415972\n",
      "In model, X: (28831, 35), b: 0.020768270009993376, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 179 Loss: -0.37014399423780914\n",
      "In model, X: (28831, 35), b: 0.029030210282616594, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 180 Loss: -0.31897043221577387\n",
      "In model, X: (28831, 35), b: -0.005866115722156015, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 181 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: 0.005444626187593908, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 182 Loss: -0.3649369930623825\n",
      "In model, X: (28831, 35), b: 0.016717214720770007, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 183 Loss: -0.3659564676781758\n",
      "In model, X: (28831, 35), b: 0.027431376560519003, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 184 Loss: -0.36938017498114645\n",
      "In model, X: (28831, 35), b: 0.020872464283113953, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 185 Loss: -0.36905333568879933\n",
      "In model, X: (28831, 35), b: 0.029845444755522126, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 186 Loss: -0.3227693371022419\n",
      "In model, X: (28831, 35), b: -0.003094954799537545, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 187 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: 0.008215787110212376, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 188 Loss: -0.3649948602500131\n",
      "In model, X: (28831, 35), b: 0.019467564710711836, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 189 Loss: -0.36701832334745466\n",
      "In model, X: (28831, 35), b: 0.029623299856908637, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 190 Loss: -0.35102532855266044\n",
      "In model, X: (28831, 35), b: 0.011635600783980782, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 191 Loss: -0.36505503728642436\n",
      "In model, X: (28831, 35), b: 0.02284922500790642, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 192 Loss: -0.37130202079739183\n",
      "In model, X: (28831, 35), b: 0.03059436045239795, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 193 Loss: -0.3351710126971304\n",
      "In model, X: (28831, 35), b: 0.004066078892095331, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 194 Loss: -0.3649266689727282\n",
      "In model, X: (28831, 35), b: 0.015352541380389183, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 195 Loss: -0.36522470195873774\n",
      "In model, X: (28831, 35), b: 0.026437831519475583, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 196 Loss: -0.37524295504679783\n",
      "In model, X: (28831, 35), b: 0.028872704321637153, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 197 Loss: -0.3717166383680176\n",
      "In model, X: (28831, 35), b: 0.024086189808380943, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 198 Loss: -0.372863631088333\n",
      "In model, X: (28831, 35), b: 0.030957266080449214, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 199 Loss: -0.34404936864495805\n",
      "In model, X: (28831, 35), b: 0.009189250536022481, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 200 Loss: -0.36495016934808333\n",
      "In model, X: (28831, 35), b: 0.02045837058041914, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 201 Loss: -0.3667174381651578\n",
      "In model, X: (28831, 35), b: 0.03080487260957461, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 202 Loss: -0.3572738766086769\n",
      "In model, X: (28831, 35), b: 0.016457019089254652, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 203 Loss: -0.3652327161996114\n",
      "In model, X: (28831, 35), b: 0.0275457777171205, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 204 Loss: -0.37523863208107494\n",
      "In model, X: (28831, 35), b: 0.029758673563814904, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 205 Loss: -0.3720510287768454\n",
      "In model, X: (28831, 35), b: 0.025263634511324042, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 206 Loss: -0.3733852642683868\n",
      "In model, X: (28831, 35), b: 0.03171849116986576, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 207 Loss: -0.34714190027649394\n",
      "In model, X: (28831, 35), b: 0.011701583810211276, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 208 Loss: -0.3649948602500131\n",
      "In model, X: (28831, 35), b: 0.022953361410710738, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 209 Loss: -0.36817118786222286\n",
      "In model, X: (28831, 35), b: 0.03259922870633004, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 210 Loss: -0.33410413838583086\n",
      "In model, X: (28831, 35), b: 0.005539304271987401, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 211 Loss: -0.36492952116877475\n",
      "In model, X: (28831, 35), b: 0.016836172226619567, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 212 Loss: -0.3651834056001204\n",
      "In model, X: (28831, 35), b: 0.02797695818617699, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 213 Loss: -0.37488568557009777\n",
      "In model, X: (28831, 35), b: 0.031376077190026976, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 214 Loss: -0.3681371775482884\n",
      "In model, X: (28831, 35), b: 0.0239673921167211, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 215 Loss: -0.3684886438732924\n",
      "In model, X: (28831, 35), b: 0.03345717741726565, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 216 Loss: -0.3288444976810839\n",
      "In model, X: (28831, 35), b: 0.003678061270641813, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 217 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: 0.014988803180391738, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 218 Loss: -0.3650630515272981\n",
      "In model, X: (28831, 35), b: 0.026205895893096815, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 219 Loss: -0.37179824041619375\n",
      "In model, X: (28831, 35), b: 0.0337255795669205, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 220 Loss: -0.3405683697329168\n",
      "In model, X: (28831, 35), b: 0.010056604458775561, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 221 Loss: -0.3649450073032562\n",
      "In model, X: (28831, 35), b: 0.021332661480731093, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 222 Loss: -0.3657782464178909\n",
      "In model, X: (28831, 35), b: 0.03214394100624183, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 223 Loss: -0.3719989421079383\n",
      "In model, X: (28831, 35), b: 0.02758966959588493, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 224 Loss: -0.37389750714438236\n",
      "In model, X: (28831, 35), b: 0.033826012421386255, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 225 Loss: -0.3524186225512775\n",
      "In model, X: (28831, 35), b: 0.01677145282136694, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 226 Loss: -0.3650813898578261\n",
      "In model, X: (28831, 35), b: 0.027978140067733692, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 227 Loss: -0.37394569134424305\n",
      "In model, X: (28831, 35), b: 0.03410694305893734, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 228 Loss: -0.353314436051671\n",
      "In model, X: (28831, 35), b: 0.017531970968867213, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 229 Loss: -0.36507337561695236\n",
      "In model, X: (28831, 35), b: 0.02873518972645453, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 230 Loss: -0.3742834813209529\n",
      "In model, X: (28831, 35), b: 0.034129863041549635, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 231 Loss: -0.35878117110565516\n",
      "In model, X: (28831, 35), b: 0.020755242890753995, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 232 Loss: -0.36531123156655065\n",
      "In model, X: (28831, 35), b: 0.031795442675707684, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 233 Loss: -0.37567051531834555\n",
      "In model, X: (28831, 35), b: 0.03238855425699171, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 234 Loss: -0.3748138523890765\n",
      "In model, X: (28831, 35), b: 0.0312994482179745, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 235 Loss: -0.3753292442400566\n",
      "In model, X: (28831, 35), b: 0.03340481986486141, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 236 Loss: -0.3715992769359905\n",
      "In model, X: (28831, 35), b: 0.028576683483882603, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 237 Loss: -0.37298326404565235\n",
      "In model, X: (28831, 35), b: 0.03547202385397748, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 238 Loss: -0.3389609991781895\n",
      "In model, X: (28831, 35), b: 0.011015711182603688, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 239 Loss: -0.3649134926870273\n",
      "In model, X: (28831, 35), b: 0.02230564215967698, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 240 Loss: -0.36547165684374167\n",
      "In model, X: (28831, 35), b: 0.03328687763538022, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 241 Loss: -0.3745040774439124\n",
      "In model, X: (28831, 35), b: 0.03163270295356333, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 242 Loss: -0.3750573986407359\n",
      "In model, X: (28831, 35), b: 0.034601729339713526, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 243 Loss: -0.36750805438117734\n",
      "In model, X: (28831, 35), b: 0.02675570974872298, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 244 Loss: -0.36762565764394334\n",
      "In model, X: (28831, 35), b: 0.03667905614669739, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 245 Loss: -0.3200359045331405\n",
      "In model, X: (28831, 35), b: 0.002500567710100539, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 246 Loss: -0.3649060207934195\n",
      "In model, X: (28831, 35), b: 0.013814778108629902, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 247 Loss: -0.3649450073032562\n",
      "In model, X: (28831, 35), b: 0.025090835130585437, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 248 Loss: -0.36598336259701175\n",
      "In model, X: (28831, 35), b: 0.03582580790294851, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 249 Loss: -0.36346785397916903\n",
      "In model, X: (28831, 35), b: 0.02532327522555446, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 250 Loss: -0.36593065745420816\n",
      "In model, X: (28831, 35), b: 0.0360721219530353, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 251 Loss: -0.3636554749838314\n",
      "In model, X: (28831, 35), b: 0.02568746669859614, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 252 Loss: -0.36599653888271255\n",
      "In model, X: (28831, 35), b: 0.03641897098217977, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 253 Loss: -0.36256751929839026\n",
      "In model, X: (28831, 35), b: 0.025297621747443207, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 254 Loss: -0.36570489309577886\n",
      "In model, X: (28831, 35), b: 0.03615052313830722, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 255 Loss: -0.3689217363866937\n",
      "In model, X: (28831, 35), b: 0.029310663284334803, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 256 Loss: -0.3690851117328606\n",
      "In model, X: (28831, 35), b: 0.038554185881539194, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 257 Loss: -0.30669339049739236\n",
      "In model, X: (28831, 35), b: -0.0023891773801699295, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 258 Loss: -0.3649220492751669\n",
      "In model, X: (28831, 35), b: 0.008931969995918311, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 259 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: 0.020242711905668235, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 260 Loss: -0.36504471319677\n",
      "In model, X: (28831, 35), b: 0.03147021008471163, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 261 Loss: -0.3728130953345874\n",
      "In model, X: (28831, 35), b: 0.03846615195283728, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 262 Loss: -0.3446169012957994\n",
      "In model, X: (28831, 35), b: 0.017145509112845608, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 263 Loss: -0.3649369930623825\n",
      "In model, X: (28831, 35), b: 0.028418097646021708, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 264 Loss: -0.3669923726636022\n",
      "In model, X: (28831, 35), b: 0.038736851764852136, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 265 Loss: -0.3495415222194739\n",
      "In model, X: (28831, 35), b: 0.02006119923121741, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 266 Loss: -0.36498684600913944\n",
      "In model, X: (28831, 35), b: 0.031309508342937435, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 267 Loss: -0.3706729355795185\n",
      "In model, X: (28831, 35), b: 0.03980036849297593, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 268 Loss: -0.3270653827489674\n",
      "In model, X: (28831, 35), b: 0.0091319040439715, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 269 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: 0.020442645953721424, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 270 Loss: -0.36498684600913944\n",
      "In model, X: (28831, 35), b: 0.03169095506544145, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 271 Loss: -0.3704975650710677\n",
      "In model, X: (28831, 35), b: 0.04028933874967022, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 272 Loss: -0.33207858198756235\n",
      "In model, X: (28831, 35), b: 0.012231044576656469, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 273 Loss: -0.36494554965052217\n",
      "In model, X: (28831, 35), b: 0.023534849508847513, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 274 Loss: -0.3650865519026532\n",
      "In model, X: (28831, 35), b: 0.034734599777655396, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 275 Loss: -0.3739367409831433\n",
      "In model, X: (28831, 35), b: 0.03957190275167548, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 276 Loss: -0.3624267245579182\n",
      "In model, X: (28831, 35), b: 0.028389494924036012, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 277 Loss: -0.3658361136055216\n",
      "In model, X: (28831, 35), b: 0.039179963516870114, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 278 Loss: -0.369740685949704\n",
      "In model, X: (28831, 35), b: 0.03290547276270126, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 279 Loss: -0.3712731985364184\n",
      "In model, X: (28831, 35), b: 0.04107115857771148, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 280 Loss: -0.3366582719756549\n",
      "In model, X: (28831, 35), b: 0.01550821028557794, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 281 Loss: -0.36492952116877475\n",
      "In model, X: (28831, 35), b: 0.026805078240210105, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 282 Loss: -0.36534845057487264\n",
      "In model, X: (28831, 35), b: 0.03785221500272268, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 283 Loss: -0.3754107665435888\n",
      "In model, X: (28831, 35), b: 0.03925001598083652, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 284 Loss: -0.3734803441641734\n",
      "In model, X: (28831, 35), b: 0.036134793624909144, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 285 Loss: -0.37426188192111043\n",
      "In model, X: (28831, 35), b: 0.04068545090353284, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 286 Loss: -0.36062443752950607\n",
      "In model, X: (28831, 35), b: 0.02846623861167136, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 287 Loss: -0.365555876602774\n",
      "In model, X: (28831, 35), b: 0.03941972617713909, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 288 Loss: -0.37419828281280126\n",
      "In model, X: (28831, 35), b: 0.03719295638073277, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 289 Loss: -0.37479194549777245\n",
      "In model, X: (28831, 35), b: 0.040945861240728486, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 290 Loss: -0.36218980863557465\n",
      "In model, X: (28831, 35), b: 0.02964264498796414, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 291 Loss: -0.3657077452918254\n",
      "In model, X: (28831, 35), b: 0.040505951845166456, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 292 Loss: -0.37040911140150634\n",
      "In model, X: (28831, 35), b: 0.034685827662887796, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 293 Loss: -0.3719703359054027\n",
      "In model, X: (28831, 35), b: 0.04247258497272793, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 294 Loss: -0.3321584933403874\n",
      "In model, X: (28831, 35), b: 0.014485401426449225, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 295 Loss: -0.3649191970791204\n",
      "In model, X: (28831, 35), b: 0.025796143336199142, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 296 Loss: -0.3650710657681717\n",
      "In model, X: (28831, 35), b: 0.037016704537683645, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 297 Loss: -0.373625604285006\n",
      "In model, X: (28831, 35), b: 0.042080698155664294, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 298 Loss: -0.3575506601416552\n",
      "In model, X: (28831, 35), b: 0.028054316121165324, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 299 Loss: -0.3651203763676627\n",
      "In model, X: (28831, 35), b: 0.03922284999095825, w: (35, 1)\n",
      "0.8985467032014152\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = np.random.rand(X_train1.shape[1],1)  # assuming X is N-by-n. \n",
    "                                        # if X is n-by-N, use X_train.shape[0]\n",
    "y_train1 = y_train1.reshape(-1,1)\n",
    "y_test1 = y_test1.reshape(-1,1)\n",
    "print(w1.shape)\n",
    "print(X_train1.shape)\n",
    "print(y_train1.shape)\n",
    "b1 = 0\n",
    "w1, b1, loss1 = train(w1, b1, X_train1, y_train1, iter=300, lr=0.1)\n",
    "plt.figure()\n",
    "plt.plot(loss1)\n",
    "\n",
    "#training accuracy \n",
    "z1 = model(w1,b1,X_train1)\n",
    "print(accuracy(np.squeeze(y_train1), predict(z1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBqSssY6OUGJ",
    "outputId": "9d7af0f7-54ec-42c9-c611-f04d215a4fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In model, X: (12357, 35), b: 0.03922284999095825, w: (35, 1)\n",
      "0.903293679695719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-84be012d15e9>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-t))\n"
     ]
    }
   ],
   "source": [
    "z1 = model(w1,b1,X_test1)\n",
    "y_test1=np.squeeze(y_test1)\n",
    "print(accuracy(y_test1, predict(z1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "fsI9xje4Oase"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
