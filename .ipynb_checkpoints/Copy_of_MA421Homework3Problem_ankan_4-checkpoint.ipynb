{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnzRlaQaW6y1"
   },
   "source": [
    "In this homework, you will write a python implementation of logistic regression. You will test it on two datasets. \n",
    "First we import some libraries that we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Szla9qyoPuqg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0byO4vq0Xcxz"
   },
   "source": [
    "We define some functions involved. Use the formulations that avoid overflows.  \n",
    "1. sigmoid function sigmoid(t)\n",
    "2. log of sigmoid(t), called log_sig(t)\n",
    "3. log of 1-sigmoid = 1/(1+e^t), called log_one_sig(t)\n",
    "4. cross-entropy loss function given the inputs of label y and prediction y_hat = sigmoid(z), where y, y_hat, and z are vectors of dimension N. (N = # of data points.) You should implement this function with z, rather than y_hat, as the input; namely, the loss function should be\n",
    "\n",
    "    loss = -y log(sigmoid(z)) - (1-y) log (1-sigmoid(z)) \n",
    "\n",
    "  where log(sigmoid(z)) and log (1-sigmoid(z)) should be computed by the functions log_sig(z) and log_one_sig(z) in parts 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "id": "kuzmD54GT9yb"
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "  return 1/(1+np.exp(-t))\n",
    "\n",
    "def customloss(y, z): \n",
    "  # loss function for y and yhat = sigmoid(z)\n",
    "  return -((y*log_sig(z))  - ((1-y)*log_one_sig(z)))\n",
    "\n",
    "# def log_sig(t):\n",
    "#   return np.log(sigmoid(t))\n",
    "\n",
    "def log_sig(z):\n",
    "  return np.log(sigmoid(z))\n",
    "  # if np.isnan(z).any():\n",
    "  #   print('z is nan in loss')\n",
    "  # if (z <= 0).any():\n",
    "  #       return z - np.log(1+np.exp(z))\n",
    "  # else:\n",
    "  #       return -np.log(1+np.exp(-z))\n",
    "\n",
    "def log_one_sig(t):\n",
    "  return 1./(1+np.exp(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLulJqXcbEpw"
   },
   "source": [
    "Define the model output z=w^T x + b, or z = x^Tw + B, given the data input X (an N-by-n array containing N data points) and the model parameters w (n-dimensional weigth vector) and b (bias).\n",
    "\n",
    "Note that mathematically it's easier to write the data matrix as an n-by-N matrix, with each column being a data point. In python, the data is more commonly represented as as an N-by-n array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "id": "eI9PNMZnhy0d"
   },
   "outputs": [],
   "source": [
    "def model(w,b,X):\n",
    "  # using X as Nxn\n",
    "  print(f'In model, X: {X.shape}, b: {b}, w: {w.shape}')\n",
    "  return (X @ w)+b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv_xi0ajaEEY"
   },
   "source": [
    "Define the function that computes the gradient of the cross-entropy loss given the label y (N-vector), the model prediction y_hat = sigmoid(z) (N-vector), and the dataset X (an n-by-N or N-by-n array). It's probably easier to return the gradients with respect w and b separately, which can be used to update w and b later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "id": "I8KJF8lrZlFi"
   },
   "outputs": [],
   "source": [
    "def gradients(X, y, y_hat):\n",
    "  # Using X as Nxn\n",
    "  # print(f'grad: y shape: {y.shape}, X shape: {X.shape}')\n",
    "  return (np.transpose(X) @ (y_hat - y))/X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgqML9T2cOqd"
   },
   "source": [
    "Write the function that minimizes the loss (i.e. training) by the gradient descent algorithm using a fixed number of iteration (*iter*) and learning rate (*lr*). Your function should take *iter* and *lr* as well as the initial weight w, initial bias b, the input data X and the label y as the inputs. It produces new w and b as output. Also compute the loss value at each iteration and output the sequence of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "id": "6bIdE16li086"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def train(w, b, X, y, iter, lr):\n",
    "  print(f'>> {X.shape}')\n",
    "  losslist=list()\n",
    "  for k in range(iter):\n",
    "    z = model(w, b, X)\n",
    "    y_hat = sigmoid(z)\n",
    "    grad = gradients(X, y, y_hat)\n",
    "    print(f'gradient shape: {grad.shape}')\n",
    "    w = w - (lr * grad)\n",
    "    b = np.mean((b*np.ones(y_hat.shape)) - (lr * (y_hat - y)))\n",
    "    myloss = customloss(y, z)\n",
    "    losslist.append(np.mean(myloss))\n",
    "    print(f'Iter: {k} Loss: {losslist[-1]}')\n",
    "  return w, b, losslist\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "id": "XbA52seSg7wW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGC-EjrzeHyU"
   },
   "source": [
    "1. Write the function that uses a trained model to produce class prediction (0 or 1) for an input dataset X, i.e. turn the model output z = model(w,b,X) into predicted label y_label (N-vector of 0 or 1). \n",
    "2. For an input dataset X with a known label y (e.g. a training or testing dataset) and a predicted label y_label, compute the accuracy of prediction (i.e. # correct predictions/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "id": "HChwCsuWf07D"
   },
   "outputs": [],
   "source": [
    "def predict(z):\n",
    "  ypred = sigmoid(z)\n",
    "  ypred[ypred<=0.5]=0\n",
    "  ypred[ypred>0.5]=1\n",
    "  ypred = ypred.astype(int)\n",
    "  ypred = np.squeeze(ypred)\n",
    "  # print(f'In pred, {ypred.shape}')\n",
    "  return ypred\n",
    "\n",
    "def accuracy(y, y_label):\n",
    "  diff_bool = (y == y_label)\n",
    "  diff_true = diff_bool[diff_bool==True]\n",
    "  total_sample = len(diff_bool)\n",
    "  return (len(diff_true)/total_sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icfCmavagcp5"
   },
   "source": [
    "We are ready to test your programs on some datasets. First, we use a synthetic dataset generated using [scikit-learn](https://scikit-learn.org/stable/datasets.html) package. We generate a dataset for training and simultaneously a dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "EXJOlxH2nYw3",
    "outputId": "cd4033c7-cdd6-4644-81f7-92d93353adb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba02720d00>]"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X_train, y_train = make_moons(n_samples=500, noise=0.1)\n",
    "X_test, y_test = make_moons(n_samples=1000, noise=0.1)\n",
    "\n",
    "print(X_train.shape)\n",
    "plt.figure()\n",
    "plt.plot(X_train[:, 0][y_train==0], X_train[:, 1][y_train==0], \"g^\")\n",
    "plt.plot(X_train[:, 0][y_train==1], X_train[:, 1][y_train==1], \"bs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2_uUM_Zufoz"
   },
   "source": [
    "Here is another toy test example you may try but not part of homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "EJbVzPfLdOvC",
    "outputId": "41c42107-78d0-4bdc-d908-c7df54628c9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba01fd5070>]"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X_train, y_train = make_classification(n_samples=1000, n_features=4)\n",
    "X_test=X_train[500:,]\n",
    "y_test=y_train[500:,]\n",
    "X_train=X_train[:500,]\n",
    "y_train=y_train[:500,]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_train[:, 0][y_train==0], X_train[:, 1][y_train==0], \"g^\")\n",
    "plt.plot(X_train[:, 0][y_train==1], X_train[:, 1][y_train==1], \"bs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMm__k_AjOFp"
   },
   "source": [
    "We now train the model using (X_train, y_train). We initialize weight as a random vector, and b=0. We plot the loss convergence history. You should get the loss down to about 0.2.\n",
    "We compute the prediction accuracy on (X_train, y_train). You should get an accuracy in the 80s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mi868jT_mpnq",
    "outputId": "cb239b83-1064-4988-829c-d86c0f1a0002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(500, 4)\n",
      "(500, 1)\n",
      ">> (500, 4)\n",
      "In model, X: (500, 4), b: 0, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 0 Loss: 0.5794594170520441\n",
      "In model, X: (500, 4), b: 0.002120018476036685, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 1 Loss: 0.57543664409589\n",
      "In model, X: (500, 4), b: 0.004178258936881579, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 2 Loss: 0.5716666165498347\n",
      "In model, X: (500, 4), b: 0.006174674886828497, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 3 Loss: 0.5681306795311589\n",
      "In model, X: (500, 4), b: 0.00810936830626127, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 4 Loss: 0.5648115948420368\n",
      "In model, X: (500, 4), b: 0.00998258678566317, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 5 Loss: 0.5616934724294566\n",
      "In model, X: (500, 4), b: 0.011794718773252295, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 6 Loss: 0.5587616897348907\n",
      "In model, X: (500, 4), b: 0.013546286903300642, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 7 Loss: 0.5560028046393721\n",
      "In model, X: (500, 4), b: 0.015237939577804676, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 8 Loss: 0.5534044664790211\n",
      "In model, X: (500, 4), b: 0.01687044111084794, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 9 Loss: 0.5509553284104263\n",
      "In model, X: (500, 4), b: 0.018444660816434966, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 10 Loss: 0.5486449633547035\n",
      "In model, X: (500, 4), b: 0.019961561438436425, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 11 Loss: 0.5464637848938958\n",
      "In model, X: (500, 4), b: 0.02142218730022946, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 12 Loss: 0.5444029738408546\n",
      "In model, X: (500, 4), b: 0.022827652505928855, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 13 Loss: 0.5424544107349382\n",
      "In model, X: (500, 4), b: 0.02417912946681997, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 14 Loss: 0.5406106141995647\n",
      "In model, X: (500, 4), b: 0.025477837964803254, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 15 Loss: 0.5388646848996634\n",
      "In model, X: (500, 4), b: 0.026725034905509083, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 16 Loss: 0.5372102547257991\n",
      "In model, X: (500, 4), b: 0.027922004860929207, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 17 Loss: 0.5356414407807331\n",
      "In model, X: (500, 4), b: 0.02907005145672692, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 18 Loss: 0.534152803732825\n",
      "In model, X: (500, 4), b: 0.030170489623289935, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 19 Loss: 0.5327393101137263\n",
      "In model, X: (500, 4), b: 0.03122463870168597, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 20 Loss: 0.5313962981645752\n",
      "In model, X: (500, 4), b: 0.03223381637509996, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 21 Loss: 0.5301194468681933\n",
      "In model, X: (500, 4), b: 0.03319933338198756, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 22 Loss: 0.5289047478400462\n",
      "In model, X: (500, 4), b: 0.03412248895794336, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 23 Loss: 0.5277484797852102\n",
      "In model, X: (500, 4), b: 0.03500456694809128, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 24 Loss: 0.5266471852607986\n",
      "In model, X: (500, 4), b: 0.0358468325297106, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 25 Loss: 0.5255976495125494\n",
      "In model, X: (500, 4), b: 0.036650529485013, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 26 Loss: 0.5245968811803634\n",
      "In model, X: (500, 4), b: 0.03741687796581954, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 27 Loss: 0.5236420946905869\n",
      "In model, X: (500, 4), b: 0.03814707269482481, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 28 Loss: 0.5227306941730088\n",
      "In model, X: (500, 4), b: 0.038842281551766916, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 29 Loss: 0.5218602587581737\n",
      "In model, X: (500, 4), b: 0.039503644496832614, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 30 Loss: 0.5210285291260222\n",
      "In model, X: (500, 4), b: 0.040132272787787145, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 31 Loss: 0.52023339519035\n",
      "In model, X: (500, 4), b: 0.040729248451460046, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 32 Loss: 0.5194728848154153\n",
      "In model, X: (500, 4), b: 0.04129562397422804, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 33 Loss: 0.5187451534714242\n",
      "In model, X: (500, 4), b: 0.041832422179937094, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 34 Loss: 0.5180484747448252\n",
      "In model, X: (500, 4), b: 0.042340636267252554, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 35 Loss: 0.517381231627498\n",
      "In model, X: (500, 4), b: 0.042821229981694714, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 36 Loss: 0.5167419085161694\n",
      "In model, X: (500, 4), b: 0.043275137900599, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 37 Loss: 0.5161290838598668\n",
      "In model, X: (500, 4), b: 0.04370326581193805, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 38 Loss: 0.5155414233990224\n",
      "In model, X: (500, 4), b: 0.04410649117036793, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 39 Loss: 0.514977673945037\n",
      "In model, X: (500, 4), b: 0.044485663616027876, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 40 Loss: 0.5144366576538102\n",
      "In model, X: (500, 4), b: 0.04484160554355007, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 41 Loss: 0.5139172667509591\n",
      "In model, X: (500, 4), b: 0.04517511271044249, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 42 Loss: 0.513418458670272\n",
      "In model, X: (500, 4), b: 0.04548695487551406, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 43 Loss: 0.5129392515703923\n",
      "In model, X: (500, 4), b: 0.045777876459335974, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 44 Loss: 0.5124787201978571\n",
      "In model, X: (500, 4), b: 0.04604859721989483, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 45 Loss: 0.5120359920674502\n",
      "In model, X: (500, 4), b: 0.04629981293760958, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 46 Loss: 0.5116102439334024\n",
      "In model, X: (500, 4), b: 0.04653219610477175, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 47 Loss: 0.511200698527303\n",
      "In model, X: (500, 4), b: 0.046746396615240667, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 48 Loss: 0.5108066215407154\n",
      "In model, X: (500, 4), b: 0.046943042450897136, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 49 Loss: 0.5104273188324079\n",
      "In model, X: (500, 4), b: 0.047122740361941286, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 50 Loss: 0.5100621338418698\n",
      "In model, X: (500, 4), b: 0.04728607653862443, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 51 Loss: 0.5097104451923735\n",
      "In model, X: (500, 4), b: 0.047433617272440456, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 52 Loss: 0.5093716644682933\n",
      "In model, X: (500, 4), b: 0.04756590960517754, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 53 Loss: 0.5090452341527071\n",
      "In model, X: (500, 4), b: 0.04768348196455426, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 54 Loss: 0.5087306257125135\n",
      "In model, X: (500, 4), b: 0.04778684478544118, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 55 Loss: 0.5084273378193818\n",
      "In model, X: (500, 4), b: 0.04787649111590674, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 56 Loss: 0.5081348946958512\n",
      "In model, X: (500, 4), b: 0.04795289720752874, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 57 Loss: 0.5078528445768\n",
      "In model, X: (500, 4), b: 0.048016523089585365, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 58 Loss: 0.5075807582773317\n",
      "In model, X: (500, 4), b: 0.0480678131268864, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 59 Loss: 0.5073182278588719\n",
      "In model, X: (500, 4), b: 0.04810719656112867, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 60 Loss: 0.5070648653859631\n",
      "In model, X: (500, 4), b: 0.048135088035764176, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 61 Loss: 0.5068203017668608\n",
      "In model, X: (500, 4), b: 0.04815188810445646, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 62 Loss: 0.5065841856716101\n",
      "In model, X: (500, 4), b: 0.04815798372327319, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 63 Loss: 0.5063561825217977\n",
      "In model, X: (500, 4), b: 0.04815374872682283, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 64 Loss: 0.506135973546651\n",
      "In model, X: (500, 4), b: 0.04813954428859226, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 65 Loss: 0.505923254900588\n",
      "In model, X: (500, 4), b: 0.048115719365781684, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 66 Loss: 0.5057177368377166\n",
      "In model, X: (500, 4), b: 0.04808261112896459, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 67 Loss: 0.5055191429391457\n",
      "In model, X: (500, 4), b: 0.048040545376925536, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 68 Loss: 0.5053272093892957\n",
      "In model, X: (500, 4), b: 0.047989836937046705, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 69 Loss: 0.5051416842977049\n",
      "In model, X: (500, 4), b: 0.04793079005162831, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 70 Loss: 0.5049623270630988\n",
      "In model, X: (500, 4), b: 0.047863698750537226, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 71 Loss: 0.504788907776745\n",
      "In model, X: (500, 4), b: 0.047788847210583824, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 72 Loss: 0.5046212066623459\n",
      "In model, X: (500, 4), b: 0.04770651010202999, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 73 Loss: 0.5044590135499385\n",
      "In model, X: (500, 4), b: 0.04761695292263129, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 74 Loss: 0.5043021273814569\n",
      "In model, X: (500, 4), b: 0.047520432319614295, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 75 Loss: 0.5041503557457993\n",
      "In model, X: (500, 4), b: 0.04741719639998633, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 76 Loss: 0.5040035144413995\n",
      "In model, X: (500, 4), b: 0.04730748502956952, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 77 Loss: 0.5038614270644562\n",
      "In model, X: (500, 4), b: 0.04719153012114465, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 78 Loss: 0.5037239246211096\n",
      "In model, X: (500, 4), b: 0.0470695559120829, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 79 Loss: 0.5035908451619812\n",
      "In model, X: (500, 4), b: 0.046941779231835255, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 80 Loss: 0.5034620334376122\n",
      "In model, X: (500, 4), b: 0.04680840975964053, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 81 Loss: 0.5033373405734376\n",
      "In model, X: (500, 4), b: 0.04666965027280378, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 82 Loss: 0.5032166237630382\n",
      "In model, X: (500, 4), b: 0.046525696885887154, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 83 Loss: 0.5030997459784975\n",
      "In model, X: (500, 4), b: 0.04637673928114546, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 84 Loss: 0.5029865756967784\n",
      "In model, X: (500, 4), b: 0.0462229609305287, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 85 Loss: 0.5028769866411092\n",
      "In model, X: (500, 4), b: 0.04606453930956383, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 86 Loss: 0.5027708575364419\n",
      "In model, X: (500, 4), b: 0.04590164610341785, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 87 Loss: 0.5026680718781072\n",
      "In model, X: (500, 4), b: 0.04573444740543461, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 88 Loss: 0.502568517712858\n",
      "In model, X: (500, 4), b: 0.04556310390842752, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 89 Loss: 0.5024720874315401\n",
      "In model, X: (500, 4), b: 0.04538777108900081, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 90 Loss: 0.5023786775726906\n",
      "In model, X: (500, 4), b: 0.04520859938516232, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 91 Loss: 0.5022881886364036\n",
      "In model, X: (500, 4), b: 0.045025734367481585, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 92 Loss: 0.5022005249078516\n",
      "In model, X: (500, 4), b: 0.04483931690403743, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 93 Loss: 0.5021155942898933\n",
      "In model, X: (500, 4), b: 0.04464948331939084, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 94 Loss: 0.5020333081442305\n",
      "In model, X: (500, 4), b: 0.04445636554780972, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 95 Loss: 0.5019535811406193\n",
      "In model, X: (500, 4), b: 0.04426009128096396, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 96 Loss: 0.5018763311136669\n",
      "In model, X: (500, 4), b: 0.04406078411030086, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 97 Loss: 0.5018014789267813\n",
      "In model, X: (500, 4), b: 0.04385856366430305, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 98 Loss: 0.501728948342864\n",
      "In model, X: (500, 4), b: 0.04365354574082342, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 99 Loss: 0.5016586659013662\n",
      "In model, X: (500, 4), b: 0.04344584243468384, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 100 Loss: 0.5015905608013501\n",
      "In model, X: (500, 4), b: 0.043235562260717654, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 101 Loss: 0.5015245647902222\n",
      "In model, X: (500, 4), b: 0.043022810272428574, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 102 Loss: 0.5014606120578239\n",
      "In model, X: (500, 4), b: 0.042807688176432136, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 103 Loss: 0.5013986391355844\n",
      "In model, X: (500, 4), b: 0.04259029444283934, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 104 Loss: 0.5013385848004636\n",
      "In model, X: (500, 4), b: 0.04237072441173585, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 105 Loss: 0.5012803899834215\n",
      "In model, X: (500, 4), b: 0.04214907039590413, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 106 Loss: 0.5012239976821741\n",
      "In model, X: (500, 4), b: 0.04192542177993017, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 107 Loss: 0.5011693528780057\n",
      "In model, X: (500, 4), b: 0.041699865115830806, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 108 Loss: 0.5011164024564232\n",
      "In model, X: (500, 4), b: 0.04147248421533232, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 109 Loss: 0.5010650951314508\n",
      "In model, X: (500, 4), b: 0.04124336023892602, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 110 Loss: 0.5010153813733742\n",
      "In model, X: (500, 4), b: 0.04101257178182131, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 111 Loss: 0.5009672133397564\n",
      "In model, X: (500, 4), b: 0.04078019495691212, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 112 Loss: 0.5009205448095551\n",
      "In model, X: (500, 4), b: 0.0405463034748682, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 113 Loss: 0.5008753311201856\n",
      "In model, X: (500, 4), b: 0.04031096872145803, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 114 Loss: 0.5008315291073768\n",
      "In model, X: (500, 4), b: 0.040074259832206194, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 115 Loss: 0.5007890970476808\n",
      "In model, X: (500, 4), b: 0.039836243764484025, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 116 Loss: 0.5007479946035037\n",
      "In model, X: (500, 4), b: 0.03959698536712818, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 117 Loss: 0.5007081827705301\n",
      "In model, X: (500, 4), b: 0.03935654744767843, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 118 Loss: 0.5006696238274253\n",
      "In model, X: (500, 4), b: 0.03911499083732214, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 119 Loss: 0.500632281287701\n",
      "In model, X: (500, 4), b: 0.03887237445362965, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 120 Loss: 0.5005961198536404\n",
      "In model, X: (500, 4), b: 0.03862875536116132, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 121 Loss: 0.5005611053721826\n",
      "In model, X: (500, 4), b: 0.03838418883002403, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 122 Loss: 0.5005272047926712\n",
      "In model, X: (500, 4), b: 0.03813872839245177, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 123 Loss: 0.5004943861263778\n",
      "In model, X: (500, 4), b: 0.03789242589748207, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 124 Loss: 0.5004626184077171\n",
      "In model, X: (500, 4), b: 0.037645331563797346, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 125 Loss: 0.5004318716570723\n",
      "In model, X: (500, 4), b: 0.037397494030797375, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 126 Loss: 0.5004021168451556\n",
      "In model, X: (500, 4), b: 0.03714896040796674, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 127 Loss: 0.5003733258588322\n",
      "In model, X: (500, 4), b: 0.0368997763225985, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 128 Loss: 0.5003454714683393\n",
      "In model, X: (500, 4), b: 0.03664998596593306, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 129 Loss: 0.5003185272958365\n",
      "In model, X: (500, 4), b: 0.03639963213776883, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 130 Loss: 0.5002924677852261\n",
      "In model, X: (500, 4), b: 0.03614875628959939, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 131 Loss: 0.5002672681731842\n",
      "In model, X: (500, 4), b: 0.03589739856632923, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 132 Loss: 0.5002429044613511\n",
      "In model, X: (500, 4), b: 0.03564559784661889, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 133 Loss: 0.5002193533896235\n",
      "In model, X: (500, 4), b: 0.035393391781907625, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 134 Loss: 0.5001965924105047\n",
      "In model, X: (500, 4), b: 0.03514081683416058, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 135 Loss: 0.5001745996644618\n",
      "In model, X: (500, 4), b: 0.034887908312385106, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 136 Loss: 0.5001533539562456\n",
      "In model, X: (500, 4), b: 0.03463470040795957, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 137 Loss: 0.500132834732134\n",
      "In model, X: (500, 4), b: 0.03438122622881619, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 138 Loss: 0.5001130220580536\n",
      "In model, X: (500, 4), b: 0.03412751783251783, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 139 Loss: 0.5000938965985452\n",
      "In model, X: (500, 4), b: 0.03387360625826739, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 140 Loss: 0.500075439596534\n",
      "In model, X: (500, 4), b: 0.0336195215578867, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 141 Loss: 0.5000576328538725\n",
      "In model, X: (500, 4), b: 0.03336529282580074, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 142 Loss: 0.5000404587126197\n",
      "In model, X: (500, 4), b: 0.03311094822806137, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 143 Loss: 0.5000239000370278\n",
      "In model, X: (500, 4), b: 0.03285651503044387, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 144 Loss: 0.500007940196205\n",
      "In model, X: (500, 4), b: 0.03260201962564794, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 145 Loss: 0.49999256304742673\n",
      "In model, X: (500, 4), b: 0.03234748755963386, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 146 Loss: 0.49997775292006813\n",
      "In model, X: (500, 4), b: 0.03209294355712349, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 147 Loss: 0.49996349460012945\n",
      "In model, X: (500, 4), b: 0.0318384115462944, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 148 Loss: 0.49994977331533424\n",
      "In model, X: (500, 4), b: 0.03158391468269466, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 149 Loss: 0.49993657472077213\n",
      "In model, X: (500, 4), b: 0.03132947537240464, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 150 Loss: 0.4999238848850667\n",
      "In model, X: (500, 4), b: 0.031075115294471253, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 151 Loss: 0.499911690277046\n",
      "In model, X: (500, 4), b: 0.03082085542263926, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 152 Loss: 0.49989997775289474\n",
      "In model, X: (500, 4), b: 0.030566716046403167, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 153 Loss: 0.49988873454376936\n",
      "In model, X: (500, 4), b: 0.030312716791402516, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 154 Loss: 0.4998779482438579\n",
      "In model, X: (500, 4), b: 0.03005887663918259, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 155 Loss: 0.4998676067988644\n",
      "In model, X: (500, 4), b: 0.029805213946341596, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 156 Loss: 0.49985769849490397\n",
      "In model, X: (500, 4), b: 0.029551746463084835, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 157 Loss: 0.4998482119477905\n",
      "In model, X: (500, 4), b: 0.02929849135120546, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 158 Loss: 0.49983913609270086\n",
      "In model, X: (500, 4), b: 0.02904546520151083, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 159 Loss: 0.4998304601742025\n",
      "In model, X: (500, 4), b: 0.02879268405071276, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 160 Loss: 0.49982217373662924\n",
      "In model, X: (500, 4), b: 0.028540163397799297, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 161 Loss: 0.4998142666147911\n",
      "In model, X: (500, 4), b: 0.028287918219905075, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 162 Loss: 0.49980672892500694\n",
      "In model, X: (500, 4), b: 0.02803596298769664, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 163 Loss: 0.49979955105644536\n",
      "In model, X: (500, 4), b: 0.02778431168028861, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 164 Loss: 0.49979272366276384\n",
      "In model, X: (500, 4), b: 0.027532977799705954, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 165 Loss: 0.49978623765403285\n",
      "In model, X: (500, 4), b: 0.027281974384907134, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 166 Loss: 0.4997800841889364\n",
      "In model, X: (500, 4), b: 0.027031314025382364, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 167 Loss: 0.49977425466723596\n",
      "In model, X: (500, 4), b: 0.026781008874340706, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 168 Loss: 0.4997687407224897\n",
      "In model, X: (500, 4), b: 0.026531070661499286, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 169 Loss: 0.4997635342150169\n",
      "In model, X: (500, 4), b: 0.0262815107054874, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 170 Loss: 0.4997586272250975\n",
      "In model, X: (500, 4), b: 0.026032339925877925, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 171 Loss: 0.4997540120463999\n",
      "In model, X: (500, 4), b: 0.025783568854857903, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 172 Loss: 0.49974968117962576\n",
      "In model, X: (500, 4), b: 0.025535207648549888, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 173 Loss: 0.4997456273263675\n",
      "In model, X: (500, 4), b: 0.02528726609799512, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 174 Loss: 0.4997418433831663\n",
      "In model, X: (500, 4), b: 0.02503975363980935, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 175 Loss: 0.4997383224357671\n",
      "In model, X: (500, 4), b: 0.02479267936652157, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 176 Loss: 0.49973505775356125\n",
      "In model, X: (500, 4), b: 0.024546052036605863, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 177 Loss: 0.4997320427842103\n",
      "In model, X: (500, 4), b: 0.02429988008421588, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 178 Loss: 0.49972927114844545\n",
      "In model, X: (500, 4), b: 0.02405417162863138, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 179 Loss: 0.4997267366350338\n",
      "In model, X: (500, 4), b: 0.023808934483425914, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 180 Loss: 0.49972443319590915\n",
      "In model, X: (500, 4), b: 0.023564176165364315, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 181 Loss: 0.4997223549414577\n",
      "In model, X: (500, 4), b: 0.02331990390303848, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 182 Loss: 0.49972049613595565\n",
      "In model, X: (500, 4), b: 0.0230761246452496, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 183 Loss: 0.49971885119315224\n",
      "In model, X: (500, 4), b: 0.022832845069144726, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 184 Loss: 0.49971741467199343\n",
      "In model, X: (500, 4), b: 0.02259007158811528, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 185 Loss: 0.4997161812724806\n",
      "In model, X: (500, 4), b: 0.022347810359464933, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 186 Loss: 0.49971514583166093\n",
      "In model, X: (500, 4), b: 0.022106067291853895, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 187 Loss: 0.4997143033197433\n",
      "In model, X: (500, 4), b: 0.02186484805252659, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 188 Loss: 0.4997136488363361\n",
      "In model, X: (500, 4), b: 0.021624158074329314, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 189 Loss: 0.4997131776068031\n",
      "In model, X: (500, 4), b: 0.02138400256252438, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 190 Loss: 0.4997128849787324\n",
      "In model, X: (500, 4), b: 0.021144386501406928, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 191 Loss: 0.4997127664185148\n",
      "In model, X: (500, 4), b: 0.020905314660730458, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 192 Loss: 0.49971281750802876\n",
      "In model, X: (500, 4), b: 0.020666791601946927, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 193 Loss: 0.4997130339414271\n",
      "In model, X: (500, 4), b: 0.020428821684267007, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 194 Loss: 0.4997134115220219\n",
      "In model, X: (500, 4), b: 0.020191409070546014, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 195 Loss: 0.499713946159267\n",
      "In model, X: (500, 4), b: 0.01995455773300074, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 196 Loss: 0.4997146338658287\n",
      "In model, X: (500, 4), b: 0.01971827145876235, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 197 Loss: 0.4997154707547503\n",
      "In model, X: (500, 4), b: 0.01948255385527022, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 198 Loss: 0.4997164530366983\n",
      "In model, X: (500, 4), b: 0.019247408355511572, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 199 Loss: 0.4997175770172957\n",
      "In model, X: (500, 4), b: 0.019012838223111502, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 200 Loss: 0.49971883909453174\n",
      "In model, X: (500, 4), b: 0.018778846557277858, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 201 Loss: 0.49972023575625346\n",
      "In model, X: (500, 4), b: 0.01854543629760537, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 202 Loss: 0.499721763577729\n",
      "In model, X: (500, 4), b: 0.018312610228743177, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 203 Loss: 0.4997234192192855\n",
      "In model, X: (500, 4), b: 0.01808037098492984, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 204 Loss: 0.499725199424017\n",
      "In model, X: (500, 4), b: 0.01784872105439978, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 205 Loss: 0.49972710101555956\n",
      "In model, X: (500, 4), b: 0.017617662783664962, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 206 Loss: 0.4997291208959335\n",
      "In model, X: (500, 4), b: 0.017387198381675482, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 207 Loss: 0.4997312560434478\n",
      "In model, X: (500, 4), b: 0.01715732992386269, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 208 Loss: 0.49973350351066675\n",
      "In model, X: (500, 4), b: 0.016928059356068243, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 209 Loss: 0.4997358604224366\n",
      "In model, X: (500, 4), b: 0.016699388498362517, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 210 Loss: 0.4997383239739692\n",
      "In model, X: (500, 4), b: 0.016471319048755553, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 211 Loss: 0.4997408914289816\n",
      "In model, X: (500, 4), b: 0.016243852586803754, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 212 Loss: 0.4997435601178895\n",
      "In model, X: (500, 4), b: 0.01601699057711533, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 213 Loss: 0.499746327436054\n",
      "In model, X: (500, 4), b: 0.01579073437275747, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 214 Loss: 0.4997491908420768\n",
      "In model, X: (500, 4), b: 0.015565085218568107, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 215 Loss: 0.4997521478561472\n",
      "In model, X: (500, 4), b: 0.015340044254375036, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 216 Loss: 0.4997551960584342\n",
      "In model, X: (500, 4), b: 0.015115612518125082, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 217 Loss: 0.4997583330875259\n",
      "In model, X: (500, 4), b: 0.01489179094892593, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 218 Loss: 0.49976155663891314\n",
      "In model, X: (500, 4), b: 0.014668580390003126, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 219 Loss: 0.4997648644635156\n",
      "In model, X: (500, 4), b: 0.014445981591574715, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 220 Loss: 0.499768254366251\n",
      "In model, X: (500, 4), b: 0.014223995213645868, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 221 Loss: 0.49977172420464333\n",
      "In model, X: (500, 4), b: 0.014002621828725824, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 222 Loss: 0.4997752718874716\n",
      "In model, X: (500, 4), b: 0.013781861924469356, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 223 Loss: 0.4997788953734559\n",
      "In model, X: (500, 4), b: 0.013561715906244939, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 224 Loss: 0.49978259266997976\n",
      "In model, X: (500, 4), b: 0.013342184099631692, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 225 Loss: 0.4997863618318498\n",
      "In model, X: (500, 4), b: 0.013123266752847161, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 226 Loss: 0.49979020096008864\n",
      "In model, X: (500, 4), b: 0.012904964039107897, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 227 Loss: 0.4997941082007615\n",
      "In model, X: (500, 4), b: 0.012687276058924724, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 228 Loss: 0.499798081743836\n",
      "In model, X: (500, 4), b: 0.012470202842334587, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 229 Loss: 0.49980211982207334\n",
      "In model, X: (500, 4), b: 0.01225374435107076, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 230 Loss: 0.49980622070994923\n",
      "In model, X: (500, 4), b: 0.012037900480673143, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 231 Loss: 0.49981038272260536\n",
      "In model, X: (500, 4), b: 0.011822671062540376, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 232 Loss: 0.49981460421482976\n",
      "In model, X: (500, 4), b: 0.011608055865925383, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 233 Loss: 0.49981888358006427\n",
      "In model, X: (500, 4), b: 0.011394054599875953, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 234 Loss: 0.49982321924943934\n",
      "In model, X: (500, 4), b: 0.0111806669151219, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 235 Loss: 0.49982760969083584\n",
      "In model, X: (500, 4), b: 0.01096789240591029, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 236 Loss: 0.49983205340797165\n",
      "In model, X: (500, 4), b: 0.010755730611790224, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 237 Loss: 0.49983654893951235\n",
      "In model, X: (500, 4), b: 0.010544181019348554, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 238 Loss: 0.49984109485820755\n",
      "In model, X: (500, 4), b: 0.010333243063897907, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 239 Loss: 0.4998456897700495\n",
      "In model, X: (500, 4), b: 0.010122916131118377, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 240 Loss: 0.4998503323134537\n",
      "In model, X: (500, 4), b: 0.009913199558654145, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 241 Loss: 0.4998550211584629\n",
      "In model, X: (500, 4), b: 0.009704092637666275, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 242 Loss: 0.4998597550059713\n",
      "In model, X: (500, 4), b: 0.009495594614342953, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 243 Loss: 0.49986453258696983\n",
      "In model, X: (500, 4), b: 0.00928770469136828, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 244 Loss: 0.4998693526618115\n",
      "In model, X: (500, 4), b: 0.009080422029350827, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 245 Loss: 0.49987421401949594\n",
      "In model, X: (500, 4), b: 0.00887374574821302, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 246 Loss: 0.49987911547697345\n",
      "In model, X: (500, 4), b: 0.008667674928542467, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 247 Loss: 0.49988405587846674\n",
      "In model, X: (500, 4), b: 0.008462208612906258, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 248 Loss: 0.4998890340948111\n",
      "In model, X: (500, 4), b: 0.008257345807129265, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 249 Loss: 0.4998940490228115\n",
      "In model, X: (500, 4), b: 0.008053085481537436, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 250 Loss: 0.4998990995846168\n",
      "In model, X: (500, 4), b: 0.007849426572167025, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 251 Loss: 0.49990418472711007\n",
      "In model, X: (500, 4), b: 0.007646367981940717, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 252 Loss: 0.49990930342131495\n",
      "In model, X: (500, 4), b: 0.007443908581811537, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 253 Loss: 0.4999144546618179\n",
      "In model, X: (500, 4), b: 0.007242047211875432, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 254 Loss: 0.499919637466204\n",
      "In model, X: (500, 4), b: 0.007040782682453371, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 255 Loss: 0.4999248508745093\n",
      "In model, X: (500, 4), b: 0.006840113775143815, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 256 Loss: 0.4999300939486857\n",
      "In model, X: (500, 4), b: 0.0066400392438463384, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 257 Loss: 0.4999353657720808\n",
      "In model, X: (500, 4), b: 0.006440557815757204, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 258 Loss: 0.49994066544892996\n",
      "In model, X: (500, 4), b: 0.00624166819233765, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 259 Loss: 0.49994599210386254\n",
      "In model, X: (500, 4), b: 0.0060433690502556215, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 260 Loss: 0.49995134488142\n",
      "In model, X: (500, 4), b: 0.005845659042301678, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 261 Loss: 0.4999567229455866\n",
      "In model, X: (500, 4), b: 0.005648536798279772, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 262 Loss: 0.49996212547933155\n",
      "In model, X: (500, 4), b: 0.005452000925873568, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 263 Loss: 0.49996755168416424\n",
      "In model, X: (500, 4), b: 0.005256050011488975, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 264 Loss: 0.4999730007796984\n",
      "In model, X: (500, 4), b: 0.005060682621073532, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 265 Loss: 0.49997847200322987\n",
      "In model, X: (500, 4), b: 0.004865897300913263, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 266 Loss: 0.4999839646093229\n",
      "In model, X: (500, 4), b: 0.0046716925784076185, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 267 Loss: 0.499989477869408\n",
      "In model, X: (500, 4), b: 0.004478066962823082, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 268 Loss: 0.4999950110713899\n",
      "In model, X: (500, 4), b: 0.004285018946026016, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 269 Loss: 0.5000005635192645\n",
      "In model, X: (500, 4), b: 0.004092547003195317, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 270 Loss: 0.5000061345327464\n",
      "In model, X: (500, 4), b: 0.0039006495935154026, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 271 Loss: 0.5000117234469053\n",
      "In model, X: (500, 4), b: 0.0037093251608500684, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 272 Loss: 0.5000173296118107\n",
      "In model, X: (500, 4), b: 0.0035185721343977266, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 273 Loss: 0.5000229523921875\n",
      "In model, X: (500, 4), b: 0.0033283889293285107, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 274 Loss: 0.5000285911670773\n",
      "In model, X: (500, 4), b: 0.003138773947403754, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 275 Loss: 0.5000342453295109\n",
      "In model, X: (500, 4), b: 0.0029497255775782867, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 276 Loss: 0.5000399142861865\n",
      "In model, X: (500, 4), b: 0.002761242196586025, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 277 Loss: 0.5000455974571577\n",
      "In model, X: (500, 4), b: 0.00257332216950929, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 278 Loss: 0.5000512942755277\n",
      "In model, X: (500, 4), b: 0.002385963850332297, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 279 Loss: 0.5000570041871522\n",
      "In model, X: (500, 4), b: 0.0021991655824792227, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 280 Loss: 0.5000627266503483\n",
      "In model, X: (500, 4), b: 0.002012925699337275, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 281 Loss: 0.5000684611356127\n",
      "In model, X: (500, 4), b: 0.0018272425247651486, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 282 Loss: 0.500074207125343\n",
      "In model, X: (500, 4), b: 0.0016421143735872634, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 283 Loss: 0.5000799641135706\n",
      "In model, X: (500, 4), b: 0.00145753955207416, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 284 Loss: 0.5000857316056956\n",
      "In model, X: (500, 4), b: 0.0012735163584094158, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 285 Loss: 0.500091509118231\n",
      "In model, X: (500, 4), b: 0.0010900430831434409, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 286 Loss: 0.5000972961785521\n",
      "In model, X: (500, 4), b: 0.0009071180096345018, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 287 Loss: 0.500103092324652\n",
      "In model, X: (500, 4), b: 0.0007247394144773032, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 288 Loss: 0.5001088971049027\n",
      "In model, X: (500, 4), b: 0.0005429055679194681, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 289 Loss: 0.5001147100778226\n",
      "In model, X: (500, 4), b: 0.00036161473426621723, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 290 Loss: 0.5001205308118488\n",
      "In model, X: (500, 4), b: 0.00018086517227358222, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 291 Loss: 0.5001263588851159\n",
      "In model, X: (500, 4), b: 6.551355304315054e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 292 Loss: 0.500132193885239\n",
      "In model, X: (500, 4), b: -0.00017901712717037933, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 293 Loss: 0.5001380354091018\n",
      "In model, X: (500, 4), b: -0.0003581533714714605, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 294 Loss: 0.5001438830626518\n",
      "In model, X: (500, 4), b: -0.0005367553571006515, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 295 Loss: 0.5001497364606969\n",
      "In model, X: (500, 4), b: -0.0007148248475308358, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 296 Loss: 0.5001555952267102\n",
      "In model, X: (500, 4), b: -0.0008923636096495385, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 297 Loss: 0.5001614589926373\n",
      "In model, X: (500, 4), b: -0.00106937341343805, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 298 Loss: 0.500167327398709\n",
      "In model, X: (500, 4), b: -0.0012458560316598235, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 299 Loss: 0.5001732000932578\n",
      "In model, X: (500, 4), b: -0.0014218132395579111, w: (4, 1)\n",
      "0.958\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.rand(X_train.shape[1],1)  # assuming X is N-by-n. \n",
    "                                        # if X is n-by-N, use X_train.shape[0]\n",
    "y_train = y_train.reshape((-1,1))\n",
    "y_test = y_test.reshape((-1,1))\n",
    "print(w.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "b = 0\n",
    "w, b, loss = train(w, b, X_train, y_train, iter=300, lr=0.1)\n",
    "plt.figure()\n",
    "plt.plot(loss)\n",
    "\n",
    "#training accuracy \n",
    "z = model(w,b,X_train)\n",
    "print(accuracy(np.squeeze(y_train), predict(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBm8ESACmrxe"
   },
   "source": [
    "To see how well our model performs, we compute its accuracy on the testing dataset (X_test, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pt9_Aiw-zqP6",
    "outputId": "d8131ac9-d8e8-4eaa-fc3a-891c9c36c22d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In model, X: (500, 4), b: -0.0014218132395579111, w: (4, 1)\n",
      "[0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0\n",
      " 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
      " 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0\n",
      " 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0\n",
      " 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0\n",
      " 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1\n",
      " 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0\n",
      " 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1\n",
      " 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1\n",
      " 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0\n",
      " 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1]\n",
      "[0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0\n",
      " 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
      " 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0\n",
      " 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1\n",
      " 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0\n",
      " 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
      " 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0\n",
      " 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1\n",
      " 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1\n",
      " 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0\n",
      " 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1]\n",
      "0.938\n"
     ]
    }
   ],
   "source": [
    "z = model(w,b,X_test)\n",
    "y_test=np.squeeze(y_test)\n",
    "print(y_test)\n",
    "print(predict(z))\n",
    "print(accuracy(y_test, predict(z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "id": "u-YnkECyDJXw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef5x5LENm7_s"
   },
   "source": [
    "Now, we look at a real-world dataset. [The Bank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#) is available at UCI's Machine Learning Repository. Colab can read this dataset directly from [GitHub](https://github.com/madmashup/targeted-marketing-predictive-engine) using pandas package: pd.read_csv. The data is in the DataFrame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5vKPwXfYgLV",
    "outputId": "81560798-2596-480a-adb4-6e8f5a5373d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41188, 21)\n",
      "['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/madmashup/targeted-marketing-predictive-engine/master/banking.csv'\n",
    "data = pd.read_csv(url)\n",
    "print(data.shape)\n",
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG9UWfJ8n2Jr"
   },
   "source": [
    "This dataset is pretty large and cause my machine to crash. I remove some fileds. [This Webpage](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8) has a good description of this dataset. Note that you are not allowed to use any existing model such as those used in that Webpage for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGiNyyIsvUw-",
    "outputId": "737ace96-0fb8-42f7-e2c1-839ec716f91a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'job', 'marital', 'housing', 'loan', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']\n",
      "(41188, 16)\n"
     ]
    }
   ],
   "source": [
    "cat_vars=['default','education','contact','month','day_of_week',]\n",
    "data=data.drop(cat_vars, axis=1)\n",
    "print(list(data.columns))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVSJRTDeoHNj"
   },
   "source": [
    "Some data columns have k class labels. This is best represented as k columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "du0e-Dhyg2FV",
    "outputId": "642aec70-0f26-4594-d8b7-40c8359a4c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job\n",
      "marital\n",
      "housing\n",
      "loan\n",
      "poutcome\n",
      "(41188, 36)\n",
      "['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y', 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown', 'divorced', 'married', 'single', 'unknown', 'no', 'unknown', 'yes', 'no', 'unknown', 'yes', 'failure', 'nonexistent', 'success']\n"
     ]
    }
   ],
   "source": [
    "cat_vars=['job','marital','housing','loan','poutcome']\n",
    "for va in cat_vars:\n",
    "    #cat_pre='var'+'_'+var\n",
    "    print(va)\n",
    "    #print(data[va])\n",
    "    cat_list = pd.get_dummies(data[va])\n",
    "    data1=pd.concat([data,cat_list], axis=1)\n",
    "    data=data1.drop(va, axis=1)\n",
    "    #print(list(cat_list.columns))\n",
    "    #print(list(data.columns))\n",
    "    #print(data.shape)\n",
    "\n",
    "print(data.shape)\n",
    "print(list(data.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NazsRFZmpIuD"
   },
   "source": [
    "We now split the data into input data X and the label y. We covert them to numpy and split them into training and testing datasets with 30% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2kDuXGHtBdB",
    "outputId": "29a222e0-282f-4f0f-b39d-faf86a3c4389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 35)\n",
      "(12357, 35)\n",
      "Index(['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate',\n",
      "       'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'admin.',\n",
      "       'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired',\n",
      "       'self-employed', 'services', 'student', 'technician', 'unemployed',\n",
      "       'unknown', 'divorced', 'married', 'single', 'unknown', 'no', 'unknown',\n",
      "       'yes', 'no', 'unknown', 'yes', 'failure', 'nonexistent', 'success'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = data.loc[:, data.columns != 'y']\n",
    "y = data.loc[:, data.columns == 'y']\n",
    "columns = X.columns\n",
    "X=X.to_numpy()\n",
    "y=y.to_numpy()\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train1.shape)\n",
    "print(X_test1.shape)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngDOmRz9pxyR"
   },
   "source": [
    "Now, train and test as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tHoIcRBXN_bG",
    "outputId": "689f0fc0-b690-4ff0-869d-f701ef0bb358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1)\n",
      "(28831, 35)\n",
      "(28831, 1)\n",
      ">> (28831, 35)\n",
      "In model, X: (28831, 35), b: 0, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 0 Loss: 3.146863253167685e-79\n",
      "In model, X: (28831, 35), b: -1.7735770524782358e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 1 Loss: 8.42657539213221\n",
      "In model, X: (28831, 35), b: -1.7274673000886378e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 2 Loss: 7.642467015235511\n",
      "In model, X: (28831, 35), b: -1.7058106598043474e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 3 Loss: 7.332911553223028\n",
      "In model, X: (28831, 35), b: -1.6947756649449875e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 4 Loss: 7.21927546503453\n",
      "In model, X: (28831, 35), b: -1.6874478195006025e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 5 Loss: 7.172920772176586\n",
      "In model, X: (28831, 35), b: -1.6812579766556027e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 6 Loss: 7.147072980157157\n",
      "In model, X: (28831, 35), b: -1.6754526239899624e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 7 Loss: 7.128136354865539\n",
      "In model, X: (28831, 35), b: -1.6697823224319584e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 8 Loss: 7.111625002275192\n",
      "In model, X: (28831, 35), b: -1.6641599694717183e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 9 Loss: 7.09597422664429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-492-9059b5902812>:21: RuntimeWarning: overflow encountered in exp\n",
      "  return 1./(1+np.exp(t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In model, X: (28831, 35), b: -1.6585547032040838e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 10 Loss: 7.080629790018967\n",
      "In model, X: (28831, 35), b: -1.6529555759575412e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 11 Loss: 7.065395046871973\n",
      "In model, X: (28831, 35), b: -1.6473587249419428e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 12 Loss: 7.05020054113904\n",
      "In model, X: (28831, 35), b: -1.6417628055518412e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 13 Loss: 7.03502201236364\n",
      "In model, X: (28831, 35), b: -1.636167365873066e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 14 Loss: 7.019851212192407\n",
      "In model, X: (28831, 35), b: -1.6305722695560846e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 15 Loss: 7.004685542976589\n",
      "In model, X: (28831, 35), b: -1.6249774913829837e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 16 Loss: 6.989524387820282\n",
      "In model, X: (28831, 35), b: -1.6193830447438215e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 17 Loss: 6.974367809388063\n",
      "In model, X: (28831, 35), b: -1.6137889555366422e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 18 Loss: 6.9592160828279\n",
      "In model, X: (28831, 35), b: -1.6081952522999126e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 19 Loss: 6.944069520822586\n",
      "In model, X: (28831, 35), b: -1.6026019620064127e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 20 Loss: 6.928928400973471\n",
      "In model, X: (28831, 35), b: -1.5970091078893025e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 21 Loss: 6.9137929302769\n",
      "In model, X: (28831, 35), b: -1.591416708120812e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 22 Loss: 6.898663225355655\n",
      "In model, X: (28831, 35), b: -1.5858247750204883e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 23 Loss: 6.883539302405603\n",
      "In model, X: (28831, 35), b: -1.58023331475264e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 24 Loss: 6.868421075830352\n",
      "In model, X: (28831, 35), b: -1.574642327538793e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 25 Loss: 6.8533083656928495\n",
      "In model, X: (28831, 35), b: -1.5690518083886468e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 26 Loss: 6.838200913723529\n",
      "In model, X: (28831, 35), b: -1.5634617482943266e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 27 Loss: 6.823098406631701\n",
      "In model, X: (28831, 35), b: -1.5578721357665294e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 28 Loss: 6.808000504382182\n",
      "In model, X: (28831, 35), b: -1.552282958538418e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 29 Loss: 6.79290687028452\n",
      "In model, X: (28831, 35), b: -1.5466942052391604e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 30 Loss: 6.777817199448132\n",
      "In model, X: (28831, 35), b: -1.5411058668512677e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 31 Loss: 6.762731242492072\n",
      "In model, X: (28831, 35), b: -1.535517937812506e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 32 Loss: 6.747648822307926\n",
      "In model, X: (28831, 35), b: -1.5299304166935017e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 33 Loss: 6.732569842956689\n",
      "In model, X: (28831, 35), b: -1.5243433064602206e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 34 Loss: 6.717494291148384\n",
      "In model, X: (28831, 35), b: -1.5187566143992281e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 35 Loss: 6.702422231911885\n",
      "In model, X: (28831, 35), b: -1.5131703518293986e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 36 Loss: 6.687353800787132\n",
      "In model, X: (28831, 35), b: -1.5075845337392007e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 37 Loss: 6.672289195051869\n",
      "In model, X: (28831, 35), b: -1.5019991784737757e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 38 Loss: 6.657228666148657\n",
      "In model, X: (28831, 35), b: -1.496414307556927e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 39 Loss: 6.642172514729516\n",
      "In model, X: (28831, 35), b: -1.4908299456802154e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 40 Loss: 6.627121088776296\n",
      "In model, X: (28831, 35), b: -1.4852461208361672e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 41 Loss: 6.612074784290585\n",
      "In model, X: (28831, 35), b: -1.4796628645253744e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 42 Loss: 6.597034047253068\n",
      "In model, X: (28831, 35), b: -1.474080211934952e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 43 Loss: 6.581999375041025\n",
      "In model, X: (28831, 35), b: -1.4684982019710892e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 44 Loss: 6.566971315302137\n",
      "In model, X: (28831, 35), b: -1.4629168770306593e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 45 Loss: 6.5519504603889285\n",
      "In model, X: (28831, 35), b: -1.4573362824132459e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 46 Loss: 6.536937435804642\n",
      "In model, X: (28831, 35), b: -1.4517564653030841e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 47 Loss: 6.521932881647369\n",
      "In model, X: (28831, 35), b: -1.4461774732896355e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 48 Loss: 6.506937426746601\n",
      "In model, X: (28831, 35), b: -1.440599352447122e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 49 Loss: 6.49195165608238\n",
      "In model, X: (28831, 35), b: -1.4350221450588261e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 50 Loss: 6.476976073179407\n",
      "In model, X: (28831, 35), b: -1.4294458871489049e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 51 Loss: 6.462011060429069\n",
      "In model, X: (28831, 35), b: -1.4238706060623213e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 52 Loss: 6.447056841524364\n",
      "In model, X: (28831, 35), b: -1.4182963183913982e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 53 Loss: 6.432113451039459\n",
      "In model, X: (28831, 35), b: -1.4127230285574127e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 54 Loss: 6.417180716179761\n",
      "In model, X: (28831, 35), b: -1.4071507282925784e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 55 Loss: 6.402258254472258\n",
      "In model, X: (28831, 35), b: -1.4015793971247286e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 56 Loss: 6.387345488584371\n",
      "In model, X: (28831, 35), b: -1.396009003767516e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 57 Loss: 6.372441676004066\n",
      "In model, X: (28831, 35), b: -1.3904395081181616e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 58 Loss: 6.3575459479631355\n",
      "In model, X: (28831, 35), b: -1.3848708634333684e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 59 Loss: 6.342657349948371\n",
      "In model, X: (28831, 35), b: -1.379303018249082e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 60 Loss: 6.327774876354215\n",
      "In model, X: (28831, 35), b: -1.373735917745829e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 61 Loss: 6.312897494452906\n",
      "In model, X: (28831, 35), b: -1.3681695044976897e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 62 Loss: 6.298024157105188\n",
      "In model, X: (28831, 35), b: -1.362603718795997e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 63 Loss: 6.283153808009454\n",
      "In model, X: (28831, 35), b: -1.3570384989151958e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 64 Loss: 6.268285386176118\n",
      "In model, X: (28831, 35), b: -1.3514737817238715e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 65 Loss: 6.25341783666389\n",
      "In model, X: (28831, 35), b: -1.3459095039305872e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 66 Loss: 6.238550132376388\n",
      "In model, X: (28831, 35), b: -1.3403456040425628e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 67 Loss: 6.223681307854995\n",
      "In model, X: (28831, 35), b: -1.3347820248911522e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 68 Loss: 6.208810502059812\n",
      "In model, X: (28831, 35), b: -1.3292187164246974e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 69 Loss: 6.193937004547308\n",
      "In model, X: (28831, 35), b: -1.3236556384321548e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 70 Loss: 6.1790602989779675\n",
      "In model, X: (28831, 35), b: -1.3180927629331602e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 71 Loss: 6.164180099337816\n",
      "In model, X: (28831, 35), b: -1.3125300761035345e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 72 Loss: 6.149296376735384\n",
      "In model, X: (28831, 35), b: -1.3069675797368883e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 73 Loss: 6.13440937702001\n",
      "In model, X: (28831, 35), b: -1.3014052923243663e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 74 Loss: 6.119519630915054\n",
      "In model, X: (28831, 35), b: -1.2958432498468688e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 75 Loss: 6.10462795856534\n",
      "In model, X: (28831, 35), b: -1.2902815063271373e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 76 Loss: 6.089735469564873\n",
      "In model, X: (28831, 35), b: -1.2847201341091581e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 77 Loss: 6.074843558138401\n",
      "In model, X: (28831, 35), b: -1.2791592237474023e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 78 Loss: 6.059953891699456\n",
      "In model, X: (28831, 35), b: -1.2735988833189144e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 79 Loss: 6.045068389854079\n",
      "In model, X: (28831, 35), b: -1.2680392369285177e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 80 Loss: 6.030189190254199\n",
      "In model, X: (28831, 35), b: -1.2624804221678257e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 81 Loss: 6.015318597627558\n",
      "In model, X: (28831, 35), b: -1.2569225863198225e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 82 Loss: 6.000459012935138\n",
      "In model, X: (28831, 35), b: -1.2513658811840333e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 83 Loss: 5.985612841111827\n",
      "In model, X: (28831, 35), b: -1.245810456545981e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 84 Loss: 5.970782378430761\n",
      "In model, X: (28831, 35), b: -1.2402564525343693e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 85 Loss: 5.955969684260223\n",
      "In model, X: (28831, 35), b: -1.2347039913850114e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 86 Loss: 5.941176446555538\n",
      "In model, X: (28831, 35), b: -1.2291531694139811e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 87 Loss: 5.92640385499738\n",
      "In model, X: (28831, 35), b: -1.2236040502119231e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 88 Loss: 5.911652498848475\n",
      "In model, X: (28831, 35), b: -1.2180566601092064e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 89 Loss: 5.896922306723625\n",
      "In model, X: (28831, 35), b: -1.2125109867519964e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 90 Loss: 5.8822125413581094\n",
      "In model, X: (28831, 35), b: -1.2069669811640535e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 91 Loss: 5.867521854111062\n",
      "In model, X: (28831, 35), b: -1.2014245630345284e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 92 Loss: 5.85284839297169\n",
      "In model, X: (28831, 35), b: -1.1958836283368557e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 93 Loss: 5.838189947212998\n",
      "In model, X: (28831, 35), b: -1.1903440579429315e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 94 Loss: 5.823544104873602\n",
      "In model, X: (28831, 35), b: -1.1848057257956937e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 95 Loss: 5.808908398302224\n",
      "In model, X: (28831, 35), b: -1.1792685054797467e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 96 Loss: 5.794280418508048\n",
      "In model, X: (28831, 35), b: -1.1737322745984141e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 97 Loss: 5.779657889347797\n",
      "In model, X: (28831, 35), b: -1.1681969170488265e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 98 Loss: 5.76503870445653\n",
      "In model, X: (28831, 35), b: -1.1626623238745184e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 99 Loss: 5.750420939666539\n",
      "In model, X: (28831, 35), b: -1.1571283936924872e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 100 Loss: 5.73580285854875\n",
      "In model, X: (28831, 35), b: -1.1515950336541395e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 101 Loss: 5.721182927387678\n",
      "In model, X: (28831, 35), b: -1.1460621615408267e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 102 Loss: 5.706559849162803\n",
      "In model, X: (28831, 35), b: -1.1405297090558087e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 103 Loss: 5.691932616560461\n",
      "In model, X: (28831, 35), b: -1.134997625850913e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 104 Loss: 5.6773005751209515\n",
      "In model, X: (28831, 35), b: -1.1294658834903655e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 105 Loss: 5.662663482258204\n",
      "In model, X: (28831, 35), b: -1.1239344784894684e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 106 Loss: 5.648021547360833\n",
      "In model, X: (28831, 35), b: -1.1184034337438007e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 107 Loss: 5.633375441824226\n",
      "In model, X: (28831, 35), b: -1.1128727979771499e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 108 Loss: 5.618726273659807\n",
      "In model, X: (28831, 35), b: -1.1073426431568405e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 109 Loss: 5.604075527029199\n",
      "In model, X: (28831, 35), b: -1.1018130600640932e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 110 Loss: 5.589424971139907\n",
      "In model, X: (28831, 35), b: -1.096284152336729e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 111 Loss: 5.5747765450132585\n",
      "In model, X: (28831, 35), b: -1.0907560293454811e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 112 Loss: 5.560132225197256\n",
      "In model, X: (28831, 35), b: -1.0852287982698317e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 113 Loss: 5.545493883419482\n",
      "In model, X: (28831, 35), b: -1.07970255574542e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 114 Loss: 5.530863141191101\n",
      "In model, X: (28831, 35), b: -1.0741773794835696e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 115 Loss: 5.516241228824976\n",
      "In model, X: (28831, 35), b: -1.0686533203210008e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 116 Loss: 5.501628857275065\n",
      "In model, X: (28831, 35), b: -1.063130395255842e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 117 Loss: 5.487026112749076\n",
      "In model, X: (28831, 35), b: -1.057608582190911e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 118 Loss: 5.472432386537228\n",
      "In model, X: (28831, 35), b: -1.0520878173464192e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 119 Loss: 5.457846355997211\n",
      "In model, X: (28831, 35), b: -1.0465679965330861e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 120 Loss: 5.443266035560456\n",
      "In model, X: (28831, 35), b: -1.041048981440681e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 121 Loss: 5.428688914722633\n",
      "In model, X: (28831, 35), b: -1.0355306114605444e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 122 Loss: 5.414112187805654\n",
      "In model, X: (28831, 35), b: -1.0300127201895496e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 123 Loss: 5.399533056096338\n",
      "In model, X: (28831, 35), b: -1.0244951540300184e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 124 Loss: 5.384949053789775\n",
      "In model, X: (28831, 35), b: -1.0189777890502296e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 125 Loss: 5.370358329746765\n",
      "In model, X: (28831, 35), b: -1.0134605423063138e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 126 Loss: 5.35575982074697\n",
      "In model, X: (28831, 35), b: -1.0079433752917404e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 127 Loss: 5.341153280087379\n",
      "In model, X: (28831, 35), b: -1.0024262894161246e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 128 Loss: 5.326539165612565\n",
      "In model, X: (28831, 35), b: -9.969093154290192e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 129 Loss: 5.31191842599019\n",
      "In model, X: (28831, 35), b: -9.913924998053024e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 130 Loss: 5.29729224160778\n",
      "In model, X: (28831, 35), b: -9.858758911864208e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 131 Loss: 5.282661775688698\n",
      "In model, X: (28831, 35), b: -9.803595293361208e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 132 Loss: 5.268027978186593\n",
      "In model, X: (28831, 35), b: -9.748434381236915e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 133 Loss: 5.253391467129609\n",
      "In model, X: (28831, 35), b: -9.693276230757052e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 134 Loss: 5.238752494381738\n",
      "In model, X: (28831, 35), b: -9.638120731775044e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 135 Loss: 5.2241109876139795\n",
      "In model, X: (28831, 35), b: -9.582967659279952e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 136 Loss: 5.2094666487233345\n",
      "In model, X: (28831, 35), b: -9.527816742205525e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 137 Loss: 5.194819082008566\n",
      "In model, X: (28831, 35), b: -9.472667735003546e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 138 Loss: 5.1801679240458505\n",
      "In model, X: (28831, 35), b: -9.417520478489751e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 139 Loss: 5.165512951513431\n",
      "In model, X: (28831, 35), b: -9.362374941036392e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 140 Loss: 5.150854151872466\n",
      "In model, X: (28831, 35), b: -9.307231236923426e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 141 Loss: 5.136191752275659\n",
      "In model, X: (28831, 35), b: -9.252089623975615e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 142 Loss: 5.121526211507509\n",
      "In model, X: (28831, 35), b: -9.196950486292413e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 143 Loss: 5.106858186064225\n",
      "In model, X: (28831, 35), b: -9.141814309418812e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 144 Loss: 5.092188483864988\n",
      "In model, X: (28831, 35), b: -9.086681654878292e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 145 Loss: 5.077518017957378\n",
      "In model, X: (28831, 35), b: -9.03155313916306e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 146 Loss: 5.06284776903855\n",
      "In model, X: (28831, 35), b: -8.976429419716957e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 147 Loss: 5.048178760898471\n",
      "In model, X: (28831, 35), b: -8.921311187714369e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 148 Loss: 5.033512048025821\n",
      "In model, X: (28831, 35), b: -8.866199164934691e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 149 Loss: 5.018848710311028\n",
      "In model, X: (28831, 35), b: -8.811094100042007e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 150 Loss: 5.004189846518797\n",
      "In model, X: (28831, 35), b: -8.755996758379434e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 151 Loss: 4.989536556432315\n",
      "In model, X: (28831, 35), b: -8.700907899362888e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 152 Loss: 4.9748899019106245\n",
      "In model, X: (28831, 35), b: -8.645828237257787e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 153 Loss: 4.960250840419959\n",
      "In model, X: (28831, 35), b: -8.59075838511198e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 154 Loss: 4.945620131763841\n",
      "In model, X: (28831, 35), b: -8.535698788003543e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 155 Loss: 4.9309982297665895\n",
      "In model, X: (28831, 35), b: -8.480649659417257e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 156 Loss: 4.916385183444132\n",
      "In model, X: (28831, 35), b: -8.42561094056693e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 157 Loss: 4.9017805815858955\n",
      "In model, X: (28831, 35), b: -8.370582302692409e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 158 Loss: 4.887183573643451\n",
      "In model, X: (28831, 35), b: -8.315563203755303e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 159 Loss: 4.8725929835809705\n",
      "In model, X: (28831, 35), b: -8.260552994747979e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 160 Loss: 4.85800750463222\n",
      "In model, X: (28831, 35), b: -8.205551053585172e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 161 Loss: 4.843425933619309\n",
      "In model, X: (28831, 35), b: -8.150556915034891e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 162 Loss: 4.828847388799296\n",
      "In model, X: (28831, 35), b: -8.095570368397055e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 163 Loss: 4.814271463188119\n",
      "In model, X: (28831, 35), b: -8.040591508205382e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 164 Loss: 4.7996982907681\n",
      "In model, X: (28831, 35), b: -7.985620739123e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 165 Loss: 4.785128531374148\n",
      "In model, X: (28831, 35), b: -7.930658746666194e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 166 Loss: 4.77056329784932\n",
      "In model, X: (28831, 35), b: -7.87570644742675e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 167 Loss: 4.756004051539148\n",
      "In model, X: (28831, 35), b: -7.820764928147937e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 168 Loss: 4.741452483597882\n",
      "In model, X: (28831, 35), b: -7.76583537670438e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 169 Loss: 4.726910387921519\n",
      "In model, X: (28831, 35), b: -7.710919003431655e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 170 Loss: 4.712379523320535\n",
      "In model, X: (28831, 35), b: -7.656016950076384e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 171 Loss: 4.69786146064941\n",
      "In model, X: (28831, 35), b: -7.601130185987795e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 172 Loss: 4.683357414914114\n",
      "In model, X: (28831, 35), b: -7.546259396379964e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 173 Loss: 4.668868071429883\n",
      "In model, X: (28831, 35), b: -7.491404874521424e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 174 Loss: 4.654393426837966\n",
      "In model, X: (28831, 35), b: -7.436566436751811e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 175 Loss: 4.639932676912196\n",
      "In model, X: (28831, 35), b: -7.381743383164054e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 176 Loss: 4.625484188215892\n",
      "In model, X: (28831, 35), b: -7.326934523739306e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 177 Loss: 4.6110455834382815\n",
      "In model, X: (28831, 35), b: -7.272138277426187e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 178 Loss: 4.596613947470522\n",
      "In model, X: (28831, 35), b: -7.217352832516137e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 179 Loss: 4.582186127980145\n",
      "In model, X: (28831, 35), b: -7.162576338292856e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 180 Loss: 4.567759074069031\n",
      "In model, X: (28831, 35), b: -7.107807089251062e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 181 Loss: 4.553330144694608\n",
      "In model, X: (28831, 35), b: -7.0530436683905505e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 182 Loss: 4.5388973310958045\n",
      "In model, X: (28831, 35), b: -6.998285031750622e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 183 Loss: 4.52445936725084\n",
      "In model, X: (28831, 35), b: -6.9435305342002866e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 184 Loss: 4.510015734323967\n",
      "In model, X: (28831, 35), b: -6.888779909199845e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 185 Loss: 4.495566586452265\n",
      "In model, X: (28831, 35), b: -6.834033220066636e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 186 Loss: 4.481112631981275\n",
      "In model, X: (28831, 35), b: -6.779290798784061e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 187 Loss: 4.46665499986736\n",
      "In model, X: (28831, 35), b: -6.7245531839061966e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 188 Loss: 4.452195111705695\n",
      "In model, X: (28831, 35), b: -6.669821064438836e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 189 Loss: 4.437734570780119\n",
      "In model, X: (28831, 35), b: -6.6150952331764755e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 190 Loss: 4.423275073140357\n",
      "In model, X: (28831, 35), b: -6.560376551166563e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 191 Loss: 4.408818342397429\n",
      "In model, X: (28831, 35), b: -6.50566592448211e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 192 Loss: 4.39436608905237\n",
      "In model, X: (28831, 35), b: -6.450964294828512e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 193 Loss: 4.379919995776071\n",
      "In model, X: (28831, 35), b: -6.396272646145747e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 194 Loss: 4.3654817311234275\n",
      "In model, X: (28831, 35), b: -6.3415920296797085e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 195 Loss: 4.351052994606612\n",
      "In model, X: (28831, 35), b: -6.2869236092512345e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 196 Loss: 4.336635594623164\n",
      "In model, X: (28831, 35), b: -6.2322687257722725e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 197 Loss: 4.322231556000722\n",
      "In model, X: (28831, 35), b: -6.177628974505305e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 198 Loss: 4.307843244403809\n",
      "In model, X: (28831, 35), b: -6.123006279418213e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 199 Loss: 4.293473479533083\n",
      "In model, X: (28831, 35), b: -6.0684029365244945e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 200 Loss: 4.279125588696642\n",
      "In model, X: (28831, 35), b: -6.0138215852143414e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 201 Loss: 4.264803332252152\n",
      "In model, X: (28831, 35), b: -5.959265061279548e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 202 Loss: 4.250510626559455\n",
      "In model, X: (28831, 35), b: -5.904736102042946e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 203 Loss: 4.236251022874278\n",
      "In model, X: (28831, 35), b: -5.85023692793517e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 204 Loss: 4.222026996566906\n",
      "In model, X: (28831, 35), b: -5.795768812975512e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 205 Loss: 4.207839251602767\n",
      "In model, X: (28831, 35), b: -5.741331833087885e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 206 Loss: 4.193686367894311\n",
      "In model, X: (28831, 35), b: -5.6869249646683506e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 207 Loss: 4.179565074128326\n",
      "In model, X: (28831, 35), b: -5.632546552009231e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 208 Loss: 4.1654711478407025\n",
      "In model, X: (28831, 35), b: -5.578194946837981e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 209 Loss: 4.151400571344703\n",
      "In model, X: (28831, 35), b: -5.523869008575867e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 210 Loss: 4.137350392255217\n",
      "In model, X: (28831, 35), b: -5.469568235594999e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 211 Loss: 4.1233189062016455\n",
      "In model, X: (28831, 35), b: -5.415292512929322e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 212 Loss: 4.109305170118885\n",
      "In model, X: (28831, 35), b: -5.361041661004667e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 213 Loss: 4.095308197226489\n",
      "In model, X: (28831, 35), b: -5.306815042498799e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 214 Loss: 4.081326291401836\n",
      "In model, X: (28831, 35), b: -5.252611417631382e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 215 Loss: 4.0673568400114\n",
      "In model, X: (28831, 35), b: -5.198429097967059e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 216 Loss: 4.053396627342062\n",
      "In model, X: (28831, 35), b: -5.144266318288521e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 217 Loss: 4.0394425044155176\n",
      "In model, X: (28831, 35), b: -5.090121672479405e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 218 Loss: 4.025492133735757\n",
      "In model, X: (28831, 35), b: -5.035994454380067e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 219 Loss: 4.011544533267969\n",
      "In model, X: (28831, 35), b: -4.981884806359895e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 220 Loss: 3.997600263377485\n",
      "In model, X: (28831, 35), b: -4.927793679049058e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 221 Loss: 3.9836612804471128\n",
      "In model, X: (28831, 35), b: -4.873722682520817e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 222 Loss: 3.969730610760555\n",
      "In model, X: (28831, 35), b: -4.819673912171813e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 223 Loss: 3.9558119948461914\n",
      "In model, X: (28831, 35), b: -4.765649775862999e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 224 Loss: 3.941909547604035\n",
      "In model, X: (28831, 35), b: -4.711652794650086e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 225 Loss: 3.928027385326512\n",
      "In model, X: (28831, 35), b: -4.657685346241266e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 226 Loss: 3.9141691697050445\n",
      "In model, X: (28831, 35), b: -4.6037493667724045e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 227 Loss: 3.900337603009044\n",
      "In model, X: (28831, 35), b: -4.549846084005962e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 228 Loss: 3.8865340063445077\n",
      "In model, X: (28831, 35), b: -4.495975881954175e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 229 Loss: 3.872758151552765\n",
      "In model, X: (28831, 35), b: -4.442138372404049e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 230 Loss: 3.8590084643958695\n",
      "In model, X: (28831, 35), b: -4.388332680382468e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 231 Loss: 3.8452825904998362\n",
      "In model, X: (28831, 35), b: -4.3345578695847495e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 232 Loss: 3.8315781747221136\n",
      "In model, X: (28831, 35), b: -4.2808133795366505e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 233 Loss: 3.817893618379807\n",
      "In model, X: (28831, 35), b: -4.227099343366933e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 234 Loss: 3.80422858680984\n",
      "In model, X: (28831, 35), b: -4.17341669941334e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 235 Loss: 3.7905841296087694\n",
      "In model, X: (28831, 35), b: -4.119767074970122e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 236 Loss: 3.776962397060873\n",
      "In model, X: (28831, 35), b: -4.066152477290889e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 237 Loss: 3.7633660366109476\n",
      "In model, X: (28831, 35), b: -4.012574861093321e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 238 Loss: 3.749797408873155\n",
      "In model, X: (28831, 35), b: -3.959035657827488e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 239 Loss: 3.7362577836513613\n",
      "In model, X: (28831, 35), b: -3.905535363236249e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 240 Loss: 3.7227466872852126\n",
      "In model, X: (28831, 35), b: -3.8520732884237134e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 241 Loss: 3.7092615770766035\n",
      "In model, X: (28831, 35), b: -3.798647563072711e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 242 Loss: 3.695797977728069\n",
      "In model, X: (28831, 35), b: -3.74525540837147e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 243 Loss: 3.682350081664706\n",
      "In model, X: (28831, 35), b: -3.691893584814925e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 244 Loss: 3.6689116193974485\n",
      "In model, X: (28831, 35), b: -3.638558841602941e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 245 Loss: 3.6554766820475635\n",
      "In model, X: (28831, 35), b: -3.585248223745917e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 246 Loss: 3.642040248447944\n",
      "In model, X: (28831, 35), b: -3.5319592155667576e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 247 Loss: 3.6285983959570793\n",
      "In model, X: (28831, 35), b: -3.4786898152525347e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 248 Loss: 3.6151483753969726\n",
      "In model, X: (28831, 35), b: -3.4254386534492114e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 249 Loss: 3.601688751445317\n",
      "In model, X: (28831, 35), b: -3.3722051853079026e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 250 Loss: 3.588219654411955\n",
      "In model, X: (28831, 35), b: -3.3189898770009376e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 251 Loss: 3.574743001161514\n",
      "In model, X: (28831, 35), b: -3.2657942632530873e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 252 Loss: 3.561262476243561\n",
      "In model, X: (28831, 35), b: -3.212620800416665e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 253 Loss: 3.5477831601893324\n",
      "In model, X: (28831, 35), b: -3.1594725362937763e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 254 Loss: 3.5343108650186097\n",
      "In model, X: (28831, 35), b: -3.106352697619731e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 255 Loss: 3.520851369874491\n",
      "In model, X: (28831, 35), b: -3.0532643209404346e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 256 Loss: 3.5074097799469355\n",
      "In model, X: (28831, 35), b: -3.000210018029179e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 257 Loss: 3.493990158144086\n",
      "In model, X: (28831, 35), b: -2.947191889583087e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 258 Loss: 3.480595436283622\n",
      "In model, X: (28831, 35), b: -2.89421151924586e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 259 Loss: 3.467227471572639\n",
      "In model, X: (28831, 35), b: -2.8412699483487693e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 260 Loss: 3.453887068348748\n",
      "In model, X: (28831, 35), b: -2.788367581263291e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 261 Loss: 3.440573881974703\n",
      "In model, X: (28831, 35), b: -2.7355040700277124e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 262 Loss: 3.4272862971390126\n",
      "In model, X: (28831, 35), b: -2.6826782936719403e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 263 Loss: 3.4140214813088603\n",
      "In model, X: (28831, 35), b: -2.629888516475332e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 264 Loss: 3.4007757464149244\n",
      "In model, X: (28831, 35), b: -2.5771326913428653e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 265 Loss: 3.3875451388804425\n",
      "In model, X: (28831, 35), b: -2.5244087620689296e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 266 Loss: 3.374325988903564\n",
      "In model, X: (28831, 35), b: -2.471714810903812e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 267 Loss: 3.3611151540145783\n",
      "In model, X: (28831, 35), b: -2.4190490059970537e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 268 Loss: 3.3479098945965116\n",
      "In model, X: (28831, 35), b: -2.3664094411692005e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 269 Loss: 3.3347075603300134\n",
      "In model, X: (28831, 35), b: -2.313794034065136e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 270 Loss: 3.321505383542138\n",
      "In model, X: (28831, 35), b: -2.261200621339237e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 271 Loss: 3.3083006137759443\n",
      "In model, X: (28831, 35), b: -2.208627276440578e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 272 Loss: 3.2950910240002598\n",
      "In model, X: (28831, 35), b: -2.1560727316269317e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 273 Loss: 3.2818755729037514\n",
      "In model, X: (28831, 35), b: -2.103536696040038e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 274 Loss: 3.268654865859973\n",
      "In model, X: (28831, 35), b: -2.051019888579263e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 275 Loss: 3.2554311204434936\n",
      "In model, X: (28831, 35), b: -1.9985237311479807e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 276 Loss: 3.2422075714090077\n",
      "In model, X: (28831, 35), b: -1.9460497977972073e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 277 Loss: 3.2289875050112524\n",
      "In model, X: (28831, 35), b: -1.893599218438271e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 278 Loss: 3.2157732737307723\n",
      "In model, X: (28831, 35), b: -1.8411722582797282e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 279 Loss: 3.202565659273933\n",
      "In model, X: (28831, 35), b: -1.7887682288152305e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 280 Loss: 3.189363821503678\n",
      "In model, X: (28831, 35), b: -1.736385756747444e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 281 Loss: 3.1761658417612444\n",
      "In model, X: (28831, 35), b: -1.6840233061525053e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 282 Loss: 3.1629696506000258\n",
      "In model, X: (28831, 35), b: -1.63167978302991e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 283 Loss: 3.1497740330243724\n",
      "In model, X: (28831, 35), b: -1.57935506853499e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 284 Loss: 3.136579452508816\n",
      "In model, X: (28831, 35), b: -1.5270503953354127e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 285 Loss: 3.123388565084061\n",
      "In model, X: (28831, 35), b: -1.4747685568091267e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 286 Loss: 3.1102064278293886\n",
      "In model, X: (28831, 35), b: -1.422513988649933e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 287 Loss: 3.0970404875679236\n",
      "In model, X: (28831, 35), b: -1.3702927657360908e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 288 Loss: 3.0839004333376794\n",
      "In model, X: (28831, 35), b: -1.318112511183532e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 289 Loss: 3.0707979112594845\n",
      "In model, X: (28831, 35), b: -1.2659821475592952e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 290 Loss: 3.0577459862912986\n",
      "In model, X: (28831, 35), b: -1.2139113845509402e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 291 Loss: 3.044758183738102\n",
      "In model, X: (28831, 35), b: -1.1619098823956623e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 292 Loss: 3.031847035293426\n",
      "In model, X: (28831, 35), b: -1.1099861766978887e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 293 Loss: 3.019022315108805\n",
      "In model, X: (28831, 35), b: -1.058146665675435e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 294 Loss: 3.0062895114342827\n",
      "In model, X: (28831, 35), b: -1.0063951232232796e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 295 Loss: 2.993649317905602\n",
      "In model, X: (28831, 35), b: -9.547331012435964e-07, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 296 Loss: 2.981098693561246\n",
      "In model, X: (28831, 35), b: -9.031611121124391e-07, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 297 Loss: 2.9686331830109833\n",
      "In model, X: (28831, 35), b: -8.516799313861055e-07, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 298 Loss: 2.9562492672952216\n",
      "In model, X: (28831, 35), b: -8.002913060856965e-07, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 299 Loss: 2.9439455189078974\n",
      "In model, X: (28831, 35), b: -7.489979180849505e-07, w: (35, 1)\n",
      "0.8739551177551941\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = np.random.rand(X_train1.shape[1],1)  # assuming X is N-by-n. \n",
    "                                        # if X is n-by-N, use X_train.shape[0]\n",
    "y_train1 = y_train1.reshape(-1,1)\n",
    "y_test1 = y_test1.reshape(-1,1)\n",
    "print(w1.shape)\n",
    "print(X_train1.shape)\n",
    "print(y_train1.shape)\n",
    "b1 = 0\n",
    "w1, b1, loss1 = train(w1, b1, X_train1, y_train1, iter=300, lr=2e-5)\n",
    "plt.figure()\n",
    "plt.plot(loss1)\n",
    "\n",
    "#training accuracy \n",
    "z1 = model(w1,b1,X_train1)\n",
    "print(accuracy(np.squeeze(y_train1), predict(z1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBqSssY6OUGJ",
    "outputId": "6b1f588d-1d6c-4dda-bdbd-2f9fc17d40ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In model, X: (12357, 35), b: -7.489979180849505e-07, w: (35, 1)\n",
      "0.8730274338431658\n"
     ]
    }
   ],
   "source": [
    "z1 = model(w1,b1,X_test1)\n",
    "y_test1=np.squeeze(y_test1)\n",
    "print(accuracy(y_test1, predict(z1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "id": "fsI9xje4Oase"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
