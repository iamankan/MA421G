{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnzRlaQaW6y1"
   },
   "source": [
    "In this homework, you will write a python implementation of logistic regression. You will test it on two datasets. \n",
    "First we import some libraries that we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Szla9qyoPuqg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0byO4vq0Xcxz"
   },
   "source": [
    "We define some functions involved. Use the formulations that avoid overflows.  \n",
    "1. sigmoid function sigmoid(t)\n",
    "2. log of sigmoid(t), called log_sig(t)\n",
    "3. log of 1-sigmoid = 1/(1+e^t), called log_one_sig(t)\n",
    "4. cross-entropy loss function given the inputs of label y and prediction y_hat = sigmoid(z), where y, y_hat, and z are vectors of dimension N. (N = # of data points.) You should implement this function with z, rather than y_hat, as the input; namely, the loss function should be\n",
    "\n",
    "    loss = -y log(sigmoid(z)) - (1-y) log (1-sigmoid(z)) \n",
    "\n",
    "  where log(sigmoid(z)) and log (1-sigmoid(z)) should be computed by the functions log_sig(z) and log_one_sig(z) in parts 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "id": "kuzmD54GT9yb"
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "  return 1/(1+np.exp(-t))\n",
    "\n",
    "def customloss(y, z): \n",
    "  # loss function for y and yhat = sigmoid(z)\n",
    "  return (-(y*log_sig(z)) - ((1-y)*log_one_sig(z)))\n",
    "\n",
    "# def log_sig(t):\n",
    "#   return np.log(sigmoid(t))\n",
    "\n",
    "def log_sig(z):\n",
    "  # return np.log(sigmoid(z))\n",
    "  if np.isnan(z).any():\n",
    "    print('z is nan in loss')\n",
    "  if (z <= 0).any():\n",
    "        return z - np.log(1+np.exp(z))\n",
    "  else:\n",
    "        return -np.log(1+np.exp(-z))\n",
    "\n",
    "def log_one_sig(t):\n",
    "  return 1./(1+np.exp(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "We0D7AEdfn_3",
    "outputId": "6d5b4cf9-5227-452a-d8bd-9455816376c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-310-47a6eab891c2>:1: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RSaRWNd6Y6h2",
    "outputId": "cbe202e6-e4af-43c5-ac93-4c1a3fe4e29e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_one_sig([float('nan')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FT0pO4uAY_nY",
    "outputId": "ef5e296d-cba8-4b18-c6c3-b153a613a67a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_sig(np.array([float('nan')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLulJqXcbEpw"
   },
   "source": [
    "Define the model output z=w^T x + b, or z = x^Tw + B, given the data input X (an N-by-n array containing N data points) and the model parameters w (n-dimensional weigth vector) and b (bias).\n",
    "\n",
    "Note that mathematically it's easier to write the data matrix as an n-by-N matrix, with each column being a data point. In python, the data is more commonly represented as as an N-by-n array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "id": "eI9PNMZnhy0d"
   },
   "outputs": [],
   "source": [
    "def model(w,b,X):\n",
    "  # using X as Nxn\n",
    "  print(f'In model, X: {X.shape}, b: {b}, w: {w.shape}')\n",
    "  return (X @ w)+b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv_xi0ajaEEY"
   },
   "source": [
    "Define the function that computes the gradient of the cross-entropy loss given the label y (N-vector), the model prediction y_hat = sigmoid(z) (N-vector), and the dataset X (an n-by-N or N-by-n array). It's probably easier to return the gradients with respect w and b separately, which can be used to update w and b later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "id": "I8KJF8lrZlFi"
   },
   "outputs": [],
   "source": [
    "def gradients(X, y, y_hat):\n",
    "  # Using X as Nxn\n",
    "  # print(f'grad: y shape: {y.shape}, X shape: {X.shape}')\n",
    "  return (np.transpose(X) @ (y_hat - y))/X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgqML9T2cOqd"
   },
   "source": [
    "Write the function that minimizes the loss (i.e. training) by the gradient descent algorithm using a fixed number of iteration (*iter*) and learning rate (*lr*). Your function should take *iter* and *lr* as well as the initial weight w, initial bias b, the input data X and the label y as the inputs. It produces new w and b as output. Also compute the loss value at each iteration and output the sequence of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "id": "6bIdE16li086"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def train(w, b, X, y, iter, lr):\n",
    "  print(f'>> {X.shape}')\n",
    "  losslist=list()\n",
    "  for k in range(iter):\n",
    "    z = model(w, b, X)\n",
    "    y_hat = sigmoid(z)\n",
    "    grad = gradients(X, y, y_hat)\n",
    "    print(f'gradient shape: {grad.shape}')\n",
    "    w = w - (lr * grad)\n",
    "    b = np.mean((b*np.ones(y_hat.shape)) - (lr * (y_hat - y)))\n",
    "    myloss = customloss(y, z)\n",
    "    losslist.append(np.mean(myloss))\n",
    "    print(f'Iter: {k} Loss: {losslist[-1]}')\n",
    "  return w, b, losslist\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "id": "XbA52seSg7wW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGC-EjrzeHyU"
   },
   "source": [
    "1. Write the function that uses a trained model to produce class prediction (0 or 1) for an input dataset X, i.e. turn the model output z = model(w,b,X) into predicted label y_label (N-vector of 0 or 1). \n",
    "2. For an input dataset X with a known label y (e.g. a training or testing dataset) and a predicted label y_label, compute the accuracy of prediction (i.e. # correct predictions/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "id": "HChwCsuWf07D"
   },
   "outputs": [],
   "source": [
    "def predict(z):\n",
    "  ypred = sigmoid(z)\n",
    "  ypred[ypred<=0.5]=0\n",
    "  ypred[ypred>0.5]=1\n",
    "  ypred = ypred.astype(int)\n",
    "  ypred = np.squeeze(ypred)\n",
    "  # print(f'In pred, {ypred.shape}')\n",
    "  return ypred\n",
    "\n",
    "def accuracy(y, y_label):\n",
    "  diff_bool = (y == y_label)\n",
    "  diff_true = diff_bool[diff_bool==True]\n",
    "  total_sample = len(diff_bool)\n",
    "  return (len(diff_true)/total_sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icfCmavagcp5"
   },
   "source": [
    "We are ready to test your programs on some datasets. First, we use a synthetic dataset generated using [scikit-learn](https://scikit-learn.org/stable/datasets.html) package. We generate a dataset for training and simultaneously a dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "EXJOlxH2nYw3",
    "outputId": "802347fc-f3f7-44f2-98da-c834611ebb32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba022ef640>]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X_train, y_train = make_moons(n_samples=500, noise=0.1)\n",
    "X_test, y_test = make_moons(n_samples=1000, noise=0.1)\n",
    "\n",
    "print(X_train.shape)\n",
    "plt.figure()\n",
    "plt.plot(X_train[:, 0][y_train==0], X_train[:, 1][y_train==0], \"g^\")\n",
    "plt.plot(X_train[:, 0][y_train==1], X_train[:, 1][y_train==1], \"bs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2_uUM_Zufoz"
   },
   "source": [
    "Here is another toy test example you may try but not part of homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "EJbVzPfLdOvC",
    "outputId": "695e26c1-30ed-4447-ecbf-c3ca63d07366"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba022c8e20>]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X_train, y_train = make_classification(n_samples=1000, n_features=4)\n",
    "X_test=X_train[500:,]\n",
    "y_test=y_train[500:,]\n",
    "X_train=X_train[:500,]\n",
    "y_train=y_train[:500,]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_train[:, 0][y_train==0], X_train[:, 1][y_train==0], \"g^\")\n",
    "plt.plot(X_train[:, 0][y_train==1], X_train[:, 1][y_train==1], \"bs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMm__k_AjOFp"
   },
   "source": [
    "We now train the model using (X_train, y_train). We initialize weight as a random vector, and b=0. We plot the loss convergence history. You should get the loss down to about 0.2.\n",
    "We compute the prediction accuracy on (X_train, y_train). You should get an accuracy in the 80s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mi868jT_mpnq",
    "outputId": "a09fbd00-ec05-4889-8f89-ff1f663d5ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(500, 4)\n",
      "(500, 1)\n",
      ">> (500, 4)\n",
      "In model, X: (500, 4), b: 0, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 0 Loss: -0.07642467372725602\n",
      "In model, X: (500, 4), b: -1.2955808907858007e-09, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 1 Loss: -0.07642485546576569\n",
      "In model, X: (500, 4), b: -2.591155051288044e-09, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 2 Loss: -0.07642503720396916\n",
      "In model, X: (500, 4), b: -3.886722481514391e-09, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 3 Loss: -0.07642521894186648\n",
      "In model, X: (500, 4), b: -5.182283181472564e-09, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 4 Loss: -0.07642540067945755\n",
      "In model, X: (500, 4), b: -6.477837151170224e-09, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 5 Loss: -0.07642558241674245\n",
      "In model, X: (500, 4), b: -7.773384390615111e-09, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 6 Loss: -0.07642576415372117\n",
      "In model, X: (500, 4), b: -9.068924899814934e-09, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 7 Loss: -0.07642594589039367\n",
      "In model, X: (500, 4), b: -1.0364458678777341e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 8 Loss: -0.07642612762676\n",
      "In model, X: (500, 4), b: -1.1659985727510052e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 9 Loss: -0.07642630936282012\n",
      "In model, X: (500, 4), b: -1.2955506046020779e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 10 Loss: -0.07642649109857406\n",
      "In model, X: (500, 4), b: -1.4251019634317175e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 11 Loss: -0.07642667283402181\n",
      "In model, X: (500, 4), b: -1.5546526492406975e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 12 Loss: -0.07642685456916339\n",
      "In model, X: (500, 4), b: -1.6842026620297856e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 13 Loss: -0.07642703630399875\n",
      "In model, X: (500, 4), b: -1.8137520017997527e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 14 Loss: -0.07642721803852791\n",
      "In model, X: (500, 4), b: -1.9433006685513682e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 15 Loss: -0.07642739977275093\n",
      "In model, X: (500, 4), b: -2.0728486622854e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 16 Loss: -0.0764275815066677\n",
      "In model, X: (500, 4), b: -2.20239598300262e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 17 Loss: -0.07642776324027832\n",
      "In model, X: (500, 4), b: -2.331942630703798e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 18 Loss: -0.07642794497358275\n",
      "In model, X: (500, 4), b: -2.4614886053897023e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 19 Loss: -0.07642812670658101\n",
      "In model, X: (500, 4), b: -2.5910339070611046e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 20 Loss: -0.07642830843927305\n",
      "In model, X: (500, 4), b: -2.7205785357187732e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 21 Loss: -0.07642849017165894\n",
      "In model, X: (500, 4), b: -2.850122491363476e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 22 Loss: -0.0764286719037386\n",
      "In model, X: (500, 4), b: -2.979665773995985e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 23 Loss: -0.07642885363551212\n",
      "In model, X: (500, 4), b: -3.109208383617071e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 24 Loss: -0.07642903536697944\n",
      "In model, X: (500, 4), b: -3.238750320227502e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 25 Loss: -0.07642921709814054\n",
      "In model, X: (500, 4), b: -3.368291583828047e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 26 Loss: -0.07642939882899552\n",
      "In model, X: (500, 4), b: -3.497832174419477e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 27 Loss: -0.07642958055954428\n",
      "In model, X: (500, 4), b: -3.6273720920025616e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 28 Loss: -0.07642976228978685\n",
      "In model, X: (500, 4), b: -3.756911336578072e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 29 Loss: -0.07642994401972325\n",
      "In model, X: (500, 4), b: -3.886449908146775e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 30 Loss: -0.07643012574935347\n",
      "In model, X: (500, 4), b: -4.015987806709441e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 31 Loss: -0.07643030747867752\n",
      "In model, X: (500, 4), b: -4.1455250322668417e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 32 Loss: -0.07643048920769539\n",
      "In model, X: (500, 4), b: -4.2750615848197455e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 33 Loss: -0.07643067093640707\n",
      "In model, X: (500, 4), b: -4.40459746436892e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 34 Loss: -0.07643085266481257\n",
      "In model, X: (500, 4), b: -4.534132670915139e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 35 Loss: -0.07643103439291189\n",
      "In model, X: (500, 4), b: -4.663667204459169e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 36 Loss: -0.07643121612070505\n",
      "In model, X: (500, 4), b: -4.79320106500178e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 37 Loss: -0.07643139784819203\n",
      "In model, X: (500, 4), b: -4.9227342525437445e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 38 Loss: -0.07643157957537283\n",
      "In model, X: (500, 4), b: -5.0522667670858285e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 39 Loss: -0.07643176130224748\n",
      "In model, X: (500, 4), b: -5.181798608628804e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 40 Loss: -0.07643194302881592\n",
      "In model, X: (500, 4), b: -5.31132977717344e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 41 Loss: -0.0764321247550782\n",
      "In model, X: (500, 4), b: -5.440860272720506e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 42 Loss: -0.07643230648103432\n",
      "In model, X: (500, 4), b: -5.570390095270774e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 43 Loss: -0.07643248820668425\n",
      "In model, X: (500, 4), b: -5.6999192448250115e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 44 Loss: -0.07643266993202799\n",
      "In model, X: (500, 4), b: -5.829447721383989e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 45 Loss: -0.07643285165706558\n",
      "In model, X: (500, 4), b: -5.958975524948475e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 46 Loss: -0.07643303338179701\n",
      "In model, X: (500, 4), b: -6.088502655519241e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 47 Loss: -0.07643321510622224\n",
      "In model, X: (500, 4), b: -6.218029113097058e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 48 Loss: -0.07643339683034131\n",
      "In model, X: (500, 4), b: -6.347554897682691e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 49 Loss: -0.07643357855415422\n",
      "In model, X: (500, 4), b: -6.477080009276916e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 50 Loss: -0.07643376027766095\n",
      "In model, X: (500, 4), b: -6.606604447880497e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 51 Loss: -0.07643394200086152\n",
      "In model, X: (500, 4), b: -6.736128213494206e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 52 Loss: -0.07643412372375591\n",
      "In model, X: (500, 4), b: -6.865651306118812e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 53 Loss: -0.0764343054463441\n",
      "In model, X: (500, 4), b: -6.995173725755088e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 54 Loss: -0.07643448716862616\n",
      "In model, X: (500, 4), b: -7.124695472403799e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 55 Loss: -0.07643466889060208\n",
      "In model, X: (500, 4), b: -7.254216546065718e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 56 Loss: -0.07643485061227179\n",
      "In model, X: (500, 4), b: -7.383736946741614e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 57 Loss: -0.07643503233363534\n",
      "In model, X: (500, 4), b: -7.513256674432257e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 58 Loss: -0.07643521405469272\n",
      "In model, X: (500, 4), b: -7.642775729138416e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 59 Loss: -0.07643539577544393\n",
      "In model, X: (500, 4), b: -7.77229411086086e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 60 Loss: -0.07643557749588901\n",
      "In model, X: (500, 4), b: -7.901811819600361e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 61 Loss: -0.07643575921602791\n",
      "In model, X: (500, 4), b: -8.03132885535769e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 62 Loss: -0.07643594093586063\n",
      "In model, X: (500, 4), b: -8.160845218133613e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 63 Loss: -0.07643612265538721\n",
      "In model, X: (500, 4), b: -8.290360907928903e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 64 Loss: -0.0764363043746076\n",
      "In model, X: (500, 4), b: -8.419875924744327e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 65 Loss: -0.07643648609352187\n",
      "In model, X: (500, 4), b: -8.549390268580654e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 66 Loss: -0.07643666781212995\n",
      "In model, X: (500, 4), b: -8.678903939438658e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 67 Loss: -0.07643684953043185\n",
      "In model, X: (500, 4), b: -8.808416937319106e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 68 Loss: -0.07643703124842761\n",
      "In model, X: (500, 4), b: -8.937929262222769e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 69 Loss: -0.07643721296611722\n",
      "In model, X: (500, 4), b: -9.067440914150416e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 70 Loss: -0.07643739468350065\n",
      "In model, X: (500, 4), b: -9.196951893102817e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 71 Loss: -0.07643757640057795\n",
      "In model, X: (500, 4), b: -9.32646219908074e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 72 Loss: -0.07643775811734908\n",
      "In model, X: (500, 4), b: -9.455971832084957e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 73 Loss: -0.07643793983381406\n",
      "In model, X: (500, 4), b: -9.585480792116238e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 74 Loss: -0.07643812154997284\n",
      "In model, X: (500, 4), b: -9.714989079175353e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 75 Loss: -0.07643830326582553\n",
      "In model, X: (500, 4), b: -9.84449669326307e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 76 Loss: -0.07643848498137203\n",
      "In model, X: (500, 4), b: -9.97400363438016e-08, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 77 Loss: -0.07643866669661235\n",
      "In model, X: (500, 4), b: -1.0103509902527392e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 78 Loss: -0.07643884841154655\n",
      "In model, X: (500, 4), b: -1.0233015497705536e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 79 Loss: -0.07643903012617458\n",
      "In model, X: (500, 4), b: -1.0362520419915361e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 80 Loss: -0.07643921184049647\n",
      "In model, X: (500, 4), b: -1.0492024669157639e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 81 Loss: -0.0764393935545122\n",
      "In model, X: (500, 4), b: -1.0621528245433138e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 82 Loss: -0.07643957526822176\n",
      "In model, X: (500, 4), b: -1.075103114874263e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 83 Loss: -0.0764397569816252\n",
      "In model, X: (500, 4), b: -1.0880533379086884e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 84 Loss: -0.07643993869472245\n",
      "In model, X: (500, 4), b: -1.101003493646667e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 85 Loss: -0.07644012040751358\n",
      "In model, X: (500, 4), b: -1.1139535820882756e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 86 Loss: -0.07644030211999853\n",
      "In model, X: (500, 4), b: -1.1269036032335912e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 87 Loss: -0.07644048383217734\n",
      "In model, X: (500, 4), b: -1.139853557082691e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 88 Loss: -0.07644066554405003\n",
      "In model, X: (500, 4), b: -1.1528034436356519e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 89 Loss: -0.07644084725561653\n",
      "In model, X: (500, 4), b: -1.1657532628925508e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 90 Loss: -0.0764410289668769\n",
      "In model, X: (500, 4), b: -1.1787030148534648e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 91 Loss: -0.07644121067783112\n",
      "In model, X: (500, 4), b: -1.1916526995184707e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 92 Loss: -0.07644139238847919\n",
      "In model, X: (500, 4), b: -1.2046023168876458e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 93 Loss: -0.07644157409882112\n",
      "In model, X: (500, 4), b: -1.2175518669610666e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 94 Loss: -0.07644175580885691\n",
      "In model, X: (500, 4), b: -1.2305013497388107e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 95 Loss: -0.07644193751858654\n",
      "In model, X: (500, 4), b: -1.2434507652209544e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 96 Loss: -0.07644211922801006\n",
      "In model, X: (500, 4), b: -1.256400113407575e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 97 Loss: -0.07644230093712738\n",
      "In model, X: (500, 4), b: -1.2693493942987497e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 98 Loss: -0.07644248264593861\n",
      "In model, X: (500, 4), b: -1.282298607894555e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 99 Loss: -0.07644266435444365\n",
      "In model, X: (500, 4), b: -1.2952477541950685e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 100 Loss: -0.07644284606264255\n",
      "In model, X: (500, 4), b: -1.3081968332003667e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 101 Loss: -0.07644302777053534\n",
      "In model, X: (500, 4), b: -1.3211458449105268e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 102 Loss: -0.07644320947812196\n",
      "In model, X: (500, 4), b: -1.334094789325626e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 103 Loss: -0.07644339118540243\n",
      "In model, X: (500, 4), b: -1.3470436664457406e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 104 Loss: -0.0764435728923768\n",
      "In model, X: (500, 4), b: -1.3599924762709483e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 105 Loss: -0.07644375459904501\n",
      "In model, X: (500, 4), b: -1.3729412188013259e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 106 Loss: -0.07644393630540706\n",
      "In model, X: (500, 4), b: -1.3858898940369501e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 107 Loss: -0.076444118011463\n",
      "In model, X: (500, 4), b: -1.398838501977898e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 108 Loss: -0.07644429971721277\n",
      "In model, X: (500, 4), b: -1.4117870426242469e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 109 Loss: -0.07644448142265643\n",
      "In model, X: (500, 4), b: -1.4247355159760734e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 110 Loss: -0.07644466312779394\n",
      "In model, X: (500, 4), b: -1.4376839220334544e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 111 Loss: -0.07644484483262533\n",
      "In model, X: (500, 4), b: -1.4506322607964673e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 112 Loss: -0.07644502653715056\n",
      "In model, X: (500, 4), b: -1.4635805322651887e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 113 Loss: -0.07644520824136967\n",
      "In model, X: (500, 4), b: -1.4765287364396958e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 114 Loss: -0.07644538994528263\n",
      "In model, X: (500, 4), b: -1.4894768733200658e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 115 Loss: -0.07644557164888949\n",
      "In model, X: (500, 4), b: -1.5024249429063757e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 116 Loss: -0.07644575335219017\n",
      "In model, X: (500, 4), b: -1.515372945198702e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 117 Loss: -0.07644593505518474\n",
      "In model, X: (500, 4), b: -1.5283208801971223e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 118 Loss: -0.07644611675787318\n",
      "In model, X: (500, 4), b: -1.5412687479017126e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 119 Loss: -0.07644629846025547\n",
      "In model, X: (500, 4), b: -1.554216548312551e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 120 Loss: -0.07644648016233163\n",
      "In model, X: (500, 4), b: -1.5671642814297138e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 121 Loss: -0.07644666186410166\n",
      "In model, X: (500, 4), b: -1.5801119472532783e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 122 Loss: -0.07644684356556555\n",
      "In model, X: (500, 4), b: -1.5930595457833216e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 123 Loss: -0.07644702526672334\n",
      "In model, X: (500, 4), b: -1.60600707701992e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 124 Loss: -0.07644720696757495\n",
      "In model, X: (500, 4), b: -1.6189545409631512e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 125 Loss: -0.07644738866812048\n",
      "In model, X: (500, 4), b: -1.631901937613092e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 126 Loss: -0.07644757036835984\n",
      "In model, X: (500, 4), b: -1.6448492669698194e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 127 Loss: -0.0764477520682931\n",
      "In model, X: (500, 4), b: -1.6577965290334105e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 128 Loss: -0.07644793376792021\n",
      "In model, X: (500, 4), b: -1.670743723803942e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 129 Loss: -0.0764481154672412\n",
      "In model, X: (500, 4), b: -1.6836908512814908e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 130 Loss: -0.07644829716625606\n",
      "In model, X: (500, 4), b: -1.6966379114661345e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 131 Loss: -0.0764484788649648\n",
      "In model, X: (500, 4), b: -1.7095849043579495e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 132 Loss: -0.07644866056336738\n",
      "In model, X: (500, 4), b: -1.722531829957013e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 133 Loss: -0.0764488422614639\n",
      "In model, X: (500, 4), b: -1.735478688263402e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 134 Loss: -0.07644902395925426\n",
      "In model, X: (500, 4), b: -1.7484254792771934e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 135 Loss: -0.0764492056567385\n",
      "In model, X: (500, 4), b: -1.7613722029984645e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 136 Loss: -0.07644938735391664\n",
      "In model, X: (500, 4), b: -1.7743188594272917e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 137 Loss: -0.07644956905078859\n",
      "In model, X: (500, 4), b: -1.7872654485637527e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 138 Loss: -0.07644975074735448\n",
      "In model, X: (500, 4), b: -1.8002119704079239e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 139 Loss: -0.07644993244361421\n",
      "In model, X: (500, 4), b: -1.8131584249598826e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 140 Loss: -0.07645011413956786\n",
      "In model, X: (500, 4), b: -1.826104812219706e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 141 Loss: -0.07645029583521534\n",
      "In model, X: (500, 4), b: -1.839051132187471e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 142 Loss: -0.07645047753055674\n",
      "In model, X: (500, 4), b: -1.8519973848632542e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 143 Loss: -0.07645065922559201\n",
      "In model, X: (500, 4), b: -1.8649435702471327e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 144 Loss: -0.07645084092032116\n",
      "In model, X: (500, 4), b: -1.8778896883391837e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 145 Loss: -0.07645102261474418\n",
      "In model, X: (500, 4), b: -1.8908357391394843e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 146 Loss: -0.07645120430886107\n",
      "In model, X: (500, 4), b: -1.9037817226481111e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 147 Loss: -0.07645138600267187\n",
      "In model, X: (500, 4), b: -1.916727638865141e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 148 Loss: -0.07645156769617653\n",
      "In model, X: (500, 4), b: -1.9296734877906515e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 149 Loss: -0.07645174938937506\n",
      "In model, X: (500, 4), b: -1.9426192694247193e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 150 Loss: -0.0764519310822675\n",
      "In model, X: (500, 4), b: -1.9555649837674216e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 151 Loss: -0.07645211277485382\n",
      "In model, X: (500, 4), b: -1.9685106308188353e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 152 Loss: -0.07645229446713404\n",
      "In model, X: (500, 4), b: -1.9814562105790374e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 153 Loss: -0.07645247615910812\n",
      "In model, X: (500, 4), b: -1.994401723048105e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 154 Loss: -0.0764526578507761\n",
      "In model, X: (500, 4), b: -2.0073471682261148e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 155 Loss: -0.07645283954213794\n",
      "In model, X: (500, 4), b: -2.020292546113144e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 156 Loss: -0.07645302123319367\n",
      "In model, X: (500, 4), b: -2.0332378567092695e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 157 Loss: -0.07645320292394331\n",
      "In model, X: (500, 4), b: -2.0461831000145684e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 158 Loss: -0.07645338461438682\n",
      "In model, X: (500, 4), b: -2.0591282760291176e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 159 Loss: -0.07645356630452424\n",
      "In model, X: (500, 4), b: -2.072073384752994e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 160 Loss: -0.07645374799435553\n",
      "In model, X: (500, 4), b: -2.085018426186275e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 161 Loss: -0.0764539296838807\n",
      "In model, X: (500, 4), b: -2.0979634003290373e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 162 Loss: -0.07645411137309979\n",
      "In model, X: (500, 4), b: -2.1109083071813583e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 163 Loss: -0.07645429306201275\n",
      "In model, X: (500, 4), b: -2.1238531467433146e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 164 Loss: -0.0764544747506196\n",
      "In model, X: (500, 4), b: -2.136797919014983e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 165 Loss: -0.07645465643892035\n",
      "In model, X: (500, 4), b: -2.1497426239964407e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 166 Loss: -0.07645483812691498\n",
      "In model, X: (500, 4), b: -2.1626872616877645e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 167 Loss: -0.07645501981460354\n",
      "In model, X: (500, 4), b: -2.1756318320890318e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 168 Loss: -0.07645520150198595\n",
      "In model, X: (500, 4), b: -2.1885763352003194e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 169 Loss: -0.07645538318906225\n",
      "In model, X: (500, 4), b: -2.2015207710217045e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 170 Loss: -0.0764555648758325\n",
      "In model, X: (500, 4), b: -2.214465139553264e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 171 Loss: -0.07645574656229658\n",
      "In model, X: (500, 4), b: -2.2274094407950744e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 172 Loss: -0.0764559282484546\n",
      "In model, X: (500, 4), b: -2.2403536747472132e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 173 Loss: -0.07645610993430649\n",
      "In model, X: (500, 4), b: -2.2532978414097575e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 174 Loss: -0.07645629161985226\n",
      "In model, X: (500, 4), b: -2.2662419407827838e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 175 Loss: -0.07645647330509196\n",
      "In model, X: (500, 4), b: -2.27918597286637e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 176 Loss: -0.07645665499002553\n",
      "In model, X: (500, 4), b: -2.292129937660592e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 177 Loss: -0.07645683667465303\n",
      "In model, X: (500, 4), b: -2.3050738351655275e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 178 Loss: -0.07645701835897441\n",
      "In model, X: (500, 4), b: -2.318017665381253e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 179 Loss: -0.0764572000429897\n",
      "In model, X: (500, 4), b: -2.330961428307846e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 180 Loss: -0.07645738172669889\n",
      "In model, X: (500, 4), b: -2.3439051239453834e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 181 Loss: -0.07645756341010197\n",
      "In model, X: (500, 4), b: -2.356848752293942e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 182 Loss: -0.07645774509319896\n",
      "In model, X: (500, 4), b: -2.369792313353599e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 183 Loss: -0.07645792677598982\n",
      "In model, X: (500, 4), b: -2.3827358071244308e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 184 Loss: -0.07645810845847462\n",
      "In model, X: (500, 4), b: -2.395679233606516e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 185 Loss: -0.0764582901406533\n",
      "In model, X: (500, 4), b: -2.40862259279993e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 186 Loss: -0.07645847182252588\n",
      "In model, X: (500, 4), b: -2.4215658847047503e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 187 Loss: -0.07645865350409238\n",
      "In model, X: (500, 4), b: -2.434509109321054e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 188 Loss: -0.07645883518535276\n",
      "In model, X: (500, 4), b: -2.447452266648918e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 189 Loss: -0.07645901686630707\n",
      "In model, X: (500, 4), b: -2.4603953566884194e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 190 Loss: -0.0764591985469553\n",
      "In model, X: (500, 4), b: -2.4733383794396356e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 191 Loss: -0.0764593802272974\n",
      "In model, X: (500, 4), b: -2.486281334902643e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 192 Loss: -0.07645956190733341\n",
      "In model, X: (500, 4), b: -2.499224223077518e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 193 Loss: -0.07645974358706334\n",
      "In model, X: (500, 4), b: -2.512167043964338e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 194 Loss: -0.07645992526648716\n",
      "In model, X: (500, 4), b: -2.5251097975631813e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 195 Loss: -0.07646010694560489\n",
      "In model, X: (500, 4), b: -2.5380524838741237e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 196 Loss: -0.07646028862441656\n",
      "In model, X: (500, 4), b: -2.550995102897242e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 197 Loss: -0.0764604703029221\n",
      "In model, X: (500, 4), b: -2.563937654632614e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 198 Loss: -0.07646065198112156\n",
      "In model, X: (500, 4), b: -2.576880139080316e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 199 Loss: -0.0764608336590149\n",
      "In model, X: (500, 4), b: -2.589822556240426e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 200 Loss: -0.0764610153366022\n",
      "In model, X: (500, 4), b: -2.6027649061130206e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 201 Loss: -0.07646119701388339\n",
      "In model, X: (500, 4), b: -2.6157071886981765e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 202 Loss: -0.07646137869085848\n",
      "In model, X: (500, 4), b: -2.6286494039959704e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 203 Loss: -0.07646156036752752\n",
      "In model, X: (500, 4), b: -2.6415915520064793e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 204 Loss: -0.07646174204389042\n",
      "In model, X: (500, 4), b: -2.6545336327297813e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 205 Loss: -0.07646192371994727\n",
      "In model, X: (500, 4), b: -2.6674756461659523e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 206 Loss: -0.07646210539569802\n",
      "In model, X: (500, 4), b: -2.68041759231507e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 207 Loss: -0.07646228707114269\n",
      "In model, X: (500, 4), b: -2.6933594711772113e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 208 Loss: -0.07646246874628128\n",
      "In model, X: (500, 4), b: -2.7063012827524523e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 209 Loss: -0.07646265042111376\n",
      "In model, X: (500, 4), b: -2.719243027040871e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 210 Loss: -0.07646283209564018\n",
      "In model, X: (500, 4), b: -2.732184704042544e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 211 Loss: -0.0764630137698605\n",
      "In model, X: (500, 4), b: -2.7451263137575483e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 212 Loss: -0.07646319544377474\n",
      "In model, X: (500, 4), b: -2.7580678561859617e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 213 Loss: -0.07646337711738292\n",
      "In model, X: (500, 4), b: -2.7710093313278603e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 214 Loss: -0.07646355879068498\n",
      "In model, X: (500, 4), b: -2.7839507391833213e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 215 Loss: -0.07646374046368097\n",
      "In model, X: (500, 4), b: -2.796892079752422e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 216 Loss: -0.07646392213637085\n",
      "In model, X: (500, 4), b: -2.809833353035239e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 217 Loss: -0.07646410380875471\n",
      "In model, X: (500, 4), b: -2.822774559031849e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 218 Loss: -0.07646428548083246\n",
      "In model, X: (500, 4), b: -2.8357156977423297e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 219 Loss: -0.07646446715260413\n",
      "In model, X: (500, 4), b: -2.848656769166758e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 220 Loss: -0.07646464882406973\n",
      "In model, X: (500, 4), b: -2.861597773305211e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 221 Loss: -0.07646483049522924\n",
      "In model, X: (500, 4), b: -2.874538710157766e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 222 Loss: -0.07646501216608269\n",
      "In model, X: (500, 4), b: -2.887479579724499e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 223 Loss: -0.07646519383663004\n",
      "In model, X: (500, 4), b: -2.9004203820054876e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 224 Loss: -0.07646537550687131\n",
      "In model, X: (500, 4), b: -2.913361117000808e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 225 Loss: -0.07646555717680653\n",
      "In model, X: (500, 4), b: -2.926301784710539e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 226 Loss: -0.07646573884643564\n",
      "In model, X: (500, 4), b: -2.939242385134756e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 227 Loss: -0.07646592051575869\n",
      "In model, X: (500, 4), b: -2.952182918273536e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 228 Loss: -0.07646610218477566\n",
      "In model, X: (500, 4), b: -2.9651233841269564e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 229 Loss: -0.07646628385348657\n",
      "In model, X: (500, 4), b: -2.9780637826950956e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 230 Loss: -0.07646646552189137\n",
      "In model, X: (500, 4), b: -2.991004113978029e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 231 Loss: -0.0764666471899901\n",
      "In model, X: (500, 4), b: -3.003944377975834e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 232 Loss: -0.07646682885778282\n",
      "In model, X: (500, 4), b: -3.0168845746885874e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 233 Loss: -0.07646701052526941\n",
      "In model, X: (500, 4), b: -3.0298247041163667e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 234 Loss: -0.07646719219244998\n",
      "In model, X: (500, 4), b: -3.0427647662592484e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 235 Loss: -0.07646737385932442\n",
      "In model, X: (500, 4), b: -3.05570476111731e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 236 Loss: -0.07646755552589281\n",
      "In model, X: (500, 4), b: -3.0686446886906284e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 237 Loss: -0.07646773719215513\n",
      "In model, X: (500, 4), b: -3.08158454897928e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 238 Loss: -0.0764679188581114\n",
      "In model, X: (500, 4), b: -3.094524341983343e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 239 Loss: -0.07646810052376156\n",
      "In model, X: (500, 4), b: -3.107464067702893e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 240 Loss: -0.07646828218910567\n",
      "In model, X: (500, 4), b: -3.120403726138008e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 241 Loss: -0.07646846385414371\n",
      "In model, X: (500, 4), b: -3.133343317288764e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 242 Loss: -0.0764686455188757\n",
      "In model, X: (500, 4), b: -3.1462828411552394e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 243 Loss: -0.07646882718330164\n",
      "In model, X: (500, 4), b: -3.1592222977375107e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 244 Loss: -0.07646900884742146\n",
      "In model, X: (500, 4), b: -3.1721616870356537e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 245 Loss: -0.07646919051123524\n",
      "In model, X: (500, 4), b: -3.185101009049747e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 246 Loss: -0.07646937217474296\n",
      "In model, X: (500, 4), b: -3.1980402637798674e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 247 Loss: -0.07646955383794461\n",
      "In model, X: (500, 4), b: -3.210979451226091e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 248 Loss: -0.0764697355008402\n",
      "In model, X: (500, 4), b: -3.223918571388496e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 249 Loss: -0.07646991716342971\n",
      "In model, X: (500, 4), b: -3.2368576242671586e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 250 Loss: -0.07647009882571316\n",
      "In model, X: (500, 4), b: -3.2497966098621563e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 251 Loss: -0.07647028048769054\n",
      "In model, X: (500, 4), b: -3.2627355281735656e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 252 Loss: -0.07647046214936187\n",
      "In model, X: (500, 4), b: -3.275674379201464e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 253 Loss: -0.07647064381072717\n",
      "In model, X: (500, 4), b: -3.2886131629459277e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 254 Loss: -0.07647082547178637\n",
      "In model, X: (500, 4), b: -3.3015518794070346e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 255 Loss: -0.07647100713253951\n",
      "In model, X: (500, 4), b: -3.314490528584862e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 256 Loss: -0.07647118879298659\n",
      "In model, X: (500, 4), b: -3.327429110479486e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 257 Loss: -0.07647137045312763\n",
      "In model, X: (500, 4), b: -3.340367625090984e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 258 Loss: -0.07647155211296261\n",
      "In model, X: (500, 4), b: -3.3533060724194327e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 259 Loss: -0.07647173377249152\n",
      "In model, X: (500, 4), b: -3.366244452464909e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 260 Loss: -0.07647191543171436\n",
      "In model, X: (500, 4), b: -3.3791827652274913e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 261 Loss: -0.07647209709063116\n",
      "In model, X: (500, 4), b: -3.3921210107072543e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 262 Loss: -0.07647227874924188\n",
      "In model, X: (500, 4), b: -3.405059188904277e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 263 Loss: -0.07647246040754656\n",
      "In model, X: (500, 4), b: -3.4179972998186357e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 264 Loss: -0.07647264206554519\n",
      "In model, X: (500, 4), b: -3.430935343450407e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 265 Loss: -0.07647282372323778\n",
      "In model, X: (500, 4), b: -3.443873319799669e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 266 Loss: -0.0764730053806243\n",
      "In model, X: (500, 4), b: -3.456811228866498e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 267 Loss: -0.07647318703770475\n",
      "In model, X: (500, 4), b: -3.469749070650971e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 268 Loss: -0.07647336869447915\n",
      "In model, X: (500, 4), b: -3.482686845153165e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 269 Loss: -0.07647355035094748\n",
      "In model, X: (500, 4), b: -3.4956245523731574e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 270 Loss: -0.07647373200710979\n",
      "In model, X: (500, 4), b: -3.5085621923110246e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 271 Loss: -0.07647391366296606\n",
      "In model, X: (500, 4), b: -3.521499764966844e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 272 Loss: -0.07647409531851623\n",
      "In model, X: (500, 4), b: -3.534437270340693e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 273 Loss: -0.0764742769737604\n",
      "In model, X: (500, 4), b: -3.547374708432648e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 274 Loss: -0.07647445862869848\n",
      "In model, X: (500, 4), b: -3.560312079242786e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 275 Loss: -0.0764746402833305\n",
      "In model, X: (500, 4), b: -3.5732493827711847e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 276 Loss: -0.0764748219376565\n",
      "In model, X: (500, 4), b: -3.5861866190179207e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 277 Loss: -0.07647500359167643\n",
      "In model, X: (500, 4), b: -3.5991237879830705e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 278 Loss: -0.07647518524539033\n",
      "In model, X: (500, 4), b: -3.612060889666712e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 279 Loss: -0.0764753668987982\n",
      "In model, X: (500, 4), b: -3.624997924068922e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 280 Loss: -0.0764755485519\n",
      "In model, X: (500, 4), b: -3.637934891189777e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 281 Loss: -0.07647573020469577\n",
      "In model, X: (500, 4), b: -3.6508717910293545e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 282 Loss: -0.07647591185718546\n",
      "In model, X: (500, 4), b: -3.6638086235877307e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 283 Loss: -0.07647609350936913\n",
      "In model, X: (500, 4), b: -3.676745388864984e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 284 Loss: -0.07647627516124672\n",
      "In model, X: (500, 4), b: -3.689682086861191e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 285 Loss: -0.07647645681281828\n",
      "In model, X: (500, 4), b: -3.702618717576428e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 286 Loss: -0.0764766384640838\n",
      "In model, X: (500, 4), b: -3.715555281010773e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 287 Loss: -0.07647682011504327\n",
      "In model, X: (500, 4), b: -3.728491777164302e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 288 Loss: -0.07647700176569672\n",
      "In model, X: (500, 4), b: -3.7414282060370923e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 289 Loss: -0.07647718341604415\n",
      "In model, X: (500, 4), b: -3.7543645676292214e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 290 Loss: -0.0764773650660855\n",
      "In model, X: (500, 4), b: -3.767300861940766e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 291 Loss: -0.07647754671582084\n",
      "In model, X: (500, 4), b: -3.780237088971803e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 292 Loss: -0.0764777283652501\n",
      "In model, X: (500, 4), b: -3.79317324872241e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 293 Loss: -0.07647791001437333\n",
      "In model, X: (500, 4), b: -3.806109341192663e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 294 Loss: -0.07647809166319053\n",
      "In model, X: (500, 4), b: -3.81904536638264e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 295 Loss: -0.07647827331170165\n",
      "In model, X: (500, 4), b: -3.831981324292417e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 296 Loss: -0.07647845495990677\n",
      "In model, X: (500, 4), b: -3.844917214922072e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 297 Loss: -0.07647863660780584\n",
      "In model, X: (500, 4), b: -3.857853038271682e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 298 Loss: -0.07647881825539889\n",
      "In model, X: (500, 4), b: -3.870788794341324e-07, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 299 Loss: -0.07647899990268588\n",
      "In model, X: (500, 4), b: -3.8837244831310744e-07, w: (4, 1)\n",
      "0.914\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAD4CAYAAAAkRnsLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZdrH8e+dBEJvUoxUKRFBihDpBHVpYgHLIsgiugoKIm11hdfX1ZUtuu8aQEERbFhQFJGiCAQWEzokSJdeQ29Spcnz/jEn6xgTAkxgMsnvc13nyswzz5y5HwfzyzmZ3Mecc4iIiGSFsGAXICIiOYdCRUREsoxCRUREsoxCRUREsoxCRUREskxEsAsItpIlS7pKlSoFuwwRkZCSnJx8wDlXKu14rg+VSpUqkZSUFOwyRERCipltS29cp79ERCTLKFRERCTLKFRERCTLKFRERCTLKFRERCTLKFRERCTLKFRERCTLKFQu04cLtpKwfn+wyxARyVZy/R8/Xo6zP59n7KLtrN1zjPvrleOFu26kWIG8wS5LRCTodKRyGfKEhzHxqab0vq0qE5ftpGVcIt+u3B3sskREgk6hcpny5QnnmTY3MLl3U8oUiaTnJ0t58qNk9h09FezSRESCRqESoJrXFWXSU015rm11/rNuHy3jEvg8aQe6TLOI5EYKlSwQER5Gz1ur8G3f5txwbWH+PH4FD7+3mB2HTga7NBGRq0qhkoWqlCrEuB6NGdy+Jku3HabN0ETen7eFn8/rqEVEcgeFShYLCzO6Nq7EjAEtuKVSCf46ZQ0d317Axn3Hgl2aiMgVp1C5QsoWy88Hj95CXMc6bNp/nHbD5jL8Pxs4+/P5YJcmInLFKFSuIDPjvnrliO/fglY1y/DvGeu5Z/g8VqYcCXZpIiJXhELlKihVOJIRD9Xj7a71OXD8NB3enMcr367l1Nmfg12aiEiWUqhcRW1qXsvM/i14oF45RiZs4o5hc1i0+WCwyxIRyTIKlausaIE8vPpAbT5+rCFnfz7Pg6MW8sLEVRw7dTbYpYmIBEyhEiTNqpVkRv9Y/tj0ej5etI02QxKZvW5fsMsSEQmIQiWICuSN4C931+DLnk0oGBnBo+8vof+4ZRw6cSbYpYmIXBaFSjZQr0Jxvu7TjD63V2XK8l20ikvg6xW71OpFREKOQiWbiIwIZ0DrG5jydDOuK5af3mO/p8dHyexVg0oRCSEBhYqZlTCzeDPb4H0tnsG8bt6cDWbWzRsrbGbL/LYDZjbU7zkdzWyNma02s7Fp9lfEzFLMbLjf2DQzW+7NH2lm4YGsLVhujCrCV72aMOiO6iSu30/LuATGLdmuoxYRCQmBHqkMBGY556oBs7z7v2JmJYAXgYZAA+BFMyvunDvmnKubugHbgAnec6oBg4CmzrmaQL80ux0MJKYZ6+icqwPcBJQCfh/g2oImIjyMJ1pUYVq/WG6MKsJzX66kyzuL2H5QDSpFJHsLNFTaA2O822OADunMaQPEO+cOOecOA/FAW/8JZhYNlAbmeEPdgRHefJxz+/zm1gfKADP89+GcO+rdjADyAiH/o/31JQvyWfdG/P3em1iRcoQ2QxN5d64aVIpI9hVoqJRxzqVe8nAPvm/2aZUFdvjdT/HG/HUCxrlfzvFEA9FmNs/MFppZWwAzCwNeA55Jrxgzmw7sA44B4y9jPdlOWJjRpWFF4gfE0rjKNQz+eg33vzWf9XvVoFJEsp9MQ8XMZprZqnS29v7zvEC43B+hOwGf+t2PAKoBtwKdgdFmVgzoBUx1zqWktxPnXBsgCogEbr/AmnqYWZKZJe3fv/8yS766oorm591uMQzrVJdtB09w5+tzGDZzA2fOqUGliGQfEZlNcM61zOgxM9trZlHOud1mFoXvKCGtnfjCIVU54Du/fdQBIpxzyX5zUoBFzrmzwBYzW48vZBoDzc2sF1AIyGtmx51z//1djnPulJlNwndqLj6DNY0CRgHExMSEzLkkM6N93bI0q1qSl6asYcjM9Xy7ajev3l+bOuWLBbs8EZGAT39NBrp5t7sBk9KZMx1obWbFvU+HtfbGUnXm10cpABPxgsjMSuI7HbbZOdfFOVfBOVcJ3ymwD51zA82skBdqmFkEcCewNsC1ZVvXFIrkjc43M/rhGA6fPMO9b87jH1N/4KczalApIsEVaKi8ArQysw1AS+8+ZhZjZu8AOOcO4fu01hJve9kbS9WR34bKdOCgma0BZgPPOucu1HmxIDDZzFYAy/AdMY0McG3ZXqsaZYgf0IIHb6nAqMTN3DEskQWb1KBSRILHcvvfP8TExLikpKRglxGw+ZsOMGjCSrYdPMlDDSsw8I7qFMmXJ9hliUgOZWbJzrmYtOP6i/ocokmVkkzrG0v35tfz2eLttI5LZNYPe4NdlojkMgqVHCR/3nCev7MGE3o1pWj+PDw2Jok+n37PweOng12aiOQSCpUcqG75Ykx5uhn9Wlbj21W7aTUkkUnLdqrVi4hccQqVHCpvRBj9Wkbz9dPNKV+iAH0/W8bjY5LYfeSnYJcmIjmYQiWHu+Hawkzo2YT/vfNG5m06QOu4RMYu2s55tXoRkStAoZILhIcZjzevzPR+sdxUtij/89VKHnpnIVsPnAh2aSKSwyhUcpGK1xRkbPeGvHJfLVbvPErbYYmMTtzMuZ/V6kVEsoZCJZcxMzo1qED8gBY0q1qSv0/9gfvfms/aPUczf7KISCYUKrnUtUXzMfrhGN7ofDMph3/irtfnEhe/ntPn1OpFRC6fQiUXMzPurnMd8QNacFftKF6ftYG735jL99sPB7s0EQlRChWhRMG8DO10M+89EsOxU+e47635DP56DSfPnAt2aSISYhQq8l+3Vy/DjP6xdGlYgXfnbqHt0DnM33gg2GWJSAhRqMivFM6Xh791qMVnPRoRZvDQO4sY+OUKjvx0NtiliUgIUKhIuhpVvoZp/WJ5okVlPk/aQau4BGas3hPsskQkm1OoSIby5Qln0B03MvGpppQomJceHyXTe+xSDqhBpYhkQKEimapdrhiTezfjT62imbF6Ly3jEvjq+xQ1qBSR31CoyEXJGxHG07+rxjd9mnF9yYL0H7ecP36whF0/qkGliPxCoSKXpFqZwox/sgl/uasGCzcfovWQRD5auE0NKkUEUKjIZQgPM/7Y7Hpm9I+lbvlivDBxFZ1GLWTz/uPBLk1EgkyhIpetfIkCfPRYA/51f21+2HOUO4bNYWTCJjWoFMnFFCoSEDOj4y3lmTmgBS2iS/HKt2vp8OY81uxSg0qR3EihIlmiTJF8vN21Pm92qceeI6e4Z/hcXpuxTg0qRXIZhYpkGTOjXa0o4vu34J661/HGfzZy5+tzSd6mBpUiuYVCRbJc8YJ5ietYlw8evYWfzvzMAyPn89cpqzlxWg0qRXI6hYpcMbfeUJrp/WPp2qgi78/bSpuhiczZsD/YZYnIFaRQkSuqUGQEL7e/ic+faEze8DC6vruYZ79YzpGTalApkhMpVOSqaHB9Cab2bU6vW6sw4fudtBySwLRValApktMoVOSqyZcnnD+3rc6kp5pSqlAkT36cTK9Pktl37FSwSxORLBJQqJhZCTOLN7MN3tfiGczr5s3ZYGbdvLHCZrbMbztgZkP9ntPRzNaY2WozG5tmf0XMLMXMhqfzWpPNbFUg65Ir66ayRZnUuynPtrmBmT/so1VcIl8mq0GlSE4Q6JHKQGCWc64aMMu7/ytmVgJ4EWgINABeNLPizrljzrm6qRuwDZjgPacaMAho6pyrCfRLs9vBQGI6r3UfoF4hISBPeBhP3VaVqX2aU7V0If70xXK6vb+ElMMng12aiAQg0FBpD4zxbo8BOqQzpw0Q75w75Jw7DMQDbf0nmFk0UBqY4w11B0Z483HO7fObWx8oA8xIs49CwADgbwGuSa6iqqUL8cUTjfnrPTVJ2uprUDlm/lY1qBQJUYGGShnn3G7v9h583+zTKgvs8Luf4o356wSMc7+c/4gGos1snpktNLO2AGYWBrwGPJPO6wz2Hsv0R10z62FmSWaWtH+/PuIabGFhRrcmlZjeL5b6FYvz4uTVdHx7AZvUoFIk5GQaKmY208xWpbO195/nBcLl/njZCfjU734EUA24FegMjDazYkAvYKpzLiVNjXWBKs65ry7mxZxzo5xzMc65mFKlSl1myZLVypcowId/bMC/f1+HDfuOc8ewOYyYvZGzalApEjIiMpvgnGuZ0WNmttfMopxzu80sCtiXzrSd+MIhVTngO7991AEinHPJfnNSgEXOubPAFjNbjy9kGgPNzawXUAjIa2bH8f0+JsbMtnprKm1m3znn/F9XQoCZ8UD9csRGl+Slyav5v+nrmLpyN6/eX5ubyhYNdnkikolAT39NBrp5t7sBk9KZMx1obWbFvU+HtfbGUnXm10cpABPxgsjMSuI7HbbZOdfFOVfBOVcJ3ymwD51zA51zbznnrvPGmwHrFSihrXThfLzZpT4j/1CPvUdP037EPP41bS2nzqpBpUh2FmiovAK0MrMNQEvvPmYWY2bvADjnDuH7fccSb3vZG0vVkd+GynTgoJmtAWYDzzrnDgZYq4SgtjdFMWtAC+67uSxvfreJdsPmsGTrocyfKCJBYbn9bwNiYmJcUlJSsMuQi5C4fj+DJqxk548/8XDjivy5bXUKRWZ6BldErgAzS3bOxaQd11/US8iIjS7FjP6xPNKkEh8t3EabIYkkrNen90SyE4WKhJSCkRG8dE9Nxj/ZmHx5wuj23mIGfL6MH0+eCXZpIoJCRUJU/Yol+KZPc3rfVpXJy3bRMi6BqSt3Z/5EEbmiFCoSsvLlCeeZNjcwqXdTri2aj16fLOWJj5LYd1QNKkWCRaEiIa/mdUWZ2Kspz7Wtzux1+2kZl8DnSTvUoFIkCBQqkiNEhIfR89YqTOvbnOrXFuHP41fQ9d3F7DikBpUiV5NCRXKUyqUK8VmPRgzucBPfbz9M6yGJvD9vCz+rQaXIVaFQkRwnLMzo2qgiMwa0oGHlEvx1yhp+P3I+G/cdC3ZpIjmeQkVyrLLF8vP+I7cw5ME6bD5wgnbD5jL8PxvUoFLkClKoSI5mZtx7czlmDmhBq5pl+PeM9dz9xlxWphwJdmkiOZJCRXKFkoUiGfFQPd7uWp9DJ87QfsRc/vntD2pQKZLFFCqSq7SpeS3xA1rQMaY8byds5o5hc1i0Wb1KRbKKQkVynaL58/DK/bX55PGGnDt/ngdHLeR/J67k2KmzwS5NJOQpVCTXalq1JNP7xfJYs+v5ZNF22gxJZPba9K4zJyIXS6EiuVqBvBG8cFcNvuzZhIKRETz6wRL6j1vGoRNqUClyORQqIkC9CsX5uk8z+vyuGlOW76JVXAJTlu9SqxeRS6RQEfFERoQzoFU0U55uRtni+Xn60+/p/mEye9WgUuSiKVRE0rgxqggTejbhf9pVZ84GX4PKzxZv11GLyEVQqIikIyI8jB6xVZjeL5YaUUUYOGElXd5ZxPaDalApciEKFZELqFSyIJ92b8Q/7q3FipQjtB6awDtzNqtBpUgGFCoimQgLMx5qWIH4AbE0qVKSv33zA/e/NZ91e9SgUiQthYrIRYoqmp93u8UwrFNdth86yV1vzGHozPWcOacGlSKpFCoil8DMaF+3LPH9Y2lXK4qhMzdw9xtzWb7jx2CXJpItKFRELsM1hSIZ1ulm3nk4hiM/neXeN+fx92/W8NMZNaiU3E2hIhKAljXKMGNALJ0aVGD0nC20HZbIgk1qUCm5l0JFJEBF8uXhH/fWYmz3hgB0Hr2QQRNWclQNKiUXUqiIZJEmVUoyrW8sPWIrM27JdlrHJTJzzd5glyVyVQUUKmZWwszizWyD97V4BvO6eXM2mFk3b6ywmS3z2w6Y2VC/53Q0szVmttrMxqbZXxEzSzGz4X5j35nZOr/9lQ5kbSKXI3/ecP6n3Y1M6NWUovnz8PiHSfT59HsOHj8d7NJEropAj1QGArOcc9WAWd79XzGzEsCLQEOgAfCimRV3zh1zztVN3YBtwATvOdWAQUBT51xNoF+a3Q4GEtOpp4vfPtXDXIKmbvliTHm6Gf1bRvPtqt20jEtg0rKdavUiOV6godIeGOPdHgN0SGdOGyDeOXfIOXcYiAfa+k8ws2igNDDHG+oOjPDm4x8QZlYfKAPMCLB2kSsqb0QYfVtW45s+zal4TUH6fraMx8cksfvIT8EuTeSKCTRUyjjndnu39+D7Zp9WWWCH3/0Ub8xfJ2Cc++XHuGgg2szmmdlCM2sLYGZhwGvAMxnU87536usFM7OMijazHmaWZGZJ+/fvv+ACRQIVXaYwX/Zswv/eeSPzNh2gVVwinyzaxnm1epEcKNNQMbOZZrYqna29/zwvEC73/5JOwKd+9yOAasCtQGdgtJkVA3oBU51zKenso4tzrhbQ3Nu6ZvRizrlRzrkY51xMqVKlLrNkkYsXHmY83rwyM/q1oHa5ojz/1SoeemchWw+cCHZpIlkq01BxzrV0zt2UzjYJ2GtmUQDe1/R+j7ETKO93v5w3hve8OkCEcy7Zb04KMNk5d9Y5twVYjy9kGgO9zWwr8G/gYTN7xatzp/f1GDAW3+9vRLKVCtcU4JPHG/LKfbVYvfMobYYmMipxE+d+VqsXyRkCPf01Gejm3e4GTEpnznSgtZkV9z4d1tobS9WZXx+lAEzEd5SCmZXEdzpss3Oui3OugnOuEr5TYB865waaWYQ3DzPLA9wFrApwbSJXhJnRqUEF4ge0oHm1Uvxj6lrue2s+P+w+GuzSRAIWaKi8ArQysw1AS+8+ZhZjZu8AOOcO4fu01hJve9kbS9WR34bKdOCgma0BZgPPOucu9GfKkcB0M1sBLMN3JDQ6wLWJXFHXFs3H6IfrM/yhm9l5+CfufmMucfHrOX1OrV4kdFlu/4hjTEyMS0pKCnYZkssdPnGGl79ew1ff76Ra6UK8+kBt6lVI98++RLIFM0t2zsWkHddf1ItkA8UL5mXIg3V5/5FbOH76HPe/NZ/BX6/h5JlzwS5N5JIoVESykduql2ZG/1i6NKzAu3O30GZoIvM2Hgh2WSIXTaEiks0UzpeHv3WoxbgejYgIC6PLO4t4bvwKjvykBpWS/SlURLKphpWv4du+zXmyRRXGL02hVVwCM1bvCXZZIhekUBHJxvLlCWfgHdWZ2Ksp1xSKpMdHyTw1din7j6lBpWRPChWREFCrXFEm927KM62jiV+9l1ZDEvjq+xQ1qJRsR6EiEiLyhIfR+/ZqTO3bjMolC9J/3HIe/WAJO39Ug0rJPhQqIiGmaunCfPFkE168uwaLNh+idVwCHy3YqgaVki0oVERCUHiY8WjT65nRP5abKxTnhUmr6TRqIZv3Hw92aZLLKVREQlj5EgX46LEG/OuB2qzdc5S2w+bw1ndqUCnBo1ARCXFmRseY8swc0ILbbijFq9PW0uHNeazZpQaVcvUpVERyiNJF8vF21xje6lKPPUdOc8/wufx7+jpOnVWDSrl6FCoiOcwdtaKYOSCW9nXLMnz2Ru58fQ7J2w5l/kSRLKBQEcmBihXIy2sd6zDmjw04dfY8D4xcwEuTV3PitBpUypWlUBHJwVpEl2J6/1geblSRMQu20npIIonr9we7LMnBFCoiOVyhyAj+2v4mPn+iMZF5wnj4vcU888VyjpxUg0rJegoVkVzilkolmNqnOb1urcJX3++k5ZAEpq3aHeyyJIdRqIjkIvnyhPPnttWZ9FRTShWK5MmPl9Lz42T2HTsV7NIkh1CoiORCN5UtyqTeTXm2zQ3MWruPVnGJjE9Wg0oJnEJFJJfKEx7GU7dVZWqf5lQrXYhnvljOw+8tZsehk8EuTUKYQkUkl6tauhCfP9GYl9vXZOm2w7QZmsgH87aoQaVcFoWKiBAWZjzcuBLT+8cSU6kEL01ZQ8e3F7BxnxpUyqVRqIjIf5UrXoAxj97Ca7+vw4Z9x2k3bA4jZm/krBpUykVSqIjIr5gZ99cvx8wBLWhZozT/N30d7YfPY9XOI8EuTUKAQkVE0lWqcCRvdqnPyD/UY//x07QfMY9Xp61Vg0q5IIWKiFxQ25uimNm/BffXK8tb322i3bA5LNmqBpWSPoWKiGSqaIE8/OuBOnz8WEPO/Hye349cwF8mreK4GlRKGgoVEblozaqVZHq/WB5tWomPFm6jzZBEvlu3L9hlSTYSUKiYWQkzizezDd7X4hnM6+bN2WBm3byxwma2zG87YGZD/Z7T0czWmNlqMxubZn9FzCzFzIb7jeU1s1Fmtt7M1prZ/YGsTUTSVzAyghfvrsn4J5uQP284j7y/hAGfL+PwiTPBLk2ygUCPVAYCs5xz1YBZ3v1fMbMSwItAQ6AB8KKZFXfOHXPO1U3dgG3ABO851YBBQFPnXE2gX5rdDgYS04w9D+xzzkUDNYCEANcmIhdQv2JxvunTjKdvr8rkZbtoNSSBqSt3q9VLLhdoqLQHxni3xwAd0pnTBoh3zh1yzh0G4oG2/hPMLBooDczxhroDI7z5OOf2+c2tD5QBZqR5nT8C//Tmn3fOHQhgXSJyESIjwvlT6xuY3LsZUUXz0+uTpTz5cTL7jqpBZW4VaKiUcc6l9s7eg++bfVplgR1+91O8MX+dgHHulx9xooFoM5tnZgvNrC2AmYUBrwHP+D/ZzIp5Nweb2VIz+8LM0qsldX4PM0sys6T9+3XBIpFA1biuCF/1asKgO6rz3br9/C4ugc+X7NBRSy6UaaiY2UwzW5XO1t5/nhcIl/svqBPwqd/9CKAacCvQGRjtBUcvYKpzLiXN8yOAcsB851w9YAHw74xezDk3yjkX45yLKVWq1GWWLCL+IsLDeKJFFb7t25wbo4rw5y9X0PVdNajMbSIym+Cca5nRY2a218yinHO7zSwKSO9jIDvxhUOqcsB3fvuoA0Q455L95qQAi5xzZ4EtZrYeX8g0BpqbWS+gEJDXzI7j+/3LSbzfyQBfAI9ltjYRyXqVSxXis+6NGLt4O698u5bWQxJ5ts0NdGtSifAwC3Z5coUFevprMtDNu90NmJTOnOlAazMr7n06rLU3lqozvz5KAZiIF0RmVhLf6bDNzrkuzrkKzrlK+E6BfeicG+gdJU3hl/D6HbAmsKWJyOUKCzP+0KgiM/rH0rByCV7+eg2/HzmfDXuPBbs0ucICDZVXgFZmtgFo6d3HzGLM7B0A59whfJ/WWuJtL3tjqTry21CZDhw0szXAbOBZ59zBTGp5DnjJzFYAXYE/BbQyEQnYdcXy8/4jtzD0wbpsOXCCO1+fyxuzNqhBZQ5muf0XaTExMS4pKSnYZYjkeAeOn+alyav5esVuql9bmH89UJva5Ypl/kTJlsws2TkXk3Zcf1EvIldFyUKRDH+oHqO61ufwyTN0GDGPf079QQ0qcxiFiohcVa1rXsuM/i148JbyvJ24mbZDE1m4ObOz2xIqFCoictUVzZ+Hf95Xm7GPN+S8g06jFvL8Vys5dupssEuTAClURCRomlQtybR+zXm82fV8ung7rYckMnutGlSGMoWKiARVgbwR/O9dNfiyZxMKRUbw6AdL6PfZ9xxSg8qQpFARkWzh5grF+bpPM/r+rhrfrNxNq7gEpizfpVYvIUahIiLZRmREOP1bRTPl6WaUK56fpz/9nu4fJrPniBpUhgqFiohkO9WvLcKEXk15vt2NzN24n1ZxCXy6eLuOWkKAQkVEsqXwMKN7bGWm9Y2lZtkiDJqwkodGL2LbwRPBLk0uQKEiItlapZIFGft4I/5xby1W7TxCm6GJvDNnMz+f11FLdqRQEZFsLyzMeKhhBWYMiKVplZL87ZsfuO+t+azbowaV2Y1CRURCRlTR/LzTLYbXO9/MjkMnueuNOQyduZ4z59SgMrtQqIhISDEz7qlzHTMHtKBdrSiGztzA3W/MZdmOH4NdmqBQEZEQVaJgXoZ1upl3u8Vw5Kez3PfmPP7+zRp+OqMGlcGkUBGRkPa7G8swY0AsnRpUYPScLbQZmsj8TQeCXVaupVARkZBXJF8e/nFvLT7t3ggzeGj0IgZNWMlRNai86hQqIpJjNK5yDdP6xvJEbGXGLdlOq7gEZq7ZG+yychWFiojkKPnzhjOo3Y1MfKopxQvk5fEPk3j60+85ePx0sEvLFRQqIpIj1S5XjMm9mzGgVTTTVu2mZVwCk5btVKuXK0yhIiI5Vt6IMPr8rhrf9GlOxWsK0vezZTw2JoldP/4U7NJyLIWKiOR40WUK82XPJrxwVw0WbDpI6yGJfLJoG+fV6iXLKVREJFcIDzMea3Y90/vFUqd8UZ7/ahWdRy9kywE1qMxKChURyVUqXFOAjx9ryKv312LN7qO0HZrI2wmbOPezWr1kBYWKiOQ6ZsaDt1Rg5oAWxEaX4p/fruW+t+bzw+6jwS4t5ClURCTXKlMkH6O61mfEQ/XY9eNP3P3GXOJmrOP0ObV6uVwKFRHJ1cyMO2tHEd+/BffUuY7X/7ORu16fy9Lth4NdWkhSqIiIAMUL5iXuwbq8/+gtnDh9jvvfms/LU9Zw8sy5YJcWUgIKFTMrYWbxZrbB+1o8g3ndvDkbzKybN1bYzJb5bQfMbKjfczqa2RozW21mY9Psr4iZpZjZ8IvZl4jIxbrthtJM7x/LHxpW5L15vgaV8zaqQeXFCvRIZSAwyzlXDZjl3f8VMysBvAg0BBoAL5pZcefcMedc3dQN2AZM8J5TDRgENHXO1QT6pdntYCAx9c6F9iUicqkK58vD4A438fkTjYkIC6PLO4t4bvwKjvykBpWZCTRU2gNjvNtjgA7pzGkDxDvnDjnnDgPxQFv/CWYWDZQG5nhD3YER3nycc/v85tYHygAz0isonX2JiFyWBteX4Nu+zel5axXGL02hVVwC01fvCXZZ2VqgoVLGObfbu70H3zf7tMoCO/zup3hj/joB49wvTXmigWgzm2dmC82sLYCZhQGvAc9coKa0+/oNM+thZklmlrR///4L7EpEcrt8ecJ5rm11JvZqyjWFInnio2Se+mQp+4+pQWV6Mg0VM5tpZqvS2dr7z/O+iV9uz4NOwKd+9yOAasCtQGdgtJkVA3oBU51zKZewr99wzo1yzsHfsC4AAAvtSURBVMU452JKlSp1mSWLSG5Sq1xRJvduyrNtbiB+zV5aDUlgwtIUNahMIyKzCc65lhk9ZmZ7zSzKObfbzKKAfelM24kvHFKVA77z20cdIMI5l+w3JwVY5Jw7C2wxs/X4QqYx0NzMegGFgLxmdtw5N/AC+xIRyRJ5wsN46raqtKlZhj+PX8GAz5czefku/n5vLcoWyx/s8rKFQE9/TQa6ebe7AZPSmTMdaG1mxb1Ph7X2xlJ15rdHFhPxgsjMSuI7HbbZOdfFOVfBOVcJ3ymwD1MD5QL7EhHJUlVLF+aLJ5vw0t01WLzlEK3jEvhowVY1qCTwUHkFaGVmG4CW3n3MLMbM3gFwzh3C92mtJd72sjeWqiO/DYLpwEEzWwPMBp51zh28iHrS25eISJYLDzMeaeprUFmvYnFemLSaB0ctYNP+48EuLagst58PjImJcUlJScEuQ0RCmHOO8ckpDP56DafOnadfy2r0aF6ZiPCc+/flZpbsnItJO55zVywicpWYGb+PKc/MP7Xg9htK869p6+jw5jxW7zoS7NKuOoWKiEgWKV04HyO71uetLvXYc+Q09wyfx/9NX8ups7mnQaVCRUQki91RK4qZA2LpULcsI2Zv4s7X55C87VDmT8wBFCoiIldAsQJ5ea1jHcb8sQGnzp7ngZELeGnyak6cztkNKhUqIiJXUIvoUszoH0u3xpUYs2ArrYckkrg+53byUKiIiFxhBSMjeOmemnzxRGMi84Tx8HuLeeaL5fx48kywS8tyChURkaskplIJpvZpzlO3VeGr73fSMi6Rb1fuzvyJIUShIiJyFeXLE86zbaozuXdTyhSJpOcnS+n5cTL7jp0KdmlZQqEiIhIENa8rysSnmvJc2+rMWruPVnGJfJG0I+QbVCpURESCJE94GD1vrcK3fZsTXaYQz45fwcPvLWbHoZPBLu2yKVRERIKsSqlCjOvRmMHta7J022HaDE3kg3lbQrJBpUJFRCQbCAszujauxPT+sdxSqQQvTVnD799ewMZ9x4Jd2iVRqIiIZCPlihfgg0dvIa5jHTbtP067YXMZMXsjZ38+H+zSLopCRUQkmzEz7qtXjvj+LWhVowz/N30d7YfPY9XO7N+gUqEiIpJNlSocyYgu9Rj5h/rsP36a9iPm8eq07N2gUqEiIpLNtb3pWmb2b8ED9crx1nebaDdsDou3ZM8GlQoVEZEQULRAHl59oDYfP9aQMz+fp+PbC3hh4iqOZ7MGlQoVEZEQ0qxaSWb0j+WPTa/n40XbaB2XwOx1+4Jd1n8pVEREQkyBvBH85e4ajH+yCQUiI3j0/SUMGLeMwyeC36BSoSIiEqLqVyzON32a0ef2qkxevotWQxL4ZsXuoLZ6UaiIiISwyIhwBrS+gSlPNyOqaH6eGruUJz5KZu/R4DSoVKiIiOQAN0YV4ateTRh0R3US1u+nZVwC45Zsv+pHLQoVEZEcIiI8jCdaVGFav1hujCrCc1+u5A/vLmL7wavXoFKhIiKSw1xfsiCfdW/E3zrcxPIdR2gzNJF3527h56vQoFKhIiKSA4WFGX9oVJEZ/WNpVLkEg79ewwMj57Nh75VtUKlQERHJwa4rlp/3HrmFYZ3qsvXACe58fS6vz9rAmXNXpkGlQkVEJIczM9rXLcvMAS1oc9O1xMWv557hc6/IJ8QCChUzK2Fm8Wa2wftaPIN53bw5G8ysmzdW2MyW+W0HzGyo33M6mtkaM1ttZmPT7K+ImaWY2XC/sc5mttLMVpjZNDMrGcjaRERymmsKRfJG55sZ/XAMFa8pQMlCkVn+GhbIx83M7F/AIefcK2Y2ECjunHsuzZwSQBIQAzggGajvnDucZl4y0N85l2hm1YDPgdudc4fNrLRzbp/f3GFAKe+1e5tZBLALqOGcO+DVddI591Jma4iJiXFJSUmX/d9ARCQ3MrNk51xM2vFAT3+1B8Z4t8cAHdKZ0waId84d8oIkHmibprhooDQwxxvqDoxIDZ40gVIfKAPM8N+FtxU0MwOK4AsZERG5igINlTLOud3e7T34vtmnVRbY4Xc/xRvz1wkY5345bIoGos1snpktNLO2AGYWBrwGPOP/ZOfcWaAnsBLviAV497JXJSIilyUiswlmNhO4Np2Hnve/45xzZna559I6AV3T1FUNuBUoBySaWS3gD8BU51yK74DkvzXmwRcqNwObgTeAQcDf0nsxM+sB9ACoUKHCZZYsIiJpZRoqzrmWGT1mZnvNLMo5t9vMooD0+i/vxBcOqcoB3/ntow4Q4ZxL9puTAizyjkC2mNl6fCHTGGhuZr2AQkBeMzsOfOnVusnb5+fAwAusaRQwCny/U8lonoiIXJpAT39NBrp5t7sBk9KZMx1obWbFvU+HtfbGUnUGPk3znIl4QeR9iisa2Oyc6+Kcq+Ccq4TvFNiHzrmB+IKrhpmV8p7fCvghwLWJiMglyvRIJROvAJ+b2WPANqAjgJnFAE865x53zh0ys8HAEu85Lzvn/K+D2RFol2a/qUG0BvgZeNY5dzCjIpxzu8zsr/hOk531ankkwLWJiMglCugjxTmBPlIsInLprtRHikVERP4r1x+pmNl+fKfLLkdJ4EAWlhNMWkv2pLVkPzllHRDYWio650qlHcz1oRIIM0tK7/AvFGkt2ZPWkv3klHXAlVmLTn+JiEiWUaiIiEiWUagEZlSwC8hCWkv2pLVkPzllHXAF1qLfqYiISJbRkYqIiGQZhYqIiGQZhcplMLO2ZrbOzDZ6FycLKWa21btK5jIzS/LGLuoqnsFmZu+Z2T4zW+U3lm7t5vO69z6tMLN6wav8tzJYy0tmttPviqjt/B4b5K1lnZm1CU7V6TOz8mY22+9qrX298ZB7by6wlpB7b8wsn5ktNrPl3lr+6o1fb2aLvJrHmVlebzzSu7/Re7zSJb+oc07bJWxAOLAJqAzkBZbju+Jk0Gu7hDVsBUqmGfsXMNC7PRB4Ndh1ZlB7LFAPWJVZ7fh6yn2L7wJujfB1vg76GjJZy0vAM+nMreH9W4sErvf+DYYHew1+9UUB9bzbhYH1Xs0h995cYC0h9954/30LebfzAIu8/96fA5288ZFAT+92L2Ckdzv1OleX9Jo6Url0DYCNzrnNzrkzwGf4roAZ6i7mKp5B55xLBA6lGc6o9vb4Olk759xCoJh3iYZsIYO1ZKQ98Jlz7rRzbguwEd+/xWzBObfbObfUu30MX5fwsoTge3OBtWQk27433n/f497dPN7mgNuB8d542vcl9f0aD/zO/C9edREUKpfuYq5kmd05YIaZJXsXLIOLu4pndpVR7aH6XvX2Tgm953caMmTW4p0yuRnfT8Uh/d6kWQuE4HtjZuFmtgzf9a7i8R1J/eicO+dN8a/3v2vxHj8CXHMpr6dQyZ2aOefqAXcAT5lZrP+DznfsG5KfNQ/l2j1vAVWAusBufJfPDhlmVgjfRfP6OeeO+j8Wau9NOmsJyffGOfezc64uvgskNgCqX8nXU6hcup1Aeb/75byxkOGc2+l93Qd8he8f2t7U0w+W8VU8s6uMag+598o5t9f7JnAeGM0vp1Gy/VrMd1nvL4FPnHMTvOGQfG/SW0sovzcAzrkfgdn4rqBbzMxSr6flX+9/1+I9XhTI8FpW6VGoXLolQDXv0xN58f0ya3KQa7poZlbQzAqn3sZ3Jc5VXNxVPLOrjGqfDDzsfdKoEXDE71RMtpTm9wr34ntvwLeWTt6nc67Hd3ntxVe7vox4593fBX5wzsX5PRRy701GawnF98bMSplZMe92fn65Ku5s4AFvWtr3JfX9egD4j3eEefGC/emEUNzwfXJlPb5zk88Hu55LrL0yvk+qLAdWp9aP77zpLGADMBMoEexaM6j/U3ynHs7iOxf8WEa14/vkywjvfVoJxAS7/otYy0derSu8/8Gj/OY/761lHXBHsOtPs5Zm+E5trQCWeVu7UHxvLrCWkHtvgNrA917Nq4C/eOOV8QXfRuALINIbz+fd3+g9XvlSX1NtWkREJMvo9JeIiGQZhYqIiGQZhYqIiGQZhYqIiGQZhYqIiGQZhYqIiGQZhYqIiGSZ/wdnZ9REHWbmYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.rand(X_train.shape[1],1)  # assuming X is N-by-n. \n",
    "                                        # if X is n-by-N, use X_train.shape[0]\n",
    "y_train = y_train.reshape((-1,1))\n",
    "y_test = y_test.reshape((-1,1))\n",
    "print(w.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "b = 0\n",
    "w, b, loss = train(w, b, X_train, y_train, iter=300, lr=1e-6)\n",
    "plt.figure()\n",
    "plt.plot(loss)\n",
    "\n",
    "#training accuracy \n",
    "z = model(w,b,X_train)\n",
    "print(accuracy(np.squeeze(y_train), predict(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBm8ESACmrxe"
   },
   "source": [
    "To see how well our model performs, we compute its accuracy on the testing dataset (X_test, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pt9_Aiw-zqP6",
    "outputId": "fa14dce0-2cff-4d90-e0c9-b7862ccc475e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In model, X: (500, 4), b: -3.8837244831310744e-07, w: (4, 1)\n",
      "[1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1\n",
      " 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1\n",
      " 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0\n",
      " 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
      " 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1\n",
      " 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
      " 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
      " 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1\n",
      " 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 1 0\n",
      " 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1\n",
      " 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0]\n",
      "[1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1\n",
      " 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0\n",
      " 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
      " 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1\n",
      " 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0\n",
      " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
      " 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1\n",
      " 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0\n",
      " 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 1 0\n",
      " 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
      " 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0]\n",
      "0.888\n"
     ]
    }
   ],
   "source": [
    "z = model(w,b,X_test)\n",
    "y_test=np.squeeze(y_test)\n",
    "print(y_test)\n",
    "print(predict(z))\n",
    "print(accuracy(y_test, predict(z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "id": "u-YnkECyDJXw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef5x5LENm7_s"
   },
   "source": [
    "Now, we look at a real-world dataset. [The Bank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#) is available at UCI's Machine Learning Repository. Colab can read this dataset directly from [GitHub](https://github.com/madmashup/targeted-marketing-predictive-engine) using pandas package: pd.read_csv. The data is in the DataFrame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5vKPwXfYgLV",
    "outputId": "40d9d2a0-512c-48ae-8b8c-3698534e4eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41188, 21)\n",
      "['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/madmashup/targeted-marketing-predictive-engine/master/banking.csv'\n",
    "data = pd.read_csv(url)\n",
    "print(data.shape)\n",
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG9UWfJ8n2Jr"
   },
   "source": [
    "This dataset is pretty large and cause my machine to crash. I remove some fileds. [This Webpage](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8) has a good description of this dataset. Note that you are not allowed to use any existing model such as those used in that Webpage for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGiNyyIsvUw-",
    "outputId": "bfcc00f4-baf1-46d6-c02f-21a21911cff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'job', 'marital', 'housing', 'loan', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']\n",
      "(41188, 16)\n"
     ]
    }
   ],
   "source": [
    "cat_vars=['default','education','contact','month','day_of_week',]\n",
    "data=data.drop(cat_vars, axis=1)\n",
    "print(list(data.columns))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVSJRTDeoHNj"
   },
   "source": [
    "Some data columns have k class labels. This is best represented as k columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "du0e-Dhyg2FV",
    "outputId": "a274bfd0-0b9a-4bd5-96d1-ee5be28b791c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job\n",
      "marital\n",
      "housing\n",
      "loan\n",
      "poutcome\n",
      "(41188, 36)\n",
      "['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y', 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown', 'divorced', 'married', 'single', 'unknown', 'no', 'unknown', 'yes', 'no', 'unknown', 'yes', 'failure', 'nonexistent', 'success']\n"
     ]
    }
   ],
   "source": [
    "cat_vars=['job','marital','housing','loan','poutcome']\n",
    "for va in cat_vars:\n",
    "    #cat_pre='var'+'_'+var\n",
    "    print(va)\n",
    "    #print(data[va])\n",
    "    cat_list = pd.get_dummies(data[va])\n",
    "    data1=pd.concat([data,cat_list], axis=1)\n",
    "    data=data1.drop(va, axis=1)\n",
    "    #print(list(cat_list.columns))\n",
    "    #print(list(data.columns))\n",
    "    #print(data.shape)\n",
    "\n",
    "print(data.shape)\n",
    "print(list(data.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NazsRFZmpIuD"
   },
   "source": [
    "We now split the data into input data X and the label y. We covert them to numpy and split them into training and testing datasets with 30% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2kDuXGHtBdB",
    "outputId": "c5a003c8-636d-4aef-cf69-1a81ad01ea20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 35)\n",
      "(12357, 35)\n",
      "Index(['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate',\n",
      "       'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'admin.',\n",
      "       'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired',\n",
      "       'self-employed', 'services', 'student', 'technician', 'unemployed',\n",
      "       'unknown', 'divorced', 'married', 'single', 'unknown', 'no', 'unknown',\n",
      "       'yes', 'no', 'unknown', 'yes', 'failure', 'nonexistent', 'success'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = data.loc[:, data.columns != 'y']\n",
    "y = data.loc[:, data.columns == 'y']\n",
    "columns = X.columns\n",
    "X=X.to_numpy()\n",
    "y=y.to_numpy()\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train1.shape)\n",
    "print(X_test1.shape)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngDOmRz9pxyR"
   },
   "source": [
    "Now, train and test as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tHoIcRBXN_bG",
    "outputId": "5b316202-9fb1-4957-9723-6a47cc118cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1)\n",
      "(28831, 35)\n",
      "(28831, 1)\n",
      ">> (28831, 35)\n",
      "In model, X: (28831, 35), b: 0, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 0 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -8.867885262391179e-06, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 1 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -1.7735770524782347e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 2 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -2.6603655787173526e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 3 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -3.5471541049564715e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 4 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -4.4339426311955874e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 5 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -5.320731157434703e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 6 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -6.207519683673821e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 7 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -7.094308209912939e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 8 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -7.981096736152056e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 9 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -8.867885262391175e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 10 Loss: -2.3455799047037054e-249\n",
      "In model, X: (28831, 35), b: -9.754673788630295e-05, w: (35, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-389-0efe7576d8b3>:21: RuntimeWarning: overflow encountered in exp\n",
      "  return 1./(1+np.exp(t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient shape: (35, 1)\n",
      "Iter: 11 Loss: -8.307599990319254e-150\n",
      "In model, X: (28831, 35), b: -0.0001064146231486941, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 12 Loss: -2.9636837671935745e-50\n",
      "In model, X: (28831, 35), b: -0.00011528250841108528, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 13 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00012391329764798416, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 14 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013242392819222203, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 15 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014092658158613733, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 16 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014149443391298119, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 17 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001417793657796169, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 18 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001419287054571172, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 19 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014201137612871635, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 20 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014205654888205672, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 21 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014207706689033228, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 22 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014208247142094083, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 23 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014207801748498237, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 24 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014206681067205213, w: (35, 1)\n",
      "gradient shape: (35, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-389-0efe7576d8b3>:16: RuntimeWarning: overflow encountered in exp\n",
      "  return z - np.log(1+np.exp(z))\n",
      "<ipython-input-389-0efe7576d8b3>:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  return (-(y*log_sig(z)) - ((1-y)*log_one_sig(z)))\n",
      "<ipython-input-389-0efe7576d8b3>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 25 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014205088622658887, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 26 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014203164371267494, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 27 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014201005929904246, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 28 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014198681761608527, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 29 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014196240087516207, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 30 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014193714992327157, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 31 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014191130632362772, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 32 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014188504158306505, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 33 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014185847760563716, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 34 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014183170109282041, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 35 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001418047737260881, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 36 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014177773938845958, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 37 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.000141750629295902, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 38 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014172346564784633, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 39 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014169626422591883, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 40 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014166903624450043, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 41 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.000141641789668593, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 42 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014161453015223, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 43 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014158726170653796, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 44 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014155998717518852, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 45 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014153270857265017, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 46 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001415054273247356, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 47 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014147814443959446, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 48 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014145086062921, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 49 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014142357639568891, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 50 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001413962920925179, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 51 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014136900796803055, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 52 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.000141341724196236, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 53 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014131444089867488, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 54 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001412871581599057, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 55 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014125987603847443, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 56 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014123259457467908, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 57 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014120531379606434, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 58 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014117803372130634, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 59 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014115075436295824, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 60 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014112347572938882, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 61 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014109619782614948, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 62 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014106892065693721, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 63 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014104164422427156, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 64 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014101436852996866, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 65 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001409870935754725, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 66 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001409598193620845, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 67 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014093254589112145, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 68 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014090527316402217, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 69 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014087800118241852, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 70 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001408507299481803, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 71 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001408234594634423, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 72 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.000140796189730618, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 73 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014076892075240416, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 74 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014074165253177888, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 75 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014071438507199454, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 76 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014068711837656776, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 77 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014065985244926654, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 78 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014063258729409602, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 79 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014060532291528265, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 80 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014057805931725777, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 81 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014055079650464092, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 82 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014052353448222203, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 83 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014049627325494443, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 84 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014046901282788743, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 85 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014044175320624888, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 86 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014041449439532828, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 87 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014038723640051004, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 88 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014035997922724763, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 89 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014033272288104738, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 90 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014030546736745376, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 91 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014027821269203464, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 92 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001402509588603675, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 93 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001402237058780266, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 94 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014019645375057058, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 95 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014016920248353118, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 96 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001401419520824029, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 97 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001401147025526334, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 98 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014008745389961524, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 99 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001400602061286781, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 100 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00014003295924508284, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 101 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.000140005713254016, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 102 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013997846816058576, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 103 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013995122396981898, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 104 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013992398068665945, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 105 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013989673831596726, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 106 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013986949686251935, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 107 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001398422563310113, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 108 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013981501672606018, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 109 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013978777805220886, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 110 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013976054031393116, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 111 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013973330351563833, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 112 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013970606766168641, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 113 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001396788327563851, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 114 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013965159880400716, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 115 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013962436580879916, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 116 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013959713377499305, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 117 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013956990270681862, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 118 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001395426726085167, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 119 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013951544348435346, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 120 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013948821533863484, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 121 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013946098817572215, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 122 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013943376200004786, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 123 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013940653681613203, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 124 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013937931262859903, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 125 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013935208944219457, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 126 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013932486726180303, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 127 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013929764609246466, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 128 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001392704259393931, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 129 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001392432068079925, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 130 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013921598870387484, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 131 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013918877163287673, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 132 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013916155560107573, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 133 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001391343406148067, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 134 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013910712668067717, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 135 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013907991380558213, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 136 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013905270199671835, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 137 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013902549126159762, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 138 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001389982816080593, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 139 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013897107304428164, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 140 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001389438655787922, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 141 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013891665922047717, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 142 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013888945397858943, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 143 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013886224986275517, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 144 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001388350468829791, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 145 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001388078450496487, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 146 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001387806443735362, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 147 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001387534448657997, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 148 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001387262465379822, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 149 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013869904940200904, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 150 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001386718534701837, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 151 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001386446587551817, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 152 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013861746527004269, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 153 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013859027302816067, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 154 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013856308204327237, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 155 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001385358923294436, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 156 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013850870390105387, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 157 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013848151677277868, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 158 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013845433095957028, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 159 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013842714647663634, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 160 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001383999633394169, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 161 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013837278156355904, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 162 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013834560116489033, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 163 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013831842215939038, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 164 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001382912445631607, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 165 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.000138264068392393, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 166 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013823689366333633, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 167 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001382097203922632, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 168 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013818254859543407, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 169 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013815537828906156, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 170 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001381282094892739, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 171 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013810104221207787, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 172 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001380738764733215, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 173 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001380467122886575, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 174 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001380195496735065, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 175 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001379923886430212, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 176 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001379652292120519, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 177 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013793807139511296, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 178 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013791091520635107, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 179 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013788376065951605, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 180 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013785660776793326, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 181 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001378294565444793, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 182 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013780230700156034, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 183 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013777515915109402, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 184 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013774801300449463, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 185 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013772086857266226, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 186 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013769372586597574, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 187 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013766658489428995, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 188 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013763944566693722, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 189 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013761230819273293, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 190 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001375851724799858, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 191 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013755803853651203, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 192 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013753090636965394, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 193 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001375037759863026, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 194 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001374766473929241, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 195 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013744952059558988, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 196 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013742239560000962, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 197 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001373952724115678, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 198 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001373681510353624, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 199 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001373410314762456, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 200 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013731391373886676, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 201 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013728679782771568, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 202 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013725968374716754, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 203 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001372325715015271, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 204 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001372054610950735, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 205 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001371783525321037, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 206 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001371512458169751, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 207 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001371241409541463, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 208 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013709703794821595, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 209 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001370699368039594, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 210 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013704283752636267, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 211 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013701574012065312, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 212 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001369886445923276, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 213 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001369615509471773, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 214 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013693445919130877, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 215 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013690736933116238, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 216 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013688028137352716, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 217 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013685319532555213, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 218 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013682611119475508, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 219 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013679902898902825, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 220 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013677194871664116, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 221 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013674487038624152, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 222 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013671779400685314, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 223 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013669071958787287, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 224 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013666364713906556, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 225 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013663657667055754, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 226 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013660950819282947, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 227 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013658244171670857, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 228 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001365553772533602, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 229 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013652831481427964, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 230 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013650125441128335, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 231 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013647419605650145, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 232 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013644713976236977, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 233 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013642008554162294, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 234 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001363930334072882, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 235 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013636598337267932, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 236 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013633893545139178, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 237 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013631188965729826, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 238 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001362848460045444, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 239 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013625780450754536, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 240 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013623076518098188, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 241 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013620372803979682, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 242 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013617669309919073, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 243 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013614966037461715, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 244 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001361226298817768, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 245 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013609560163661016, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 246 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013606857565528864, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 247 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013604155195420358, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 248 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013601453054995285, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 249 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013598751145932466, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 250 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013596049469927874, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 251 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001359334802869234, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 252 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013590646823949008, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 253 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001358794585743033, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 254 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013585245130874756, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 255 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001358254464602297, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 256 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013579844404613763, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 257 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001357714440837955, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 258 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001357444465904149, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 259 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013571745158304279, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 260 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001356904590785061, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 261 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013566346909335436, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 262 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013563648164379919, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 263 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013560949674565264, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 264 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013558251441426436, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 265 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013555553466445828, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 266 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013552855751046934, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 267 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013550158296588168, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 268 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001354746110435678, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 269 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013544764175563064, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 270 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013542067511334872, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 271 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013539371112712443, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 272 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001353667498064376, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 273 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013533979115980341, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 274 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001353128351947357, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 275 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013528588191771634, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 276 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001352589313341705, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 277 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001352319834484483, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 278 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013520503826381286, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 279 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001351780957824345, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 280 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013515115600539132, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 281 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013512421893267614, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 282 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013509728456320854, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 283 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001350703528948527, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 284 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013504342392444018, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 285 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001350164976477968, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 286 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001349895740597741, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 287 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013496265315428348, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 288 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001349357349243337, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 289 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001349088193620706, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 290 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001348819064588187, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 291 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013485499620512407, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 292 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001348280885907984, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 293 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013480118360496339, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 294 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013477428123609574, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 295 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013474738147207176, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 296 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013472048430021217, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 297 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013469358970732623, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 298 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00013466669767975568, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 299 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0001346398082034177, w: (35, 1)\n",
      "0.8731226804481288\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = np.random.rand(X_train1.shape[1],1)  # assuming X is N-by-n. \n",
    "                                        # if X is n-by-N, use X_train.shape[0]\n",
    "y_train1 = y_train1.reshape(-1,1)\n",
    "y_test1 = y_test1.reshape(-1,1)\n",
    "print(w1.shape)\n",
    "print(X_train1.shape)\n",
    "print(y_train1.shape)\n",
    "b1 = 0\n",
    "w1, b1, loss1 = train(w1, b1, X_train1, y_train1, iter=300, lr=1e-5)\n",
    "plt.figure()\n",
    "plt.plot(loss1)\n",
    "\n",
    "#training accuracy \n",
    "z1 = model(w1,b1,X_train1)\n",
    "print(accuracy(np.squeeze(y_train1), predict(z1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBqSssY6OUGJ",
    "outputId": "de808023-85f5-4d2b-ae07-fae22af4af01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In model, X: (12357, 35), b: -0.00014122144718115427, w: (35, 1)\n",
      "0.8935825847697662\n"
     ]
    }
   ],
   "source": [
    "z1 = model(w1,b1,X_test1)\n",
    "y_test1=np.squeeze(y_test1)\n",
    "print(accuracy(y_test1, predict(z1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "id": "fsI9xje4Oase"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
