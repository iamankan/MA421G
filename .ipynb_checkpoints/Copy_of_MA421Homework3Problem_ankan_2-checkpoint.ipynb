{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnzRlaQaW6y1"
   },
   "source": [
    "In this homework, you will write a python implementation of logistic regression. You will test it on two datasets. \n",
    "First we import some libraries that we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Szla9qyoPuqg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0byO4vq0Xcxz"
   },
   "source": [
    "We define some functions involved. Use the formulations that avoid overflows.  \n",
    "1. sigmoid function sigmoid(t)\n",
    "2. log of sigmoid(t), called log_sig(t)\n",
    "3. log of 1-sigmoid = 1/(1+e^t), called log_one_sig(t)\n",
    "4. cross-entropy loss function given the inputs of label y and prediction y_hat = sigmoid(z), where y, y_hat, and z are vectors of dimension N. (N = # of data points.) You should implement this function with z, rather than y_hat, as the input; namely, the loss function should be\n",
    "\n",
    "    loss = -y log(sigmoid(z)) - (1-y) log (1-sigmoid(z)) \n",
    "\n",
    "  where log(sigmoid(z)) and log (1-sigmoid(z)) should be computed by the functions log_sig(z) and log_one_sig(z) in parts 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "kuzmD54GT9yb"
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "  return 1/(1+np.exp(-t))\n",
    "\n",
    "def customloss(y, z): \n",
    "  # loss function for y and yhat = sigmoid(z)\n",
    "  return (-(y*log_sig(z)) - ((1-y)*log_one_sig(z)))\n",
    "\n",
    "# def log_sig(t):\n",
    "#   return np.log(sigmoid(t))\n",
    "\n",
    "def log_sig(z):\n",
    "  if (z <= 0).any():\n",
    "        return z - np.log(1+np.exp(z))\n",
    "  else:\n",
    "        return -np.log(1+np.exp(-z))\n",
    "\n",
    "def log_one_sig(t):\n",
    "  return 1./(1+np.exp(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "We0D7AEdfn_3",
    "outputId": "1176f685-f395-446d-aff2-5c728a3088f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-268-47a6eab891c2>:1: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RSaRWNd6Y6h2",
    "outputId": "3f293db9-0bca-4a0d-ea79-84526b185f7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_one_sig([-431983.39381293])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FT0pO4uAY_nY",
    "outputId": "687fe2a0-3318-497f-97ce-1a36a588b9c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([431983.39381293])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_sig(np.array([-431983.39381293]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLulJqXcbEpw"
   },
   "source": [
    "Define the model output z=w^T x + b, or z = x^Tw + B, given the data input X (an N-by-n array containing N data points) and the model parameters w (n-dimensional weigth vector) and b (bias).\n",
    "\n",
    "Note that mathematically it's easier to write the data matrix as an n-by-N matrix, with each column being a data point. In python, the data is more commonly represented as as an N-by-n array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "eI9PNMZnhy0d"
   },
   "outputs": [],
   "source": [
    "def model(w,b,X):\n",
    "  # using X as Nxn\n",
    "  print(f'In model, X: {X.shape}, b: {b}, w: {w.shape}')\n",
    "  return (X @ w)+b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv_xi0ajaEEY"
   },
   "source": [
    "Define the function that computes the gradient of the cross-entropy loss given the label y (N-vector), the model prediction y_hat = sigmoid(z) (N-vector), and the dataset X (an n-by-N or N-by-n array). It's probably easier to return the gradients with respect w and b separately, which can be used to update w and b later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "I8KJF8lrZlFi"
   },
   "outputs": [],
   "source": [
    "def gradients(X, y, y_hat):\n",
    "  # Using X as Nxn\n",
    "  # print(f'grad: y shape: {y.shape}, X shape: {X.shape}')\n",
    "  return (np.transpose(X) @ (y_hat - y))/X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgqML9T2cOqd"
   },
   "source": [
    "Write the function that minimizes the loss (i.e. training) by the gradient descent algorithm using a fixed number of iteration (*iter*) and learning rate (*lr*). Your function should take *iter* and *lr* as well as the initial weight w, initial bias b, the input data X and the label y as the inputs. It produces new w and b as output. Also compute the loss value at each iteration and output the sequence of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "6bIdE16li086"
   },
   "outputs": [],
   "source": [
    "def train(w, b, X, y, iter, lr):\n",
    "  print(f'>> {X.shape}')\n",
    "  losslist=list()\n",
    "  for k in range(iter):\n",
    "    z = model(w, b, X)\n",
    "    y_hat = sigmoid(z)\n",
    "    grad = gradients(X, y, y_hat)\n",
    "    print(f'gradient shape: {grad.shape}')\n",
    "    w = w - (lr * grad)\n",
    "    b = np.mean((b*np.ones(y_hat.shape)) - (lr * (y_hat - y)))\n",
    "    myloss = customloss(y, z)\n",
    "    if np.isnan(np.mean(myloss)):\n",
    "      print(f'z: {z}')\n",
    "    losslist.append(np.mean(myloss))\n",
    "    print(f'Iter: {k} Loss: {losslist[-1]}')\n",
    "  return w, b, losslist\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "XbA52seSg7wW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGC-EjrzeHyU"
   },
   "source": [
    "1. Write the function that uses a trained model to produce class prediction (0 or 1) for an input dataset X, i.e. turn the model output z = model(w,b,X) into predicted label y_label (N-vector of 0 or 1). \n",
    "2. For an input dataset X with a known label y (e.g. a training or testing dataset) and a predicted label y_label, compute the accuracy of prediction (i.e. # correct predictions/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "id": "HChwCsuWf07D"
   },
   "outputs": [],
   "source": [
    "def predict(z):\n",
    "  ypred = sigmoid(z)\n",
    "  ypred[ypred<=0.5]=0\n",
    "  ypred[ypred>0.5]=1\n",
    "  ypred = ypred.astype(int)\n",
    "  ypred = np.squeeze(ypred)\n",
    "  # print(f'In pred, {ypred.shape}')\n",
    "  return ypred\n",
    "\n",
    "def accuracy(y, y_label):\n",
    "  diff_bool = (y == y_label)\n",
    "  diff_true = diff_bool[diff_bool==True]\n",
    "  total_sample = len(diff_bool)\n",
    "  return (len(diff_true)/total_sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icfCmavagcp5"
   },
   "source": [
    "We are ready to test your programs on some datasets. First, we use a synthetic dataset generated using [scikit-learn](https://scikit-learn.org/stable/datasets.html) package. We generate a dataset for training and simultaneously a dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "EXJOlxH2nYw3",
    "outputId": "4684e359-67e9-4d8e-dff9-089d83beef75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba09650a00>]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X_train, y_train = make_moons(n_samples=500, noise=0.1)\n",
    "X_test, y_test = make_moons(n_samples=1000, noise=0.1)\n",
    "\n",
    "print(X_train.shape)\n",
    "plt.figure()\n",
    "plt.plot(X_train[:, 0][y_train==0], X_train[:, 1][y_train==0], \"g^\")\n",
    "plt.plot(X_train[:, 0][y_train==1], X_train[:, 1][y_train==1], \"bs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2_uUM_Zufoz"
   },
   "source": [
    "Here is another toy test example you may try but not part of homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "EJbVzPfLdOvC",
    "outputId": "b1378a1b-945b-447e-f095-d62e2b24dcd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba0969fc70>]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X_train, y_train = make_classification(n_samples=1000, n_features=4)\n",
    "X_test=X_train[500:,]\n",
    "y_test=y_train[500:,]\n",
    "X_train=X_train[:500,]\n",
    "y_train=y_train[:500,]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_train[:, 0][y_train==0], X_train[:, 1][y_train==0], \"g^\")\n",
    "plt.plot(X_train[:, 0][y_train==1], X_train[:, 1][y_train==1], \"bs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMm__k_AjOFp"
   },
   "source": [
    "We now train the model using (X_train, y_train). We initialize weight as a random vector, and b=0. We plot the loss convergence history. You should get the loss down to about 0.2.\n",
    "We compute the prediction accuracy on (X_train, y_train). You should get an accuracy in the 80s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mi868jT_mpnq",
    "outputId": "e39f8c4e-570a-4eb2-9b24-ce5233df7e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(500, 4)\n",
      "(500, 1)\n",
      ">> (500, 4)\n",
      "In model, X: (500, 4), b: 0, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 0 Loss: -0.02388199542979992\n",
      "In model, X: (500, 4), b: 0.0007656258867319356, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 1 Loss: -0.032651588457274824\n",
      "In model, X: (500, 4), b: 0.0014410285315288383, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 2 Loss: -0.04084368644873047\n",
      "In model, X: (500, 4), b: 0.0020307564577688774, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 3 Loss: -0.048504263658366624\n",
      "In model, X: (500, 4), b: 0.0025392513274758743, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 4 Loss: -0.05567535183812396\n",
      "In model, X: (500, 4), b: 0.0029708218862768004, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 5 Loss: -0.06239537118839616\n",
      "In model, X: (500, 4), b: 0.0033296261691685247, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 6 Loss: -0.0686994378701094\n",
      "In model, X: (500, 4), b: 0.0036196600508930014, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 7 Loss: -0.07461964867897754\n",
      "In model, X: (500, 4), b: 0.0038447506420271602, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 8 Loss: -0.08018534379609692\n",
      "In model, X: (500, 4), b: 0.0040085533651791185, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 9 Loss: -0.08542334869355794\n",
      "In model, X: (500, 4), b: 0.004114551807471989, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 10 Loss: -0.09035819635596419\n",
      "In model, X: (500, 4), b: 0.00416605964903527, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 11 Loss: -0.0950123310142491\n",
      "In model, X: (500, 4), b: 0.004166224124642161, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 12 Loss: -0.09940629459527489\n",
      "In model, X: (500, 4), b: 0.0041180305972029615, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 13 Loss: -0.10355889707873145\n",
      "In model, X: (500, 4), b: 0.004024307915840298, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 14 Loss: -0.10748737192686492\n",
      "In model, X: (500, 4), b: 0.003887734304214278, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 15 Loss: -0.11120751771571222\n",
      "In model, X: (500, 4), b: 0.0037108435816045953, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 16 Loss: -0.11473382705118902\n",
      "In model, X: (500, 4), b: 0.003496031563756842, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 17 Loss: -0.11807960380164491\n",
      "In model, X: (500, 4), b: 0.0032455625254992518, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 18 Loss: -0.12125706962224966\n",
      "In model, X: (500, 4), b: 0.002961575634770747, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 19 Loss: -0.12427746068751068\n",
      "In model, X: (500, 4), b: 0.0026460912895873365, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 20 Loss: -0.12715111548783992\n",
      "In model, X: (500, 4), b: 0.002301017306840649, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 21 Loss: -0.1298875544856573\n",
      "In model, X: (500, 4), b: 0.0019281549256123986, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 22 Loss: -0.13249555236706165\n",
      "In model, X: (500, 4), b: 0.0015292045986315193, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 23 Loss: -0.1349832035674242\n",
      "In model, X: (500, 4), b: 0.0011057715541664588, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 24 Loss: -0.13735798169396332\n",
      "In model, X: (500, 4), b: 0.0006593711174829433, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 25 Loss: -0.13962679341585324\n",
      "In model, X: (500, 4), b: 0.0001914337863640867, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 26 Loss: -0.1417960273429793\n",
      "In model, X: (500, 4), b: -0.00029668994062945875, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 27 Loss: -0.1438715983682162\n",
      "In model, X: (500, 4), b: -0.0008037249812571607, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 28 Loss: -0.14585898790512397\n",
      "In model, X: (500, 4), b: -0.001328467327698196, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 29 Loss: -0.14776328041320477\n",
      "In model, X: (500, 4), b: -0.0018697799394638349, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 30 Loss: -0.1495891965662496\n",
      "In model, X: (500, 4), b: -0.00242658886359842, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 31 Loss: -0.1513411233857177\n",
      "In model, X: (500, 4), b: -0.002997879573708433, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 32 Loss: -0.15302314163037187\n",
      "In model, X: (500, 4), b: -0.0035826935187405744, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 33 Loss: -0.154639050705376\n",
      "In model, X: (500, 4), b: -0.004180124872062971, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 34 Loss: -0.1561923913285746\n",
      "In model, X: (500, 4), b: -0.004789317471234801, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 35 Loss: -0.15768646616852658\n",
      "In model, X: (500, 4), b: -0.0054094619388352335, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 36 Loss: -0.1591243586478948\n",
      "In model, X: (500, 4), b: -0.006039792974826969, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 37 Loss: -0.16050895008680596\n",
      "In model, X: (500, 4), b: -0.006679586811124371, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 38 Loss: -0.16184293534364144\n",
      "In model, X: (500, 4), b: -0.0073281588192978045, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 39 Loss: -0.16312883709523057\n",
      "In model, X: (500, 4), b: -0.007984861262655794, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 40 Loss: -0.16436901888444616\n",
      "In model, X: (500, 4), b: -0.008649081184290051, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 41 Loss: -0.1655656970506117\n",
      "In model, X: (500, 4), b: -0.009320238423033386, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 42 Loss: -0.16672095164678716\n",
      "In model, X: (500, 4), b: -0.009997783749657502, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 43 Loss: -0.16783673643778985\n",
      "In model, X: (500, 4), b: -0.010681197116019487, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 44 Loss: -0.1689148880636184\n",
      "In model, X: (500, 4), b: -0.011369986010246464, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 45 Loss: -0.16995713444467914\n",
      "In model, X: (500, 4), b: -0.01206368391142316, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 46 Loss: -0.17096510249777874\n",
      "In model, X: (500, 4), b: -0.012761848837613643, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 47 Loss: -0.1719403252251571\n",
      "In model, X: (500, 4), b: -0.013464061981403732, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 48 Loss: -0.17288424823281826\n",
      "In model, X: (500, 4), b: -0.014169926427492959, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 49 Loss: -0.17379823572900524\n",
      "In model, X: (500, 4), b: -0.014879065947193123, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 50 Loss: -0.17468357604879647\n",
      "In model, X: (500, 4), b: -0.015591123865003845, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 51 Loss: -0.1755414867464214\n",
      "In model, X: (500, 4), b: -0.016305761992733557, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 52 Loss: -0.17637311929294977\n",
      "In model, X: (500, 4), b: -0.017022659626917215, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 53 Loss: -0.17717956341346108\n",
      "In model, X: (500, 4), b: -0.01774151260554937, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 54 Loss: -0.17796185109460408\n",
      "In model, X: (500, 4), b: -0.01846203242040397, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 55 Loss: -0.1787209602905777\n",
      "In model, X: (500, 4), b: -0.019183945381450067, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 56 Loss: -0.1794578183529705\n",
      "In model, X: (500, 4), b: -0.019906991830096633, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 57 Loss: -0.18017330520755429\n",
      "In model, X: (500, 4), b: -0.020630925398209998, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 58 Loss: -0.1808682562990208\n",
      "In model, X: (500, 4), b: -0.0213555123100448, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 59 Loss: -0.18154346532274265\n",
      "In model, X: (500, 4), b: -0.02208053072441455, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 60 Loss: -0.18219968676092171\n",
      "In model, X: (500, 4), b: -0.022805770114601158, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 61 Loss: -0.18283763823893406\n",
      "In model, X: (500, 4), b: -0.02353103068366504, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 62 Loss: -0.18345800271627702\n",
      "In model, X: (500, 4), b: -0.024256122812969367, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 63 Loss: -0.18406143052525129\n",
      "In model, X: (500, 4), b: -0.024980866541873818, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 64 Loss: -0.1846485412693645\n",
      "In model, X: (500, 4), b: -0.025705091076685824, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 65 Loss: -0.1852199255923985\n",
      "In model, X: (500, 4), b: -0.02642863432708139, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 66 Loss: -0.1857761468281412\n",
      "In model, X: (500, 4), b: -0.027151342468323052, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 67 Loss: -0.18631774253992772\n",
      "In model, X: (500, 4), b: -0.027873069527710848, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 68 Loss: -0.1868452259583582\n",
      "In model, X: (500, 4), b: -0.02859367699380283, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 69 Loss: -0.18735908732485754\n",
      "In model, X: (500, 4), b: -0.029313033447036018, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 70 Loss: -0.18785979514809864\n",
      "In model, X: (500, 4), b: -0.030031014210466486, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 71 Loss: -0.18834779737973167\n",
      "In model, X: (500, 4), b: -0.03074750101942926, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 72 Loss: -0.18882352251533002\n",
      "In model, X: (500, 4), b: -0.031462381708995395, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 73 Loss: -0.18928738062598277\n",
      "In model, X: (500, 4), b: -0.032175549918174964, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 74 Loss: -0.18973976432552375\n",
      "In model, X: (500, 4), b: -0.03288690480988143, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 75 Loss: -0.19018104967798682\n",
      "In model, X: (500, 4), b: -0.03359635080573506, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 76 Loss: -0.19061159704951108\n",
      "In model, X: (500, 4), b: -0.03430379733484123, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 77 Loss: -0.19103175190858865\n",
      "In model, X: (500, 4), b: -0.03500915859573375, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 78 Loss: -0.1914418455782389\n",
      "In model, X: (500, 4), b: -0.03571235333072386, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 79 Loss: -0.1918421959434188\n",
      "In model, X: (500, 4), b: -0.03641330461194295, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 80 Loss: -0.19223310811672087\n",
      "In model, X: (500, 4), b: -0.03711193963841134, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 81 Loss: -0.19261487506517966\n",
      "In model, X: (500, 4), b: -0.03780818954350653, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 82 Loss: -0.19298777820079172\n",
      "In model, X: (500, 4), b: -0.03850198921224299, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 83 Loss: -0.19335208793716022\n",
      "In model, X: (500, 4), b: -0.03919327710781174, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 84 Loss: -0.1937080642144938\n",
      "In model, X: (500, 4), b: -0.03988199510686143, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 85 Loss: -0.19405595699502634\n",
      "In model, X: (500, 4), b: -0.04056808834303426, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 86 Loss: -0.19439600673077118\n",
      "In model, X: (500, 4), b: -0.04125150505829954, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 87 Loss: -0.19472844480538518\n",
      "In model, X: (500, 4), b: -0.041932196461655184, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 88 Loss: -0.19505349395178917\n",
      "In model, X: (500, 4), b: -0.042610116594793294, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 89 Loss: -0.1953713686470743\n",
      "In model, X: (500, 4), b: -0.04328522220434998, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 90 Loss: -0.19568227548611508\n",
      "In model, X: (500, 4), b: -0.04395747262038225, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 91 Loss: -0.19598641353520868\n",
      "In model, X: (500, 4), b: -0.04462682964073597, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 92 Loss: -0.1962839746669704\n",
      "In model, X: (500, 4), b: -0.045293257420988525, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 93 Loss: -0.196575143877627\n",
      "In model, X: (500, 4), b: -0.045956722369668516, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 94 Loss: -0.19686009958777367\n",
      "In model, X: (500, 4), b: -0.0466171930484722, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 95 Loss: -0.1971390139275867\n",
      "In model, X: (500, 4), b: -0.04727464007721251, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 96 Loss: -0.19741205300741577\n",
      "In model, X: (500, 4), b: -0.04792903604325193, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 97 Loss: -0.19767937717462028\n",
      "In model, X: (500, 4), b: -0.048580355415184795, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 98 Loss: -0.1979411412574539\n",
      "In model, X: (500, 4), b: -0.04922857446054784, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 99 Loss: -0.1981974947967506\n",
      "In model, X: (500, 4), b: -0.049873671167350704, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 100 Loss: -0.1984485822661144\n",
      "In model, X: (500, 4), b: -0.050515625169229694, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 101 Loss: -0.19869454328127012\n",
      "In model, X: (500, 4), b: -0.05115441767403925, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 102 Loss: -0.19893551279919058\n",
      "In model, X: (500, 4), b: -0.05179003139570614, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 103 Loss: -0.19917162130757435\n",
      "In model, X: (500, 4), b: -0.05242245048918099, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 104 Loss: -0.19940299500521477\n",
      "In model, X: (500, 4), b: -0.05305166048833101, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 105 Loss: -0.19962975597376295\n",
      "In model, X: (500, 4), b: -0.053677648246626616, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 106 Loss: -0.19985202234135965\n",
      "In model, X: (500, 4), b: -0.05430040188048247, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 107 Loss: -0.20006990843857925\n",
      "In model, X: (500, 4), b: -0.05491991071512128, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 108 Loss: -0.20028352494710164\n",
      "In model, X: (500, 4), b: -0.0555361652328359, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 109 Loss: -0.20049297904150434\n",
      "In model, X: (500, 4), b: -0.05614915702353203, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 110 Loss: -0.20069837452454084\n",
      "In model, X: (500, 4), b: -0.056758878737439976, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 111 Loss: -0.2008998119562503\n",
      "In model, X: (500, 4), b: -0.05736532403989031, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 112 Loss: -0.2010973887772242\n",
      "In model, X: (500, 4), b: -0.057968487568053506, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 113 Loss: -0.20129119942633297\n",
      "In model, X: (500, 4), b: -0.05856836488954922, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 114 Loss: -0.2014813354532016\n",
      "In model, X: (500, 4), b: -0.05916495246283581, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 115 Loss: -0.20166788562570326\n",
      "In model, X: (500, 4), b: -0.05975824759929533, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 116 Loss: -0.20185093603272572\n",
      "In model, X: (500, 4), b: -0.06034824842693375, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 117 Loss: -0.2020305701824509\n",
      "In model, X: (500, 4), b: -0.06093495385562043, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 118 Loss: -0.20220686909637242\n",
      "In model, X: (500, 4), b: -0.06151836354379458, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 119 Loss: -0.20237991139926578\n",
      "In model, X: (500, 4), b: -0.06209847786657054, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 120 Loss: -0.20254977340531083\n",
      "In model, X: (500, 4), b: -0.06267529788517677, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 121 Loss: -0.2027165292005573\n",
      "In model, X: (500, 4), b: -0.0632488253176673, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 122 Loss: -0.20288025072191226\n",
      "In model, X: (500, 4), b: -0.06381906251084699, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 123 Loss: -0.20304100783281856\n",
      "In model, X: (500, 4), b: -0.06438601241335533, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 124 Loss: -0.20319886839578447\n",
      "In model, X: (500, 4), b: -0.0649496785498561, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 125 Loss: -0.20335389834191533\n",
      "In model, X: (500, 4), b: -0.06551006499628292, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 126 Loss: -0.20350616173759112\n",
      "In model, X: (500, 4), b: -0.06606717635609315, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 127 Loss: -0.20365572084842343\n",
      "In model, X: (500, 4), b: -0.06662101773748501, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 128 Loss: -0.203802636200622\n",
      "In model, X: (500, 4), b: -0.06717159473153526, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 129 Loss: -0.2039469666398901\n",
      "In model, X: (500, 4), b: -0.06771891339121629, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 130 Loss: -0.20408876938796466\n",
      "In model, X: (500, 4), b: -0.06826298021125407, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 131 Loss: -0.20422810009690948\n",
      "In model, X: (500, 4), b: -0.06880380210879024, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 132 Loss: -0.2043650129012645\n",
      "In model, X: (500, 4), b: -0.06934138640481276, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 133 Loss: -0.20449956046814896\n",
      "In model, X: (500, 4), b: -0.06987574080632225, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 134 Loss: -0.20463179404541132\n",
      "In model, X: (500, 4), b: -0.070406873389202, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 135 Loss: -0.2047617635079128\n",
      "In model, X: (500, 4), b: -0.07093479258176129, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 136 Loss: -0.20488951740203004\n",
      "In model, X: (500, 4), b: -0.07145950714892345, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 137 Loss: -0.20501510298845319\n",
      "In model, X: (500, 4), b: -0.07198102617703092, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 138 Loss: -0.20513856628335753\n",
      "In model, X: (500, 4), b: -0.07249935905924147, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 139 Loss: -0.20525995209801823\n",
      "In model, X: (500, 4), b: -0.07301451548149038, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 140 Loss: -0.20537930407693653\n",
      "In model, X: (500, 4), b: -0.07352650540899493, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 141 Loss: -0.20549666473454248\n",
      "In model, X: (500, 4), b: -0.07403533907327856, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 142 Loss: -0.20561207549053456\n",
      "In model, X: (500, 4), b: -0.07454102695969311, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 143 Loss: -0.20572557670391575\n",
      "In model, X: (500, 4), b: -0.07504357979541844, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 144 Loss: -0.2058372077057807\n",
      "In model, X: (500, 4), b: -0.07554300853791979, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 145 Loss: -0.20594700683090733\n",
      "In model, X: (500, 4), b: -0.07603932436384422, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 146 Loss: -0.20605501144820265\n",
      "In model, X: (500, 4), b: -0.07653253865833813, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 147 Loss: -0.20616125799005158\n",
      "In model, X: (500, 4), b: -0.07702266300476862, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 148 Loss: -0.2062657819806138\n",
      "In model, X: (500, 4), b: -0.07750970917483271, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 149 Loss: -0.20636861806311202\n",
      "In model, X: (500, 4), b: -0.07799368911903844, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 150 Loss: -0.20646980002615373\n",
      "In model, X: (500, 4), b: -0.07847461495754322, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 151 Loss: -0.20656936082912572\n",
      "In model, X: (500, 4), b: -0.07895249897133502, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 152 Loss: -0.20666733262669837\n",
      "In model, X: (500, 4), b: -0.07942735359374285, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 153 Loss: -0.20676374679247708\n",
      "In model, X: (500, 4), b: -0.07989919140226355, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 154 Loss: -0.20685863394183412\n",
      "In model, X: (500, 4), b: -0.08036802511069235, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 155 Loss: -0.20695202395395332\n",
      "In model, X: (500, 4), b: -0.08083386756154538, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 156 Loss: -0.2070439459931205\n",
      "In model, X: (500, 4), b: -0.08129673171876274, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 157 Loss: -0.2071344285292876\n",
      "In model, X: (500, 4), b: -0.08175663066068117, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 158 Loss: -0.20722349935794046\n",
      "In model, X: (500, 4), b: -0.0822135775732659, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 159 Loss: -0.20731118561929662\n",
      "In model, X: (500, 4), b: -0.08266758574359187, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 160 Loss: -0.20739751381685967\n",
      "In model, X: (500, 4), b: -0.08311866855356446, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 161 Loss: -0.2074825098353548\n",
      "In model, X: (500, 4), b: -0.08356683947387086, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 162 Loss: -0.20756619895806971\n",
      "In model, X: (500, 4), b: -0.08401211205815318, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 163 Loss: -0.2076486058836229\n",
      "In model, X: (500, 4), b: -0.0844544999373949, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 164 Loss: -0.2077297547421822\n",
      "In model, X: (500, 4), b: -0.08489401681451267, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 165 Loss: -0.2078096691111532\n",
      "In model, X: (500, 4), b: -0.0853306764591457, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 166 Loss: -0.2078883720303588\n",
      "In model, X: (500, 4), b: -0.08576449270263543, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 167 Loss: -0.20796588601672758\n",
      "In model, X: (500, 4), b: -0.08619547943318827, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 168 Loss: -0.2080422330785107\n",
      "In model, X: (500, 4), b: -0.08662365059121464, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 169 Loss: -0.20811743472904334\n",
      "In model, X: (500, 4), b: -0.08704902016483808, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 170 Loss: -0.20819151200006908\n",
      "In model, X: (500, 4), b: -0.0874716021855676, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 171 Loss: -0.208264485454642\n",
      "In model, X: (500, 4), b: -0.0878914107241279, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 172 Loss: -0.20833637519962211\n",
      "In model, X: (500, 4), b: -0.08830845988644119, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 173 Loss: -0.20840720089778023\n",
      "In model, X: (500, 4), b: -0.08872276380975547, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 174 Loss: -0.20847698177952376\n",
      "In model, X: (500, 4), b: -0.08913433665891374, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 175 Loss: -0.20854573665426004\n",
      "In model, X: (500, 4), b: -0.08954319262275913, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 176 Loss: -0.20861348392140805\n",
      "In model, X: (500, 4), b: -0.08994934591067105, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 177 Loss: -0.20868024158107198\n",
      "In model, X: (500, 4), b: -0.09035281074922756, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 178 Loss: -0.20874602724438823\n",
      "In model, X: (500, 4), b: -0.09075360137898972, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 179 Loss: -0.20881085814355776\n",
      "In model, X: (500, 4), b: -0.09115173205140326, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 180 Loss: -0.2088747511415744\n",
      "In model, X: (500, 4), b: -0.09154721702581367, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 181 Loss: -0.20893772274166028\n",
      "In model, X: (500, 4), b: -0.09194007056659047, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 182 Loss: -0.20899978909641798\n",
      "In model, X: (500, 4), b: -0.09233030694035697, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 183 Loss: -0.20906096601670912\n",
      "In model, X: (500, 4), b: -0.0927179404133219, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 184 Loss: -0.20912126898027014\n",
      "In model, X: (500, 4), b: -0.09310298524870897, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 185 Loss: -0.20918071314007222\n",
      "In model, X: (500, 4), b: -0.09348545570428143, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 186 Loss: -0.20923931333243564\n",
      "In model, X: (500, 4), b: -0.09386536602995799, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 187 Loss: -0.20929708408490638\n",
      "In model, X: (500, 4), b: -0.09424273046551697, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 188 Loss: -0.20935403962390278\n",
      "In model, X: (500, 4), b: -0.09461756323838585, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 189 Loss: -0.20941019388214047\n",
      "In model, X: (500, 4), b: -0.09498987856151306, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 190 Loss: -0.2094655605058426\n",
      "In model, X: (500, 4), b: -0.09535969063131916, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 191 Loss: -0.20952015286174264\n",
      "In model, X: (500, 4), b: -0.09572701362572492, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 192 Loss: -0.20957398404388664\n",
      "In model, X: (500, 4), b: -0.0960918617022535, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 193 Loss: -0.20962706688024157\n",
      "In model, X: (500, 4), b: -0.09645424899620425, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 194 Loss: -0.20967941393911635\n",
      "In model, X: (500, 4), b: -0.09681418961889564, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 195 Loss: -0.209731037535401\n",
      "In model, X: (500, 4), b: -0.09717169765597512, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 196 Loss: -0.2097819497366307\n",
      "In model, X: (500, 4), b: -0.09752678716579355, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 197 Loss: -0.2098321623688796\n",
      "In model, X: (500, 4), b: -0.09787947217784199, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 198 Loss: -0.209881687022491\n",
      "In model, X: (500, 4), b: -0.09822976669124892, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 199 Loss: -0.2099305350576474\n",
      "In model, X: (500, 4), b: -0.09857768467333565, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 200 Loss: -0.20997871760978773\n",
      "In model, X: (500, 4), b: -0.09892324005822818, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 201 Loss: -0.2100262455948747\n",
      "In model, X: (500, 4), b: -0.09926644674552348, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 202 Loss: -0.21007312971451814\n",
      "In model, X: (500, 4), b: -0.09960731859900841, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 203 Loss: -0.2101193804609582\n",
      "In model, X: (500, 4), b: -0.0999458694454296, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 204 Loss: -0.21016500812191385\n",
      "In model, X: (500, 4), b: -0.10028211307331267, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 205 Loss: -0.21021002278529938\n",
      "In model, X: (500, 4), b: -0.1006160632318288, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 206 Loss: -0.21025443434381444\n",
      "In model, X: (500, 4), b: -0.10094773362970762, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 207 Loss: -0.21029825249941053\n",
      "In model, X: (500, 4), b: -0.1012771379341944, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 208 Loss: -0.21034148676763845\n",
      "In model, X: (500, 4), b: -0.10160428977005051, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 209 Loss: -0.21038414648188006\n",
      "In model, X: (500, 4), b: -0.10192920271859535, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 210 Loss: -0.21042624079746747\n",
      "In model, X: (500, 4), b: -0.10225189031678865, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 211 Loss: -0.21046777869569427\n",
      "In model, X: (500, 4), b: -0.10257236605635184, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 212 Loss: -0.2105087689877208\n",
      "In model, X: (500, 4), b: -0.10289064338292697, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 213 Loss: -0.21054922031837725\n",
      "In model, X: (500, 4), b: -0.10320673569527225, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 214 Loss: -0.21058914116986782\n",
      "In model, X: (500, 4), b: -0.10352065634449278, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 215 Loss: -0.21062853986537866\n",
      "In model, X: (500, 4), b: -0.10383241863330553, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 216 Loss: -0.2106674245725926\n",
      "In model, X: (500, 4), b: -0.10414203581533726, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 217 Loss: -0.2107058033071127\n",
      "In model, X: (500, 4), b: -0.10444952109445463, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 218 Loss: -0.2107436839357992\n",
      "In model, X: (500, 4), b: -0.10475488762412496, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 219 Loss: -0.2107810741800201\n",
      "In model, X: (500, 4), b: -0.10505814850680724, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 220 Loss: -0.21081798161881937\n",
      "In model, X: (500, 4), b: -0.10535931679337188, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 221 Loss: -0.2108544136920052\n",
      "In model, X: (500, 4), b: -0.10565840548254869, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 222 Loss: -0.21089037770315947\n",
      "In model, X: (500, 4), b: -0.10595542752040202, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 223 Loss: -0.21092588082257258\n",
      "In model, X: (500, 4), b: -0.10625039579983198, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 224 Loss: -0.21096093009010347\n",
      "In model, X: (500, 4), b: -0.10654332316010129, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 225 Loss: -0.21099553241796945\n",
      "In model, X: (500, 4), b: -0.10683422238638682, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 226 Loss: -0.21102969459346588\n",
      "In model, X: (500, 4), b: -0.10712310620935463, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 227 Loss: -0.21106342328161862\n",
      "In model, X: (500, 4), b: -0.1074099873047585, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 228 Loss: -0.21109672502777121\n",
      "In model, X: (500, 4), b: -0.1076948782930605, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 229 Loss: -0.2111296062601084\n",
      "In model, X: (500, 4), b: -0.1079777917390733, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 230 Loss: -0.2111620732921177\n",
      "In model, X: (500, 4), b: -0.10825874015162343, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 231 Loss: -0.21119413232499074\n",
      "In model, X: (500, 4), b: -0.10853773598323467, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 232 Loss: -0.21122578944996684\n",
      "In model, X: (500, 4), b: -0.10881479162983126, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 233 Loss: -0.21125705065061914\n",
      "In model, X: (500, 4), b: -0.10908991943045991, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 234 Loss: -0.2112879218050859\n",
      "In model, X: (500, 4), b: -0.10936313166703031, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 235 Loss: -0.21131840868824817\n",
      "In model, X: (500, 4), b: -0.10963444056407332, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 236 Loss: -0.21134851697385476\n",
      "In model, X: (500, 4), b: -0.10990385828851652, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 237 Loss: -0.21137825223659762\n",
      "In model, X: (500, 4), b: -0.1101713969494764, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 238 Loss: -0.21140761995413646\n",
      "In model, X: (500, 4), b: -0.11043706859806657, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 239 Loss: -0.2114366255090763\n",
      "In model, X: (500, 4), b: -0.11070088522722173, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 240 Loss: -0.21146527419089825\n",
      "In model, X: (500, 4), b: -0.11096285877153672, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 241 Loss: -0.21149357119784423\n",
      "In model, X: (500, 4), b: -0.11122300110712022, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 242 Loss: -0.21152152163875837\n",
      "In model, X: (500, 4), b: -0.11148132405146254, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 243 Loss: -0.2115491305348846\n",
      "In model, X: (500, 4), b: -0.11173783936331731, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 244 Loss: -0.21157640282162288\n",
      "In model, X: (500, 4), b: -0.11199255874259623, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 245 Loss: -0.21160334335024442\n",
      "In model, X: (500, 4), b: -0.11224549383027678, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 246 Loss: -0.21162995688956743\n",
      "In model, X: (500, 4), b: -0.11249665620832236, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 247 Loss: -0.21165624812759395\n",
      "In model, X: (500, 4), b: -0.11274605739961448, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 248 Loss: -0.2116822216731094\n",
      "In model, X: (500, 4), b: -0.11299370886789648, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 249 Loss: -0.21170788205724547\n",
      "In model, X: (500, 4), b: -0.11323962201772865, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 250 Loss: -0.211733233735007\n",
      "In model, X: (500, 4), b: -0.11348380819445421, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 251 Loss: -0.2117582810867647\n",
      "In model, X: (500, 4), b: -0.11372627868417576, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 252 Loss: -0.21178302841971358\n",
      "In model, X: (500, 4), b: -0.11396704471374201, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 253 Loss: -0.21180747996929886\n",
      "In model, X: (500, 4), b: -0.11420611745074441, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 254 Loss: -0.21183163990060935\n",
      "In model, X: (500, 4), b: -0.11444350800352321, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 255 Loss: -0.21185551230973992\n",
      "In model, X: (500, 4), b: -0.11467922742118285, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 256 Loss: -0.21187910122512327\n",
      "In model, X: (500, 4), b: -0.11491328669361621, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 257 Loss: -0.21190241060883228\n",
      "In model, X: (500, 4), b: -0.11514569675153763, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 258 Loss: -0.2119254443578532\n",
      "In model, X: (500, 4), b: -0.11537646846652404, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 259 Loss: -0.2119482063053308\n",
      "In model, X: (500, 4), b: -0.11560561265106435, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 260 Loss: -0.2119707002217858\n",
      "In model, X: (500, 4), b: -0.11583314005861664, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 261 Loss: -0.211992929816306\n",
      "In model, X: (500, 4), b: -0.11605906138367278, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 262 Loss: -0.2120148987377108\n",
      "In model, X: (500, 4), b: -0.11628338726183038, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 263 Loss: -0.21203661057569045\n",
      "In model, X: (500, 4), b: -0.11650612826987185, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 264 Loss: -0.21205806886192066\n",
      "In model, X: (500, 4), b: -0.11672729492585017, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 265 Loss: -0.21207927707115284\n",
      "In model, X: (500, 4), b: -0.11694689768918128, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 266 Loss: -0.21210023862228053\n",
      "In model, X: (500, 4), b: -0.11716494696074278, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 267 Loss: -0.2121209568793831\n",
      "In model, X: (500, 4), b: -0.11738145308297886, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 268 Loss: -0.21214143515274722\n",
      "In model, X: (500, 4), b: -0.11759642634001094, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 269 Loss: -0.2121616766998658\n",
      "In model, X: (500, 4), b: -0.1178098769577543, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 270 Loss: -0.21218168472641605\n",
      "In model, X: (500, 4), b: -0.11802181510403995, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 271 Loss: -0.21220146238721646\n",
      "In model, X: (500, 4), b: -0.11823225088874205, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 272 Loss: -0.2122210127871637\n",
      "In model, X: (500, 4), b: -0.11844119436391026, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 273 Loss: -0.21224033898214947\n",
      "In model, X: (500, 4), b: -0.1186486555239072, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 274 Loss: -0.21225944397995777\n",
      "In model, X: (500, 4), b: -0.11885464430555058, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 275 Loss: -0.21227833074114402\n",
      "In model, X: (500, 4), b: -0.11905917058825999, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 276 Loss: -0.21229700217989472\n",
      "In model, X: (500, 4), b: -0.11926224419420811, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 277 Loss: -0.21231546116487018\n",
      "In model, X: (500, 4), b: -0.11946387488847612, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 278 Loss: -0.21233371052002864\n",
      "In model, X: (500, 4), b: -0.11966407237921337, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 279 Loss: -0.21235175302543405\n",
      "In model, X: (500, 4), b: -0.11986284631780084, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 280 Loss: -0.2123695914180467\n",
      "In model, X: (500, 4), b: -0.12006020629901859, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 281 Loss: -0.2123872283924975\n",
      "In model, X: (500, 4), b: -0.12025616186121671, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 282 Loss: -0.21240466660184654\n",
      "In model, X: (500, 4), b: -0.12045072248648993, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 283 Loss: -0.21242190865832575\n",
      "In model, X: (500, 4), b: -0.12064389760085552, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 284 Loss: -0.2124389571340665\n",
      "In model, X: (500, 4), b: -0.12083569657443459, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 285 Loss: -0.2124558145618123\n",
      "In model, X: (500, 4), b: -0.12102612872163633, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 286 Loss: -0.21247248343561714\n",
      "In model, X: (500, 4), b: -0.12121520330134546, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 287 Loss: -0.21248896621152916\n",
      "In model, X: (500, 4), b: -0.12140292951711236, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 288 Loss: -0.21250526530826105\n",
      "In model, X: (500, 4), b: -0.12158931651734624, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 289 Loss: -0.2125213831078465\n",
      "In model, X: (500, 4), b: -0.12177437339551068, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 290 Loss: -0.21253732195628364\n",
      "In model, X: (500, 4), b: -0.12195810919032192, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 291 Loss: -0.2125530841641654\n",
      "In model, X: (500, 4), b: -0.12214053288594942, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 292 Loss: -0.2125686720072972\n",
      "In model, X: (500, 4), b: -0.12232165341221904, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 293 Loss: -0.2125840877273027\n",
      "In model, X: (500, 4), b: -0.12250147964481803, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 294 Loss: -0.21259933353221683\n",
      "In model, X: (500, 4), b: -0.12268002040550258, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 295 Loss: -0.21261441159706768\n",
      "In model, X: (500, 4), b: -0.12285728446230709, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 296 Loss: -0.21262932406444632\n",
      "In model, X: (500, 4), b: -0.12303328052975561, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 297 Loss: -0.2126440730450659\n",
      "In model, X: (500, 4), b: -0.12320801726907496, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 298 Loss: -0.2126586606183091\n",
      "In model, X: (500, 4), b: -0.12338150328840972, w: (4, 1)\n",
      "gradient shape: (4, 1)\n",
      "Iter: 299 Loss: -0.21267308883276537\n",
      "In model, X: (500, 4), b: -0.12355374714303886, w: (4, 1)\n",
      "0.834\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.rand(X_train.shape[1],1)  # assuming X is N-by-n. \n",
    "                                        # if X is n-by-N, use X_train.shape[0]\n",
    "y_train = y_train.reshape((-1,1))\n",
    "y_test = y_test.reshape((-1,1))\n",
    "print(w.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "b = 0\n",
    "w, b, loss = train(w, b, X_train, y_train, iter=300, lr=0.1)\n",
    "plt.figure()\n",
    "plt.plot(loss)\n",
    "\n",
    "#training accuracy \n",
    "z = model(w,b,X_train)\n",
    "print(accuracy(np.squeeze(y_train), predict(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBm8ESACmrxe"
   },
   "source": [
    "To see how well our model performs, we compute its accuracy on the testing dataset (X_test, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pt9_Aiw-zqP6",
    "outputId": "ec820cd4-9be1-4685-840f-126eb84996ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In model, X: (500, 4), b: -0.12355374714303886, w: (4, 1)\n",
      "[0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1\n",
      " 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1\n",
      " 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0\n",
      " 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
      " 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
      " 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
      " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0\n",
      " 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0\n",
      " 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1\n",
      " 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0\n",
      " 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 0\n",
      " 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1\n",
      " 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1]\n",
      "[0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1\n",
      " 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
      " 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0\n",
      " 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1\n",
      " 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1\n",
      " 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
      " 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0\n",
      " 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0\n",
      " 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0\n",
      " 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1\n",
      " 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0\n",
      " 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
      " 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1\n",
      " 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1]\n",
      "0.838\n"
     ]
    }
   ],
   "source": [
    "z = model(w,b,X_test)\n",
    "y_test=np.squeeze(y_test)\n",
    "print(y_test)\n",
    "print(predict(z))\n",
    "print(accuracy(y_test, predict(z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "id": "u-YnkECyDJXw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef5x5LENm7_s"
   },
   "source": [
    "Now, we look at a real-world dataset. [The Bank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#) is available at UCI's Machine Learning Repository. Colab can read this dataset directly from [GitHub](https://github.com/madmashup/targeted-marketing-predictive-engine) using pandas package: pd.read_csv. The data is in the DataFrame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5vKPwXfYgLV",
    "outputId": "b79b038e-2f45-4077-8b4d-f631b99cd3f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41188, 21)\n",
      "['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/madmashup/targeted-marketing-predictive-engine/master/banking.csv'\n",
    "data = pd.read_csv(url)\n",
    "print(data.shape)\n",
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG9UWfJ8n2Jr"
   },
   "source": [
    "This dataset is pretty large and cause my machine to crash. I remove some fileds. [This Webpage](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8) has a good description of this dataset. Note that you are not allowed to use any existing model such as those used in that Webpage for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGiNyyIsvUw-",
    "outputId": "75ded801-042c-4312-a127-16baf1b6777f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'job', 'marital', 'housing', 'loan', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']\n",
      "(41188, 16)\n"
     ]
    }
   ],
   "source": [
    "cat_vars=['default','education','contact','month','day_of_week',]\n",
    "data=data.drop(cat_vars, axis=1)\n",
    "print(list(data.columns))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVSJRTDeoHNj"
   },
   "source": [
    "Some data columns have k class labels. This is best represented as k columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "du0e-Dhyg2FV",
    "outputId": "79e1cdc7-0137-4332-dace-6a2bdb868858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job\n",
      "marital\n",
      "housing\n",
      "loan\n",
      "poutcome\n",
      "(41188, 36)\n",
      "['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y', 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown', 'divorced', 'married', 'single', 'unknown', 'no', 'unknown', 'yes', 'no', 'unknown', 'yes', 'failure', 'nonexistent', 'success']\n"
     ]
    }
   ],
   "source": [
    "cat_vars=['job','marital','housing','loan','poutcome']\n",
    "for va in cat_vars:\n",
    "    #cat_pre='var'+'_'+var\n",
    "    print(va)\n",
    "    #print(data[va])\n",
    "    cat_list = pd.get_dummies(data[va])\n",
    "    data1=pd.concat([data,cat_list], axis=1)\n",
    "    data=data1.drop(va, axis=1)\n",
    "    #print(list(cat_list.columns))\n",
    "    #print(list(data.columns))\n",
    "    #print(data.shape)\n",
    "\n",
    "print(data.shape)\n",
    "print(list(data.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NazsRFZmpIuD"
   },
   "source": [
    "We now split the data into input data X and the label y. We covert them to numpy and split them into training and testing datasets with 30% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2kDuXGHtBdB",
    "outputId": "a50d4289-8db6-4d6d-f580-311eecb68b3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 35)\n",
      "(12357, 35)\n",
      "Index(['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate',\n",
      "       'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'admin.',\n",
      "       'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired',\n",
      "       'self-employed', 'services', 'student', 'technician', 'unemployed',\n",
      "       'unknown', 'divorced', 'married', 'single', 'unknown', 'no', 'unknown',\n",
      "       'yes', 'no', 'unknown', 'yes', 'failure', 'nonexistent', 'success'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = data.loc[:, data.columns != 'y']\n",
    "y = data.loc[:, data.columns == 'y']\n",
    "columns = X.columns\n",
    "X=X.to_numpy()\n",
    "y=y.to_numpy()\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train1.shape)\n",
    "print(X_test1.shape)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngDOmRz9pxyR"
   },
   "source": [
    "Now, train and test as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tHoIcRBXN_bG",
    "outputId": "addd11e8-ef5a-4d63-e5ef-9ae2276afe5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 1)\n",
      "(28831, 35)\n",
      "(28831, 1)\n",
      ">> (28831, 35)\n",
      "In model, X: (28831, 35), b: 0, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 0 Loss: -4.870908291552807e-83\n",
      "In model, X: (28831, 35), b: -0.08867885262391177, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 1 Loss: 273874.6086621617\n",
      "In model, X: (28831, 35), b: -0.07735770524782351, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 2 Loss: 239381.62369542063\n",
      "In model, X: (28831, 35), b: -0.06603655787173526, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 3 Loss: 204888.6387286796\n",
      "In model, X: (28831, 35), b: -0.054715410495647016, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 4 Loss: 170395.65376193862\n",
      "In model, X: (28831, 35), b: -0.04339426311955877, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 5 Loss: 135902.66879519762\n",
      "In model, X: (28831, 35), b: -0.03207311574347053, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 6 Loss: 101409.68382845657\n",
      "In model, X: (28831, 35), b: -0.02075196836738229, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 7 Loss: 66916.69886171556\n",
      "In model, X: (28831, 35), b: -0.009430820991294052, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 8 Loss: 32423.713894974546\n",
      "In model, X: (28831, 35), b: 0.0018903263847941866, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ 6644.77603831]\n",
      " [ 7892.46251673]\n",
      " [ 2827.07450851]\n",
      " ...\n",
      " [10145.36617802]\n",
      " [ 4654.30043329]\n",
      " [ 2730.37513302]]\n",
      "Iter: 9 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.08058502763653907, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 10 Loss: 252850.70750912765\n",
      "In model, X: (28831, 35), b: -0.06926388026045083, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 11 Loss: 218357.72254238662\n",
      "In model, X: (28831, 35), b: -0.05794273288436259, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 12 Loss: 183864.7375756456\n",
      "In model, X: (28831, 35), b: -0.04662158550827435, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 13 Loss: 149371.75260890456\n",
      "In model, X: (28831, 35), b: -0.03530043813218611, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 14 Loss: 114878.76764216358\n",
      "In model, X: (28831, 35), b: -0.02397929075609787, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 15 Loss: 80385.78267542254\n",
      "In model, X: (28831, 35), b: -0.012658143380009628, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 16 Loss: 45892.79770868153\n",
      "In model, X: (28831, 35), b: -0.0013369960039213895, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-125450.79965578]\n",
      " [-123093.98008582]\n",
      " [-132200.35127049]\n",
      " ...\n",
      " [-119972.6944217 ]\n",
      " [-126588.99218347]\n",
      " [-133303.21119319]]\n",
      "Iter: 17 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.009911313107798634, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 18 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -0.07876753951611312, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 19 Loss: 251113.91588369102\n",
      "In model, X: (28831, 35), b: -0.06744639214002489, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 20 Loss: 216620.93091695002\n",
      "In model, X: (28831, 35), b: -0.05612524476393666, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 21 Loss: 182127.94595020902\n",
      "In model, X: (28831, 35), b: -0.04480409738784843, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 22 Loss: 147634.960983468\n",
      "In model, X: (28831, 35), b: -0.03348295001176018, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 23 Loss: 113141.97601672699\n",
      "In model, X: (28831, 35), b: -0.022161802635671943, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 24 Loss: 78648.99104998598\n",
      "In model, X: (28831, 35), b: -0.010840655259583703, w: (35, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-289-eebd09f18394>:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1./(1+np.exp(t))\n",
      "<ipython-input-289-eebd09f18394>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-t))\n",
      "<ipython-input-289-eebd09f18394>:13: RuntimeWarning: overflow encountered in exp\n",
      "  return z - np.log(1+np.exp(z))\n",
      "<ipython-input-289-eebd09f18394>:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  return (-(y*log_sig(z)) - ((1-y)*log_one_sig(z)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient shape: (35, 1)\n",
      "Iter: 25 Loss: 44156.006083244945\n",
      "In model, X: (28831, 35), b: 0.00048049211650453557, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-121855.82891108]\n",
      " [-118268.47834163]\n",
      " [-132390.28050685]\n",
      " ...\n",
      " [-112925.68721406]\n",
      " [-124921.05791759]\n",
      " [-133597.41574347]]\n",
      "Iter: 26 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.011472133058546089, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 27 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -0.07720671956536566, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 28 Loss: 250190.27083611538\n",
      "In model, X: (28831, 35), b: -0.06588557218927742, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 29 Loss: 215697.28586937438\n",
      "In model, X: (28831, 35), b: -0.05456442481318917, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 30 Loss: 181204.30090263332\n",
      "In model, X: (28831, 35), b: -0.04324327743710093, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 31 Loss: 146711.31593589231\n",
      "In model, X: (28831, 35), b: -0.03192213006101269, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 32 Loss: 112218.3309691513\n",
      "In model, X: (28831, 35), b: -0.020600982684924447, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 33 Loss: 77725.34600241028\n",
      "In model, X: (28831, 35), b: -0.009279835308836203, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-435949.97231738]\n",
      " [-431129.16827503]\n",
      " [-449149.01711223]\n",
      " ...\n",
      " [-426228.62779272]\n",
      " [-434853.28652039]\n",
      " [-452607.49418316]]\n",
      "Iter: 34 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.002037843578472596, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-125581.91043032]\n",
      " [-120778.12406094]\n",
      " [-139832.62544228]\n",
      " ...\n",
      " [-113298.86470744]\n",
      " [-130416.18216456]\n",
      " [-141190.73294541]]\n",
      "Iter: 35 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.012408625028994603, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 36 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -0.07627022759491715, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 37 Loss: 251214.17257412925\n",
      "In model, X: (28831, 35), b: -0.06494908021882892, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 38 Loss: 216721.18760738822\n",
      "In model, X: (28831, 35), b: -0.053627932842740686, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 39 Loss: 182228.20264064724\n",
      "In model, X: (28831, 35), b: -0.04230678546665246, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 40 Loss: 147735.2176739062\n",
      "In model, X: (28831, 35), b: -0.03098563809056422, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 41 Loss: 113242.23270716518\n",
      "In model, X: (28831, 35), b: -0.019664490714475975, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 42 Loss: 78749.24774042415\n",
      "In model, X: (28831, 35), b: -0.008343343338387734, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-456944.80646897]\n",
      " [-450927.04503418]\n",
      " [-473739.6852071 ]\n",
      " ...\n",
      " [-444068.00421986]\n",
      " [-457260.81444653]\n",
      " [-477462.29081426]]\n",
      "Iter: 43 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.002960461593803309, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-146974.64024219]\n",
      " [-140975.68627226]\n",
      " [-164814.35230277]\n",
      " ...\n",
      " [-131544.28274809]\n",
      " [-153211.68618735]\n",
      " [-166438.94469919]]\n",
      "Iter: 44 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.013067636410515684, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 45 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -0.07561121621339607, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 46 Loss: 253105.66045593534\n",
      "In model, X: (28831, 35), b: -0.06429006883730784, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 47 Loss: 218612.67548919437\n",
      "In model, X: (28831, 35), b: -0.05296892146121959, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 48 Loss: 184119.69052245337\n",
      "In model, X: (28831, 35), b: -0.04164777408513134, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 49 Loss: 149626.70555571237\n",
      "In model, X: (28831, 35), b: -0.030326626709043094, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 50 Loss: 115133.72058897134\n",
      "In model, X: (28831, 35), b: -0.01900547933295485, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 51 Loss: 80640.73562223034\n",
      "In model, X: (28831, 35), b: -0.007684331956866611, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-485683.57517658]\n",
      " [-478477.12890823]\n",
      " [-506021.65277725]\n",
      " ...\n",
      " [-469738.60704596]\n",
      " [-487253.15214402]\n",
      " [-510059.18922888]]\n",
      "Iter: 52 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.003605599020206678, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-176114.4281054 ]\n",
      " [-168928.34306217]\n",
      " [-197491.22011353]\n",
      " ...\n",
      " [-157623.41421548]\n",
      " [-183595.4292322 ]\n",
      " [-199433.14194712]]\n",
      "Iter: 53 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.013702369857144646, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 54 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -0.07497648276676709, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 55 Loss: 255073.64808635798\n",
      "In model, X: (28831, 35), b: -0.06365533539067884, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 56 Loss: 220580.66311961695\n",
      "In model, X: (28831, 35), b: -0.052334188014590596, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 57 Loss: 186087.67815287595\n",
      "In model, X: (28831, 35), b: -0.041013040638502356, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 58 Loss: 151594.69318613494\n",
      "In model, X: (28831, 35), b: -0.029691893262414115, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 59 Loss: 117101.70821939391\n",
      "In model, X: (28831, 35), b: -0.018370745886325878, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-825567.32210203]\n",
      " [-817157.3964229 ]\n",
      " [-848387.41768361]\n",
      " ...\n",
      " [-809128.414414  ]\n",
      " [-822441.69404398]\n",
      " [-854845.11739345]]\n",
      "Iter: 60 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.007053066999017078, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-515199.26021496]\n",
      " [-506806.35220881]\n",
      " [-539071.02601366]\n",
      " ...\n",
      " [-496198.65132872]\n",
      " [-518004.58968815]\n",
      " [-543428.3561557 ]]\n",
      "Iter: 61 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.004222990022938459, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-206026.64795946]\n",
      " [-197655.49974707]\n",
      " [-230931.4976926 ]\n",
      " ...\n",
      " [-184487.04678114]\n",
      " [-214734.07394659]\n",
      " [-233195.6097916 ]]\n",
      "Iter: 62 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.014344040281340875, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 63 Loss: 0.0\n",
      "In model, X: (28831, 35), b: -0.0743348123425709, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 64 Loss: 257022.19963558496\n",
      "In model, X: (28831, 35), b: -0.06301366496648265, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 65 Loss: 222529.21466884392\n",
      "In model, X: (28831, 35), b: -0.051692517590394414, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 66 Loss: 188036.22970210292\n",
      "In model, X: (28831, 35), b: -0.04037137021430616, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 67 Loss: 153543.24473536192\n",
      "In model, X: (28831, 35), b: -0.029050222838217916, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 68 Loss: 119050.2597686209\n",
      "In model, X: (28831, 35), b: -0.01772907546212968, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-854796.57800226]\n",
      " [-845200.98725678]\n",
      " [-881148.98628102]\n",
      " ...\n",
      " [-835301.73635131]\n",
      " [-852911.21773796]\n",
      " [-887924.38155212]]\n",
      "Iter: 69 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.006414865063600317, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-544523.61239782]\n",
      " [-534945.45405631]\n",
      " [-571926.08515365]\n",
      " ...\n",
      " [-522469.00432901]\n",
      " [-548566.80605413]\n",
      " [-576601.69402546]]\n",
      "Iter: 70 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.004854254980796343, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-235543.66046133]\n",
      " [-225987.78138612]\n",
      " [-263976.95477475]\n",
      " ...\n",
      " [-210953.08611648]\n",
      " [-245484.55735904]\n",
      " [-266560.56281763]]\n",
      "Iter: 71 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.014975305239198758, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[41414.76205274]\n",
      " [50907.7343968 ]\n",
      " [12187.55897217]\n",
      " ...\n",
      " [68163.28199898]\n",
      " [26242.19122442]\n",
      " [11487.14203816]]\n",
      "Iter: 72 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.05581995455627418, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 73 Loss: 203963.1990919378\n",
      "In model, X: (28831, 35), b: -0.04449880718018595, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 74 Loss: 169470.2141251968\n",
      "In model, X: (28831, 35), b: -0.03317765980409772, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 75 Loss: 134977.22915845577\n",
      "In model, X: (28831, 35), b: -0.021856512428009473, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-1006428.43895562]\n",
      " [ -996015.48161383]\n",
      " [-1034753.27310921]\n",
      " ...\n",
      " [ -985919.85696551]\n",
      " [-1002903.22391287]\n",
      " [-1042626.64137058]]\n",
      "Iter: 76 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.010538833540700674, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-696060.37706855]\n",
      " [-685664.43739975]\n",
      " [-725436.88143926]\n",
      " ...\n",
      " [-672990.09388023]\n",
      " [-698466.11955705]\n",
      " [-731209.88013282]]\n",
      "Iter: 77 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0007476289475931757, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-386591.81250285]\n",
      " [-376216.60175788]\n",
      " [-417005.5661054 ]\n",
      " ...\n",
      " [-360977.30619363]\n",
      " [-394906.5964024 ]\n",
      " [-420683.55561665]]\n",
      "Iter: 78 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.011819045131561817, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -83192.97317586]\n",
      " [ -72851.79973019]\n",
      " [-114582.18546094]\n",
      " ...\n",
      " [ -55120.73589179]\n",
      " [ -97283.83538708]\n",
      " [-116203.54442863]]\n",
      "Iter: 79 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.011215528083990952, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-101192.8639981 ]\n",
      " [ -90936.78007167]\n",
      " [-132255.82405222]\n",
      " ...\n",
      " [ -73510.35435991]\n",
      " [-114806.85561485]\n",
      " [-133991.07961224]]\n",
      "Iter: 80 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.013036484714200122, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-52115.08818863]\n",
      " [-41951.89293369]\n",
      " [-83066.77822278]\n",
      " ...\n",
      " [-24279.43923264]\n",
      " [-66529.30024957]\n",
      " [-84461.03252027]]\n",
      "Iter: 81 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.008021049939131712, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-193519.94755833]\n",
      " [-183428.10035407]\n",
      " [-223750.94136097]\n",
      " ...\n",
      " [-167067.32717258]\n",
      " [-205123.28298516]\n",
      " [-226090.14783562]]\n",
      "Iter: 82 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01676164166331749, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[45238.0509792 ]\n",
      " [55244.40441759]\n",
      " [14417.78164107]\n",
      " ...\n",
      " [73466.1907561 ]\n",
      " [29166.54779763]\n",
      " [13707.13621772]]\n",
      "Iter: 83 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.05623951197087746, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 84 Loss: 209724.49218938476\n",
      "In model, X: (28831, 35), b: -0.04491836459478923, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 85 Loss: 175231.50722264376\n",
      "In model, X: (28831, 35), b: -0.033597217218700993, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 86 Loss: 140738.5222559027\n",
      "In model, X: (28831, 35), b: -0.022276069842612756, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-1064215.89410602]\n",
      " [-1053242.8045259 ]\n",
      " [-1094057.73651806]\n",
      " ...\n",
      " [-1042616.54008002]\n",
      " [-1060475.34567373]\n",
      " [-1102363.64440795]]\n",
      "Iter: 87 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.01095839095530396, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-753847.83221896]\n",
      " [-742891.76031182]\n",
      " [-784741.34484811]\n",
      " ...\n",
      " [-729686.77699474]\n",
      " [-756038.2413179 ]\n",
      " [-790946.8831702 ]]\n",
      "Iter: 88 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0003315400217693306, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-444278.68514777]\n",
      " [-433342.97446576]\n",
      " [-476210.91218439]\n",
      " ...\n",
      " [-417571.58416426]\n",
      " [-452380.51840608]\n",
      " [-480320.83588844]]\n",
      "Iter: 89 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.011454983537429558, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-139404.65897978]\n",
      " [-128500.31443989]\n",
      " [-172325.64531172]\n",
      " ...\n",
      " [-110220.33569585]\n",
      " [-153314.12945232]\n",
      " [-174369.51932663]]\n",
      "Iter: 90 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01639411155935041, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -4799.07589221]\n",
      " [  6005.69438609]\n",
      " [-37894.11293124]\n",
      " ...\n",
      " [ 25242.50689821]\n",
      " [-21148.60836568]\n",
      " [-39014.99661257]]\n",
      "Iter: 91 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.004442006010014992, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-588245.82757256]\n",
      " [-577358.10119723]\n",
      " [-619525.50823847]\n",
      " ...\n",
      " [-562862.7067645 ]\n",
      " [-593560.33995622]\n",
      " [-624587.03669867]]\n",
      "Iter: 92 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.006809771590484472, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-279765.7394961 ]\n",
      " [-268901.75155997]\n",
      " [-312070.05350599]\n",
      " ...\n",
      " [-251854.73821605]\n",
      " [-290966.57534544]\n",
      " [-315042.6624685 ]]\n",
      "Iter: 93 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01710424628767971, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[  2048.20736281]\n",
      " [ 12853.86067393]\n",
      " [-31081.09984096]\n",
      " ...\n",
      " [ 32170.10596212]\n",
      " [-14482.98970403]\n",
      " [-32138.65527071]]\n",
      "Iter: 94 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.007365936248599078, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-682897.64245565]\n",
      " [-671959.72799477]\n",
      " [-714002.25318204]\n",
      " ...\n",
      " [-658149.12101038]\n",
      " [-686523.02843357]\n",
      " [-719690.93390151]]\n",
      "Iter: 95 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.003913589262135897, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-373627.05381047]\n",
      " [-362710.57412394]\n",
      " [-405766.07609661]\n",
      " ...\n",
      " [-346337.85351438]\n",
      " [-383156.81373937]\n",
      " [-409360.94318407]]\n",
      "Iter: 96 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01490523020417745, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -72454.86036548]\n",
      " [ -61575.7892621 ]\n",
      " [-105551.21595123]\n",
      " ...\n",
      " [ -42735.67996618]\n",
      " [ -87713.68408255]\n",
      " [-107103.87156819]]\n",
      "Iter: 97 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.012459943191405117, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-141949.61991275]\n",
      " [-131152.00492408]\n",
      " [-174548.68309652]\n",
      " ...\n",
      " [-113040.49541635]\n",
      " [-155754.47501802]\n",
      " [-176561.79647858]]\n",
      "Iter: 98 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0175898380961951, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -1926.40769231]\n",
      " [  8773.80101571]\n",
      " [-34725.126659  ]\n",
      " ...\n",
      " [ 27890.77330875]\n",
      " [-18276.77340587]\n",
      " [-35779.47739499]]\n",
      "Iter: 99 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0046709229037901695, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-625184.43411894]\n",
      " [-614382.38413155]\n",
      " [-656084.3321067 ]\n",
      " ...\n",
      " [-600307.33544512]\n",
      " [-629765.41182816]\n",
      " [-661350.07255244]]\n",
      "Iter: 100 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.006598197140606484, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-316204.48218244]\n",
      " [-305424.71146137]\n",
      " [-348135.2017278 ]\n",
      " ...\n",
      " [-288791.41723259]\n",
      " [-326683.16313307]\n",
      " [-351308.94134462]]\n",
      "Iter: 101 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01725339467104247, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-24352.05652689]\n",
      " [-13620.62575926]\n",
      " [-57173.74304523]\n",
      " ...\n",
      " [  5383.08957919]\n",
      " [-40366.47708983]\n",
      " [-58366.42889142]]\n",
      "Iter: 102 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0044997607838589354, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-382072.35475642]\n",
      " [-371351.44774332]\n",
      " [-413593.22345643]\n",
      " ...\n",
      " [-355362.43195422]\n",
      " [-391232.71256146]\n",
      " [-417194.04970804]]\n",
      "Iter: 103 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01552608661369488, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -79924.49266541]\n",
      " [ -69239.48052042]\n",
      " [-112410.71793794]\n",
      " ...\n",
      " [ -50772.37570807]\n",
      " [ -94834.48124378]\n",
      " [-113963.05047236]]\n",
      "Iter: 104 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.014166439012154867, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-119031.00849487]\n",
      " [-108429.52583306]\n",
      " [-151120.9951815 ]\n",
      " ...\n",
      " [ -90440.9319481 ]\n",
      " [-133063.22951851]\n",
      " [-152929.46066571]]\n",
      "Iter: 105 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.017149344187744782, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-37483.27557598]\n",
      " [-26971.39103181]\n",
      " [-69586.88693288]\n",
      " ...\n",
      " [ -8459.79067273]\n",
      " [-52939.48898491]\n",
      " [-70836.67622638]]\n",
      "Iter: 106 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.00860999081570668, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-277449.78678318]\n",
      " [-266986.07482272]\n",
      " [-308554.56434267]\n",
      " ...\n",
      " [-250569.79284056]\n",
      " [-288247.09236334]\n",
      " [-311415.15162003]]\n",
      "Iter: 107 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.018977303777449248, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[  6430.13795055]\n",
      " [ 16837.75581774]\n",
      " [-25513.72609928]\n",
      " ...\n",
      " [ 35544.28422423]\n",
      " [ -9739.73956198]\n",
      " [-26445.81290793]]\n",
      "Iter: 108 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.008449146188690066, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-761123.05139866]\n",
      " [-750540.48860439]\n",
      " [-790879.57955673]\n",
      " ...\n",
      " [-737957.29720918]\n",
      " [-762867.73766194]\n",
      " [-797005.24970816]]\n",
      "Iter: 109 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.00285119025472154, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-451253.68003731]\n",
      " [-440690.32599934]\n",
      " [-482053.47236772]\n",
      " ...\n",
      " [-425536.28608179]\n",
      " [-458916.97425943]\n",
      " [-486081.73004691]]\n",
      "Iter: 110 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.014023192613293912, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-145001.3981431 ]\n",
      " [-134466.61090049]\n",
      " [-176803.28781239]\n",
      " ...\n",
      " [-116787.78303711]\n",
      " [-158502.13716686]\n",
      " [-178756.77484289]]\n",
      "Iter: 111 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01939588204121481, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[  1997.63901882]\n",
      " [ 12439.05271657]\n",
      " [-30040.50937533]\n",
      " ...\n",
      " [ 31187.70567267]\n",
      " [-14185.15993248]\n",
      " [-30989.6888678 ]]\n",
      "Iter: 112 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0050084188141608045, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-681147.13023419]\n",
      " [-670574.33015503]\n",
      " [-711164.29944866]\n",
      " ...\n",
      " [-657317.76192647]\n",
      " [-684457.28841115]\n",
      " [-716732.36065928]]\n",
      "Iter: 113 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0062711066965741675, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-371876.541589  ]\n",
      " [-361325.1762842 ]\n",
      " [-402928.12236323]\n",
      " ...\n",
      " [-345506.49443047]\n",
      " [-381091.07371696]\n",
      " [-406402.36994184]]\n",
      "Iter: 114 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.017290495548851228, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -69915.10342477]\n",
      " [ -59399.86514191]\n",
      " [-101930.67704592]\n",
      " ...\n",
      " [ -41105.06067315]\n",
      " [ -84875.36134072]\n",
      " [-103357.65850608]]\n",
      "Iter: 115 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.014813994560331955, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-140296.03953498]\n",
      " [-129862.1620173 ]\n",
      " [-171811.46886426]\n",
      " ...\n",
      " [-112303.20186062]\n",
      " [-153785.55840741]\n",
      " [-173704.87776279]]\n",
      "Iter: 116 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.019815557706798576, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -3318.2672425 ]\n",
      " [  7024.21300158]\n",
      " [-35039.82787655]\n",
      " ...\n",
      " [ 25570.95421653]\n",
      " [-19297.45307877]\n",
      " [-35997.96831052]]\n",
      "Iter: 117 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0016144660307834768, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-603392.76148032]\n",
      " [-592959.47993373]\n",
      " [-633263.95925065]\n",
      " ...\n",
      " [-579279.75801114]\n",
      " [-608030.12644558]\n",
      " [-638275.82665199]]\n",
      "Iter: 118 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.00965465401361318, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-294412.80954383]\n",
      " [-284001.80726354]\n",
      " [-325314.82887175]\n",
      " ...\n",
      " [-267763.83979861]\n",
      " [-304947.87775049]\n",
      " [-328234.69544416]]\n",
      "Iter: 119 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.020164175014107225, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -6562.9438566 ]\n",
      " [  3796.10412243]\n",
      " [-38330.66702604]\n",
      " ...\n",
      " [ 22364.55955289]\n",
      " [-22551.84013668]\n",
      " [-39296.03860706]]\n",
      "Iter: 120 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.00035072907674069847, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-561502.30867536]\n",
      " [-551073.37368678]\n",
      " [-591515.82443904]\n",
      " ...\n",
      " [-537030.40388757]\n",
      " [-566982.56710368]\n",
      " [-596228.07206692]]\n",
      "Iter: 121 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.011602506677240164, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-253022.2205989 ]\n",
      " [-242617.02404952]\n",
      " [-284060.36970656]\n",
      " ...\n",
      " [-226022.43533912]\n",
      " [-264388.80249289]\n",
      " [-286683.69783676]]\n",
      "Iter: 122 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02156400645178839, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ 19673.4916183 ]\n",
      " [ 30014.51841946]\n",
      " [-12139.22398067]\n",
      " ...\n",
      " [ 48791.21813449]\n",
      " [  3160.40180572]\n",
      " [-12908.56475869]]\n",
      "Iter: 123 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.017009057264335672, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-1059299.67824446]\n",
      " [-1048606.09290072]\n",
      " [-1088362.39404318]\n",
      " ...\n",
      " [-1038253.40485137]\n",
      " [-1055680.8437249 ]\n",
      " [-1096448.89550279]]\n",
      "Iter: 124 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.005691378377026869, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-748931.61635739]\n",
      " [-738255.04868663]\n",
      " [-779046.00237323]\n",
      " ...\n",
      " [-725323.64176609]\n",
      " [-751243.73936907]\n",
      " [-785032.13426504]]\n",
      "Iter: 125 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.005605489577605298, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-439162.32031739]\n",
      " [-428505.35498875]\n",
      " [-470318.42591721]\n",
      " ...\n",
      " [-413004.59553551]\n",
      " [-447390.6448631 ]\n",
      " [-474207.74274684]]\n",
      "Iter: 126 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01674974402594216, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-133700.63132566]\n",
      " [-123073.85495293]\n",
      " [-165851.13527406]\n",
      " ...\n",
      " [-105057.62704385]\n",
      " [-147749.27976022]\n",
      " [-167670.68125972]]\n",
      "Iter: 127 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02061017203747646, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-27559.20229149]\n",
      " [-17016.9840365 ]\n",
      " [-59825.5533972 ]\n",
      " ...\n",
      " [  1736.10659882]\n",
      " [-43507.04837651]\n",
      " [-60922.20422597]]\n",
      "Iter: 128 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.009280864953396872, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-345501.49695964]\n",
      " [-334983.52125623]\n",
      " [-376566.06996501]\n",
      " ...\n",
      " [-318936.20229434]\n",
      " [-355335.55478516]\n",
      " [-379801.31959993]]\n",
      "Iter: 129 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.020171919720834697, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-47065.73601768]\n",
      " [-36587.99990345]\n",
      " [-79069.20111822]\n",
      " ...\n",
      " [-18101.35938406]\n",
      " [-62572.05131076]\n",
      " [-80280.45227493]]\n",
      "Iter: 130 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01388752509861842, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-224018.68297798]\n",
      " [-213604.97706296]\n",
      " [-255197.98558773]\n",
      " ...\n",
      " [-196714.4100763 ]\n",
      " [-236046.23890977]\n",
      " [-257594.15396707]]\n",
      "Iter: 131 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.023131047695803434, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[28977.28311019]\n",
      " [39316.68311777]\n",
      " [-2875.40596958]\n",
      " ...\n",
      " [58205.65703374]\n",
      " [12196.68303468]\n",
      " [-3549.75926323]]\n",
      "Iter: 132 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.025771695067460226, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 133 Loss: 136258.37448816263\n",
      "In model, X: (28831, 35), b: -0.014450547691371987, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-1028153.61274352]\n",
      " [-1017294.90003793]\n",
      " [-1057844.87578096]\n",
      " ...\n",
      " [-1006350.49768986]\n",
      " [-1025470.35820215]\n",
      " [-1065707.13451519]]\n",
      "Iter: 134 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0031328688040631863, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-717785.55085645]\n",
      " [-706943.85582384]\n",
      " [-748528.48411101]\n",
      " ...\n",
      " [-693420.73460458]\n",
      " [-721033.25384633]\n",
      " [-754290.37327744]]\n",
      "Iter: 135 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.008153593684230666, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-408316.98629075]\n",
      " [-397496.02018197]\n",
      " [-440097.16877715]\n",
      " ...\n",
      " [-381407.94691798]\n",
      " [-417473.73069168]\n",
      " [-443764.04876127]]\n",
      "Iter: 136 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.019221541379419872, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-104998.67453481]\n",
      " [ -94211.6382658 ]\n",
      " [-137754.30965935]\n",
      " ...\n",
      " [ -75632.38024414]\n",
      " [-119929.90370864]\n",
      " [-139365.18938652]]\n",
      "Iter: 137 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.020404296029836136, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -72882.66187382]\n",
      " [ -62177.81376674]\n",
      " [-105497.70289559]\n",
      " ...\n",
      " [ -43480.44211186]\n",
      " [ -88294.61544608]\n",
      " [-106886.94569285]]\n",
      "Iter: 138 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01816365227831867, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-136660.12187192]\n",
      " [-126037.22117347]\n",
      " [-168796.58058429]\n",
      " ...\n",
      " [-108021.56670576]\n",
      " [-150726.38047258]\n",
      " [-170607.82184947]]\n",
      "Iter: 139 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.022131603441996578, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-27403.26341675]\n",
      " [-16862.92666187]\n",
      " [-59672.4562437 ]\n",
      " ...\n",
      " [  1918.57640065]\n",
      " [-43429.77484194]\n",
      " [-60740.5593576 ]]\n",
      "Iter: 140 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.010761897222996196, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-346485.13192666]\n",
      " [-335968.64423336]\n",
      " [-377549.70634961]\n",
      " ...\n",
      " [-319901.76801526]\n",
      " [-356376.63884952]\n",
      " [-380764.13362522]]\n",
      "Iter: 141 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.021642546524095706, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-48318.46200703]\n",
      " [-37842.26109125]\n",
      " [-80320.7903401 ]\n",
      " ...\n",
      " [-19338.49815864]\n",
      " [-63876.78599159]\n",
      " [-81513.10563932]]\n",
      "Iter: 142 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.015624718475681034, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-217800.68003356]\n",
      " [-207390.16921076]\n",
      " [-249000.26010387]\n",
      " ...\n",
      " [-190422.88949263]\n",
      " [-230020.4650216 ]\n",
      " [-251327.03205974]]\n",
      "Iter: 143 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.024615041391986402, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[28231.41528037]\n",
      " [38564.65650175]\n",
      " [-3607.64244865]\n",
      " ...\n",
      " [57466.46394057]\n",
      " [11396.79871887]\n",
      " [-4259.40891693]]\n",
      "Iter: 144 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.023313661915854676, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 145 Loss: 133260.2856898072\n",
      "In model, X: (28831, 35), b: -0.01199251453976644, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-1001731.01633046]\n",
      " [ -990896.30929266]\n",
      " [-1031450.6385926 ]\n",
      " ...\n",
      " [ -979742.30259936]\n",
      " [ -999596.63386476]\n",
      " [-1039104.60831648]]\n",
      "Iter: 146 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.000678304141237076, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-691458.05072603]\n",
      " [-680640.77609219]\n",
      " [-722227.73746524]\n",
      " ...\n",
      " [-666909.57057706]\n",
      " [-695252.22218093]\n",
      " [-727781.92078982]]\n",
      "Iter: 147 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.010601221369497895, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-382187.46208084]\n",
      " [-371391.62222136]\n",
      " [-413991.56037981]\n",
      " ...\n",
      " [-355098.30308106]\n",
      " [-391886.00748673]\n",
      " [-417451.93007238]]\n",
      "Iter: 148 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.021599799289098325, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -80777.17194643]\n",
      " [ -70017.74442171]\n",
      " [-113542.36987162]\n",
      " ...\n",
      " [ -51253.57826629]\n",
      " [ -96210.13037306]\n",
      " [-114959.27756038]]\n",
      "Iter: 149 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02029564829586713, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-118355.47426962]\n",
      " [-107679.61494618]\n",
      " [-150729.70670455]\n",
      " ...\n",
      " [ -89381.28353411]\n",
      " [-132939.70556833]\n",
      " [-152392.49142329]]\n",
      "Iter: 150 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.022584915815776315, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-55279.61283017]\n",
      " [-44682.73174209]\n",
      " [-87632.48823638]\n",
      " ...\n",
      " [-26002.23833022]\n",
      " [-70939.02563906]\n",
      " [-88866.23661713]]\n",
      "Iter: 151 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01777759729325023, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-190894.94739261]\n",
      " [-180370.43350247]\n",
      " [-222544.46608113]\n",
      " ...\n",
      " [-162955.04583159]\n",
      " [-203852.43829287]\n",
      " [-224684.13225322]]\n",
      "Iter: 152 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.025599039490884726, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[23245.58876305]\n",
      " [33685.0763107 ]\n",
      " [-8905.54587984]\n",
      " ...\n",
      " [52751.66261135]\n",
      " [ 6300.20511199]\n",
      " [-9586.1517115 ]]\n",
      "Iter: 153 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.016348772417361757, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 154 Loss: 115185.29125611357\n",
      "In model, X: (28831, 35), b: -0.005027625041273519, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-839597.57108136]\n",
      " [-828763.7241131 ]\n",
      " [-869895.39336371]\n",
      " ...\n",
      " [-816230.18091552]\n",
      " [-840621.44825596]\n",
      " [-876436.38629451]]\n",
      "Iter: 155 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.006283116868476403, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-529426.09889592]\n",
      " [-518610.19634414]\n",
      " [-560772.0798329 ]\n",
      " ...\n",
      " [-503501.15963987]\n",
      " [-536375.94634223]\n",
      " [-565213.87076285]]\n",
      "Iter: 156 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01751408353629923, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-221529.11734253]\n",
      " [-210738.41101355]\n",
      " [-253893.24927668]\n",
      " ...\n",
      " [-193085.09501518]\n",
      " [-234352.26945262]\n",
      " [-256249.78013256]]\n",
      "Iter: 157 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.026344855968750417, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ 20199.80434331]\n",
      " [ 30912.66110126]\n",
      " [-12786.49056758]\n",
      " ...\n",
      " [ 50462.80091999]\n",
      " [  2846.38116114]\n",
      " [-13497.43992146]]\n",
      "Iter: 158 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.011902345841754588, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-1049696.11780188]\n",
      " [-1038636.17787288]\n",
      " [-1079947.91643981]\n",
      " ...\n",
      " [-1027443.4925092 ]\n",
      " [-1047083.56055828]\n",
      " [-1087914.07706949]]\n",
      "Iter: 159 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0005846669544457897, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-739328.05591481]\n",
      " [-728285.13365879]\n",
      " [-770631.52476986]\n",
      " ...\n",
      " [-714513.72942392]\n",
      " [-742646.45620246]\n",
      " [-776497.31583174]]\n",
      "Iter: 160 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.010701795533848063, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-429859.49134911]\n",
      " [-418837.29801692]\n",
      " [-462200.209436  ]\n",
      " ...\n",
      " [-402500.94173733]\n",
      " [-439086.93304782]\n",
      " [-465970.99131556]]\n",
      "Iter: 161 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.021794022650493343, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-125850.20697148]\n",
      " [-114860.67860763]\n",
      " [-159172.64772502]\n",
      " ...\n",
      " [ -96025.24643165]\n",
      " [-140866.90393259]\n",
      " [-160883.03006093]]\n",
      "Iter: 162 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.024357235858498617, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-55083.04677641]\n",
      " [-44171.0852365 ]\n",
      " [-88415.57573018]\n",
      " ...\n",
      " [-24886.9340496 ]\n",
      " [-71323.23154627]\n",
      " [-89645.56850883]]\n",
      "Iter: 163 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.019244683554025935, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-199247.5598044 ]\n",
      " [-188406.3832533 ]\n",
      " [-231851.31573148]\n",
      " ...\n",
      " [-170456.02785431]\n",
      " [-212624.97698904]\n",
      " [-234044.90210596]]\n",
      "Iter: 164 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.027104279128234248, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ 16021.22303425]\n",
      " [ 26778.80100818]\n",
      " [-17092.36445185]\n",
      " ...\n",
      " [ 46391.94691222]\n",
      " [ -1366.88406228]\n",
      " [-17819.76654912]]\n",
      "Iter: 165 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.007337965504866772, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-947602.65663913]\n",
      " [-936560.98742594]\n",
      " [-978166.5385548 ]\n",
      " ...\n",
      " [-924524.20081138]\n",
      " [-946969.16701964]\n",
      " [-985424.03814042]]\n",
      "Iter: 166 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0039727764048831495, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-637431.1844537 ]\n",
      " [-626407.45965698]\n",
      " [-669043.22502399]\n",
      " ...\n",
      " [-611795.17953572]\n",
      " [-642723.66510591]\n",
      " [-674201.52260876]]\n",
      "Iter: 167 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.015241896449279804, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-328451.2325172 ]\n",
      " [-317449.78698679]\n",
      " [-361094.09464509]\n",
      " ...\n",
      " [-300279.26132319]\n",
      " [-339641.41641082]\n",
      " [-364160.39140093]]\n",
      "Iter: 168 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.025858940603141974, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-37539.13759261]\n",
      " [-26585.49471992]\n",
      " [-71071.04756222]\n",
      " ...\n",
      " [ -7052.04097184]\n",
      " [-54246.56407742]\n",
      " [-72163.14847031]]\n",
      "Iter: 169 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.017003888522981247, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-286361.57085813]\n",
      " [-275453.56814561]\n",
      " [-318871.01638715]\n",
      " ...\n",
      " [-258085.0961094 ]\n",
      " [-298244.59194095]\n",
      " [-321633.83449658]]\n",
      "Iter: 170 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.027190840068193, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -7300.27199717]\n",
      " [  3550.05382161]\n",
      " [-40626.47455243]\n",
      " ...\n",
      " [ 23164.71759481]\n",
      " [-24459.38534601]\n",
      " [-41493.77834599]]\n",
      "Iter: 171 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.007592462229399496, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-556271.15300026]\n",
      " [-545353.60580083]\n",
      " [-587856.1665698 ]\n",
      " ...\n",
      " [-530219.10508652]\n",
      " [-563031.90575029]\n",
      " [-592429.79706097]]\n",
      "Iter: 172 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.018826897386001767, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-248276.69242485]\n",
      " [-237384.0950699 ]\n",
      " [-280880.94429226]\n",
      " ...\n",
      " [-219704.09009739]\n",
      " [-260912.91777188]\n",
      " [-283368.70770212]]\n",
      "Iter: 173 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.028264655354854732, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ 10253.68797992]\n",
      " [ 21077.42295865]\n",
      " [-23059.88317469]\n",
      " ...\n",
      " [ 40810.93917568]\n",
      " [ -7247.41394273]\n",
      " [-23789.89319174]]\n",
      "Iter: 174 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0015393046124236442, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-823814.44056026]\n",
      " [-812779.81823397]\n",
      " [-854814.45232435]\n",
      " ...\n",
      " [-799664.4052054 ]\n",
      " [-825668.88317659]\n",
      " [-861191.25615476]]\n",
      "Iter: 175 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.009764500319767402, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-513844.27433349]\n",
      " [-502828.45947206]\n",
      " [-545889.11942002]\n",
      " ...\n",
      " [-487140.68373363]\n",
      " [-521619.75491741]\n",
      " [-550167.91003968]]\n",
      "Iter: 176 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.020978124543693032, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-206423.99001648]\n",
      " [-195434.34047311]\n",
      " [-239482.34819604]\n",
      " ...\n",
      " [-177207.94376562]\n",
      " [-220062.33986326]\n",
      " [-241678.93873536]]\n",
      "Iter: 177 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.028834251629121913, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[  8976.57628396]\n",
      " [ 19885.99131075]\n",
      " [-24601.89111302]\n",
      " ...\n",
      " [ 39781.00009678]\n",
      " [ -8677.0880998 ]\n",
      " [-25332.80567967]]\n",
      "Iter: 178 Loss: nan\n",
      "In model, X: (28831, 35), b: -4.1409814091555225e-05, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-799148.03030285]\n",
      " [-788041.87002473]\n",
      " [-830461.77469945]\n",
      " ...\n",
      " [-774572.21414925]\n",
      " [-801631.27601098]\n",
      " [-836662.69914485]]\n",
      "Iter: 179 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.011258926629320052, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-489278.65894151]\n",
      " [-478191.70741967]\n",
      " [-521635.66751044]\n",
      " ...\n",
      " [-462151.20302186]\n",
      " [-497680.51260847]\n",
      " [-525739.1794836 ]]\n",
      "Iter: 180 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.022451739920569053, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-182435.61246462]\n",
      " [-171375.9573477 ]\n",
      " [-215800.64542917]\n",
      " ...\n",
      " [-152803.60536948]\n",
      " [-196687.77595072]\n",
      " [-217825.66260917]]\n",
      "Iter: 181 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.028799074386942056, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -7795.68785069]\n",
      " [  3185.28797248]\n",
      " [-41538.70390699]\n",
      " ...\n",
      " [ 23081.80058891]\n",
      " [-25272.7566216 ]\n",
      " [-42377.50359476]]\n",
      "Iter: 182 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.009431033026077802, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-550336.16733442]\n",
      " [-539290.86794147]\n",
      " [-582351.9707216 ]\n",
      " ...\n",
      " [-523825.70305646]\n",
      " [-557533.69074656]\n",
      " [-586853.39129723]]\n",
      "Iter: 183 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02066546818268007, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-242341.706759  ]\n",
      " [-231321.35721053]\n",
      " [-275376.74844407]\n",
      " ...\n",
      " [-213310.68806733]\n",
      " [-255414.70276816]\n",
      " [-277792.30193838]]\n",
      "Iter: 184 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.029804936153313755, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[  8066.66897227]\n",
      " [ 19015.43477885]\n",
      " [-25640.241841  ]\n",
      " ...\n",
      " [ 39006.05833829]\n",
      " [ -9710.2889869 ]\n",
      " [-26353.00436569]]\n",
      "Iter: 185 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.001528253958886877, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-783345.08375384]\n",
      " [-772208.54518358]\n",
      " [-814819.15074358]\n",
      " ...\n",
      " [-758518.87833379]\n",
      " [-786258.57542057]\n",
      " [-820888.07108307]]\n",
      "Iter: 186 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.012825121913519043, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-473575.78771384]\n",
      " [-462458.8514857 ]\n",
      " [-506091.57428756]\n",
      " ...\n",
      " [-446199.83210321]\n",
      " [-482405.4809146 ]\n",
      " [-510063.67956486]]\n",
      "Iter: 187 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.023990187294532533, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-167516.16773364]\n",
      " [-156428.10350709]\n",
      " [-201032.43465413]\n",
      " ...\n",
      " [-137646.44547521]\n",
      " [-182179.18255343]\n",
      " [-202931.02789341]]\n",
      "Iter: 188 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.029064586378851502, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-27229.4606747 ]\n",
      " [-16214.22458869]\n",
      " [-61017.6727001 ]\n",
      " ...\n",
      " [  3608.84196616]\n",
      " [-44456.75851464]\n",
      " [-61966.50830725]]\n",
      "Iter: 189 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.017144472869504712, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-361739.31693266]\n",
      " [-350742.64750468]\n",
      " [-394284.40294667]\n",
      " ...\n",
      " [-333754.08913265]\n",
      " [-372544.4114809 ]\n",
      " [-397484.10769413]]\n",
      "Iter: 190 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02798350030526512, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-64671.85280404]\n",
      " [-53716.06201945]\n",
      " [-98148.88451773]\n",
      " ...\n",
      " [-34301.09172138]\n",
      " [-81121.3764495 ]\n",
      " [-99333.98863124]]\n",
      "Iter: 191 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.024372803485864267, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-166823.9028898 ]\n",
      " [-155945.79673269]\n",
      " [-199699.72689286]\n",
      " ...\n",
      " [-137525.59846221]\n",
      " [-181202.16451206]\n",
      " [-201565.17004352]]\n",
      "Iter: 192 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.029547788746904148, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-23721.99753594]\n",
      " [-12915.62266939]\n",
      " [-56883.03369743]\n",
      " ...\n",
      " [  6571.33907098]\n",
      " [-40719.61868106]\n",
      " [-57780.01285426]]\n",
      "Iter: 193 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01662154085985072, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-386342.71493481]\n",
      " [-375545.02905621]\n",
      " [-418191.15680971]\n",
      " ...\n",
      " [-359110.60976346]\n",
      " [-396395.19912597]\n",
      " [-421529.53041663]]\n",
      "Iter: 194 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0275889023804362, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -85751.63809067]\n",
      " [ -74990.75943474]\n",
      " [-118556.96722362]\n",
      " ...\n",
      " [ -56093.27515828]\n",
      " [-101521.78948199]\n",
      " [-119857.52750588]]\n",
      "Iter: 195 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.026832771826518546, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-107993.21653592]\n",
      " [ -97316.30809473]\n",
      " [-140460.51660852]\n",
      " ...\n",
      " [ -78757.37076285]\n",
      " [-123206.25298005]\n",
      " [-141903.95815071]]\n",
      "Iter: 196 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02830451680029958, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -67774.21004103]\n",
      " [ -57178.44538139]\n",
      " [-100132.66259468]\n",
      " ...\n",
      " [ -38432.09097475]\n",
      " [ -83623.50645514]\n",
      " [-101300.33339574]]\n",
      "Iter: 197 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02544111508708488, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-149005.9465617 ]\n",
      " [-138490.42898701]\n",
      " [-180829.28627331]\n",
      " ...\n",
      " [-120568.60703826]\n",
      " [-163179.21294523]\n",
      " [-182536.42145975]]\n",
      "Iter: 198 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02970388779702381, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-30707.22085647]\n",
      " [-20259.97612524]\n",
      " [-62737.17197766]\n",
      " ...\n",
      " [ -1476.10034883]\n",
      " [-47023.60833112]\n",
      " [-63645.8235373 ]]\n",
      "Iter: 199 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.019340060929605225, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-321720.9930341 ]\n",
      " [-311306.67365801]\n",
      " [-352617.91503277]\n",
      " ...\n",
      " [-295018.51509784]\n",
      " [-332425.43879011]\n",
      " [-355482.83595544]]\n",
      "Iter: 200 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0299848529937029, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-30009.33284626]\n",
      " [-19641.41833492]\n",
      " [-61802.31358224]\n",
      " ...\n",
      " [  -981.40340384]\n",
      " [-46247.82909038]\n",
      " [-62688.00633014]]\n",
      "Iter: 201 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.019499611336904997, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-324420.21451404]\n",
      " [-314084.15057269]\n",
      " [-355071.43487205]\n",
      " ...\n",
      " [-297946.38651977]\n",
      " [-334983.4266851 ]\n",
      " [-357936.39632534]]\n",
      "Iter: 202 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.030196430732694252, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-31249.90987363]\n",
      " [-20958.5389232 ]\n",
      " [-62807.56674169]\n",
      " ...\n",
      " [ -2433.78045446]\n",
      " [-47377.33127168]\n",
      " [-63683.76325563]]\n",
      "Iter: 203 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02014828414096017, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-313458.37888743]\n",
      " [-303202.66319714]\n",
      " [-343905.84438498]\n",
      " ...\n",
      " [-287104.7349735 ]\n",
      " [-324138.10271852]\n",
      " [-346678.70729083]]\n",
      "Iter: 204 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03074798585092513, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-22955.0118323 ]\n",
      " [-12746.40582323]\n",
      " [-54292.0202496 ]\n",
      " ...\n",
      " [  5711.85718772]\n",
      " [-39144.30342614]\n",
      " [-55093.83280042]]\n",
      "Iter: 205 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.018025745880718806, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-379885.6396335 ]\n",
      " [-369687.74584072]\n",
      " [-409924.03114946]\n",
      " ...\n",
      " [-354237.94203889]\n",
      " [-389235.60432993]\n",
      " [-413128.67238589]]\n",
      "Iter: 206 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02902432380031924, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -78431.27234227]\n",
      " [ -68269.00072471]\n",
      " [-109433.22762063]\n",
      " ...\n",
      " [ -50346.8513208 ]\n",
      " [ -93517.04548745]\n",
      " [-110594.41766411]]\n",
      "Iter: 207 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02794518150704836, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-109726.39422926]\n",
      " [ -99647.88760662]\n",
      " [-140358.96603193]\n",
      " ...\n",
      " [ -82139.48786072]\n",
      " [-124082.85599843]\n",
      " [-141723.83590026]]\n",
      "Iter: 208 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.030040148841463422, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-52034.00654915]\n",
      " [-42034.22793926]\n",
      " [-82626.80245138]\n",
      " ...\n",
      " [-24188.51896674]\n",
      " [-67362.64535346]\n",
      " [-83599.08657931]]\n",
      "Iter: 209 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.025218949431561506, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-188073.47363202]\n",
      " [-178145.94689278]\n",
      " [-217961.75885021]\n",
      " ...\n",
      " [-161568.70092112]\n",
      " [-200692.231521  ]\n",
      " [-219842.83804633]]\n",
      "Iter: 210 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03252011831768266, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ 12869.2338906 ]\n",
      " [ 22724.76856836]\n",
      " [-17511.10565978]\n",
      " ...\n",
      " [ 40859.13389326]\n",
      " [ -3489.11186735]\n",
      " [-18028.89047079]]\n",
      "Iter: 211 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.00022588069342700673, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-903451.5433316 ]\n",
      " [-893339.06023791]\n",
      " [-931367.14132461]\n",
      " ...\n",
      " [-882431.89265457]\n",
      " [-902654.84536129]\n",
      " [-938092.45289969]]\n",
      "Iter: 212 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.011088329705102356, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-593178.57772716]\n",
      " [-583083.52703743]\n",
      " [-622144.24019725]\n",
      " ...\n",
      " [-569599.16063228]\n",
      " [-598310.43367745]\n",
      " [-626769.76537303]]\n",
      "Iter: 213 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.022360918238278453, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-284098.41637311]\n",
      " [-274025.33662109]\n",
      " [-314096.18653996]\n",
      " ...\n",
      " [-257981.37118373]\n",
      " [-295130.27637704]\n",
      " [-316629.09645381]]\n",
      "Iter: 214 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03266579840199113, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -1662.89459478]\n",
      " [  8357.20065993]\n",
      " [-32503.45253666]\n",
      " ...\n",
      " [ 26683.17217789]\n",
      " [-18040.56625378]\n",
      " [-33119.26914637]]\n",
      "Iter: 215 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.010505617460060413, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-622247.42674965]\n",
      " [-612126.63631734]\n",
      " [-651195.12197053]\n",
      " ...\n",
      " [-598822.24668962]\n",
      " [-626905.0789168 ]\n",
      " [-656004.20430556]]\n",
      "Iter: 216 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.021778205993236502, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-313167.2653956 ]\n",
      " [-303068.445901  ]\n",
      " [-343147.06831324]\n",
      " ...\n",
      " [-287204.45724107]\n",
      " [-323724.92161639]\n",
      " [-345863.53538634]]\n",
      "Iter: 217 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03239525014709866, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-22161.36884114]\n",
      " [-12108.74822431]\n",
      " [-53035.22315988]\n",
      " ...\n",
      " [  6121.24229606]\n",
      " [-38239.19412567]\n",
      " [-53777.48411872]]\n",
      "Iter: 218 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.019518550907764016, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-383410.59394252]\n",
      " [-373367.14027089]\n",
      " [-412975.25545435]\n",
      " ...\n",
      " [-358179.0506368 ]\n",
      " [-392568.79240156]\n",
      " [-416149.61917908]]\n",
      "Iter: 219 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03055181371515883, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -80992.31315673]\n",
      " [ -70983.07689336]\n",
      " [-111528.22820807]\n",
      " ...\n",
      " [ -53312.15693933]\n",
      " [ -95906.65636636]\n",
      " [-112652.86974354]]\n",
      "Iter: 220 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.029969107600213107, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -98385.41882563]\n",
      " [ -88460.19283623]\n",
      " [-128600.32075599]\n",
      " ...\n",
      " [ -71087.42311145]\n",
      " [-112834.94771769]\n",
      " [-129835.29814952]]\n",
      "Iter: 221 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03128713333639984, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-62477.04867423]\n",
      " [-52633.44168881]\n",
      " [-92566.44693158]\n",
      " ...\n",
      " [-35110.01794542]\n",
      " [-77480.12595327]\n",
      " [-93554.49199147]]\n",
      "Iter: 222 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.028356563133117088, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-145602.98815083]\n",
      " [-135839.40487258]\n",
      " [-175151.25982759]\n",
      " ...\n",
      " [-119155.94743029]\n",
      " [-158894.22854189]\n",
      " [-176691.52667201]]\n",
      "Iter: 223 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03280316574835762, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-21924.85071156]\n",
      " [-12223.58273672]\n",
      " [-51716.33489613]\n",
      " ...\n",
      " [  5375.80667113]\n",
      " [-37466.93227672]\n",
      " [-52423.95551866]]\n",
      "Iter: 224 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.020115819670991242, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-377892.60763523]\n",
      " [-368202.37771629]\n",
      " [-406387.87998202]\n",
      " ...\n",
      " [-353603.96935692]\n",
      " [-386613.27832578]\n",
      " [-409491.81036232]]\n",
      "Iter: 225 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.031173361899842128, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -74805.35175765]\n",
      " [ -65148.43976698]\n",
      " [-104277.00271738]\n",
      " ...\n",
      " [ -48060.02156061]\n",
      " [ -89296.24557598]\n",
      " [-105326.82301475]]\n",
      "Iter: 226 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03013628377356129, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-104927.0509515 ]\n",
      " [ -95353.92936054]\n",
      " [-134033.37905617]\n",
      " ...\n",
      " [ -78669.51779089]\n",
      " [-118710.94725711]\n",
      " [-135279.00166875]]\n",
      "Iter: 227 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03227980983925447, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-45877.47030095]\n",
      " [-36382.84653756]\n",
      " [-74949.50484892]\n",
      " ...\n",
      " [-19349.42053045]\n",
      " [-60659.68286136]\n",
      " [-75793.47925933]]\n",
      "Iter: 228 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.026740573994412715, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-202001.11318276]\n",
      " [-192574.9264449 ]\n",
      " [-230309.26216403]\n",
      " ...\n",
      " [-176971.13717701]\n",
      " [-213695.73185948]\n",
      " [-232197.53325869]]\n",
      "Iter: 229 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03513778532943405, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[28723.97665481]\n",
      " [38082.09024509]\n",
      " [ -195.13613688]\n",
      " ...\n",
      " [55502.5197678 ]\n",
      " [12707.36544939]\n",
      " [ -517.41623965]]\n",
      "Iter: 230 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.01684665516528033, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 231 Loss: 146084.26017750482\n",
      "In model, X: (28831, 35), b: -0.005525507789192093, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 232 Loss: 111591.27521076378\n",
      "In model, X: (28831, 35), b: 0.005795639586896149, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-804287.38995358]\n",
      " [-794368.42588536]\n",
      " [-831969.30583171]\n",
      " ...\n",
      " [-782935.34197143]\n",
      " [-805202.29827415]\n",
      " [-837969.08308354]]\n",
      "Iter: 233 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01710638149664608, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-494115.91776815]\n",
      " [-484214.89811641]\n",
      " [-522845.9923009 ]\n",
      " ...\n",
      " [-470206.32069577]\n",
      " [-500956.79636042]\n",
      " [-526746.56755188]]\n",
      "Iter: 234 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02833734816446891, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-186213.37146689]\n",
      " [-176337.42902917]\n",
      " [-215961.96397534]\n",
      " ...\n",
      " [-159784.35497024]\n",
      " [-198927.74680998]\n",
      " [-217777.28710745]]\n",
      "Iter: 235 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03532635305503809, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[  6508.76688946]\n",
      " [ 16316.42936316]\n",
      " [-23713.16016316]\n",
      " ...\n",
      " [ 34362.00715479]\n",
      " [ -9788.12065082]\n",
      " [-24222.88893006]]\n",
      "Iter: 236 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0077620141802138805, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-765075.75052522]\n",
      " [-755090.82438591]\n",
      " [-793103.13112276]\n",
      " ...\n",
      " [-743198.36524789]\n",
      " [-766873.4766422 ]\n",
      " [-798833.97628495]]\n",
      "Iter: 237 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.019065819112404927, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-455105.58429844]\n",
      " [-445139.46562399]\n",
      " [-484177.79821843]\n",
      " ...\n",
      " [-430674.64377612]\n",
      " [-462824.34838302]\n",
      " [-487810.63016987]]\n",
      "Iter: 238 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.030265569381212797, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-148065.54163898]\n",
      " [-138126.27498787]\n",
      " [-178147.82313501]\n",
      " ...\n",
      " [-121127.21399562]\n",
      " [-161638.93291493]\n",
      " [-179700.92178552]]\n",
      "Iter: 239 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.034705235018894456, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-24577.47870577]\n",
      " [-14700.48841906]\n",
      " [-54902.40498561]\n",
      " ...\n",
      " [  3212.9537757 ]\n",
      " [-40398.07396748]\n",
      " [-55624.16366765]]\n",
      "Iter: 240 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.022874220638806408, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-356643.30992878]\n",
      " [-346785.66938271]\n",
      " [-385731.36893428]\n",
      " ...\n",
      " [-331687.66923792]\n",
      " [-366087.22874721]\n",
      " [-388687.45863489]]\n",
      "Iter: 241 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03382077122671525, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-56596.9079385 ]\n",
      " [-46776.23777382]\n",
      " [-86639.34849046]\n",
      " ...\n",
      " [-29220.19096518]\n",
      " [-71747.46314003]\n",
      " [-87561.40778826]]\n",
      "Iter: 242 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0300262445020092, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-163899.80354172]\n",
      " [-154156.07835697]\n",
      " [-193325.06667757]\n",
      " ...\n",
      " [-137636.55433923]\n",
      " [-176881.91442668]\n",
      " [-194962.17593058]]\n",
      "Iter: 243 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.035496051316039265, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-12093.10520563]\n",
      " [ -2410.57439572]\n",
      " [-41865.5610516 ]\n",
      " ...\n",
      " [ 15264.40904851]\n",
      " [-27869.9714886 ]\n",
      " [-42479.10959335]]\n",
      "Iter: 244 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.018694691668435955, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-482992.94057341]\n",
      " [-473277.04635463]\n",
      " [-511197.58532573]\n",
      " ...\n",
      " [-459487.5600875 ]\n",
      " [-489815.54275364]\n",
      " [-514987.13388306]]\n",
      "Iter: 245 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02992218984747935, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-175181.16990251]\n",
      " [-165490.4716096 ]\n",
      " [-204403.64223066]\n",
      " ...\n",
      " [-149157.47225725]\n",
      " [-187875.32638215]\n",
      " [-206108.54744476]]\n",
      "Iter: 246 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03616546965046918, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -2401.26179587]\n",
      " [  7226.38601509]\n",
      " [-32040.7255272 ]\n",
      " ...\n",
      " [ 24889.99973281]\n",
      " [-18294.84452725]\n",
      " [-32578.32296536]]\n",
      "Iter: 247 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.014494633588756049, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-609351.26331752]\n",
      " [-599629.38893162]\n",
      " [-637126.41404532]\n",
      " ...\n",
      " [-586884.47300941]\n",
      " [-613776.29421237]\n",
      " [-641764.56683849]]\n",
      "Iter: 248 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02577069061071158, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-300173.12557379]\n",
      " [-290472.89710619]\n",
      " [-328981.71162366]\n",
      " ...\n",
      " [-275167.02061244]\n",
      " [-310500.43795933]\n",
      " [-331526.65356592]]\n",
      "Iter: 249 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.036321833477764416, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-10927.06589148]\n",
      " [ -1273.65473366]\n",
      " [-40621.42916581]\n",
      " ...\n",
      " [ 16382.0547099 ]\n",
      " [-26739.21906301]\n",
      " [-41204.21832773]]\n",
      "Iter: 250 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.019038797478004243, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-495291.56393086]\n",
      " [-485599.13697895]\n",
      " [-523387.67491261]\n",
      " ...\n",
      " [-471931.78495121]\n",
      " [-501900.15184696]\n",
      " [-527237.89286255]]\n",
      "Iter: 251 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03026976414582707, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-187389.01762961]\n",
      " [-177721.66789172]\n",
      " [-216503.64658706]\n",
      " ...\n",
      " [-161509.81922568]\n",
      " [-199871.10229652]\n",
      " [-218268.61241813]]\n",
      "Iter: 252 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03725183215617798, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[  5363.04929295]\n",
      " [ 14966.04179509]\n",
      " [-24236.51523219]\n",
      " ...\n",
      " [ 32676.29727547]\n",
      " [-10704.93526771]\n",
      " [-24697.18710213]]\n",
      "Iter: 253 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.010407853438359363, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-746137.03639518]\n",
      " [-736367.32304789]\n",
      " [-773581.08551914]\n",
      " ...\n",
      " [-724660.30040435]\n",
      " [-748075.0216109 ]\n",
      " [-779126.10315556]]\n",
      "Iter: 254 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.021711658370550408, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-436166.8701684 ]\n",
      " [-426415.96428597]\n",
      " [-464655.7526148 ]\n",
      " ...\n",
      " [-412136.57893258]\n",
      " [-444025.89335172]\n",
      " [-468102.75704048]]\n",
      "Iter: 255 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03290447166179941, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-129319.59174212]\n",
      " [-119595.86009382]\n",
      " [-158816.87664921]\n",
      " ...\n",
      " [-102784.41176336]\n",
      " [-143029.09634537]\n",
      " [-160185.40328773]]\n",
      "Iter: 256 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03655679034638994, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-27899.78704796]\n",
      " [-18244.73467575]\n",
      " [-57542.23233132]\n",
      " ...\n",
      " [  -714.16049265]\n",
      " [-43427.20741471]\n",
      " [-58226.59298692]]\n",
      "Iter: 257 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02607847245283, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-322152.03478922]\n",
      " [-312528.84154557]\n",
      " [-350653.25496884]\n",
      " ...\n",
      " [-297519.21623313]\n",
      " [-332007.15803424]\n",
      " [-353315.82618725]]\n",
      "Iter: 258 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.036858535579325793, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-26621.64764391]\n",
      " [-17039.47093645]\n",
      " [-56048.73583902]\n",
      " ...\n",
      " [   382.9527629 ]\n",
      " [-42090.46957285]\n",
      " [-56707.44515283]]\n",
      "Iter: 259 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02610263380362843, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-328625.91171371]\n",
      " [-319073.11482796]\n",
      " [-356891.90170431]\n",
      " ...\n",
      " [-304232.15969315]\n",
      " [-338277.92918188]\n",
      " [-359581.31103542]]\n",
      "Iter: 260 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03692778728425692, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-31844.39280596]\n",
      " [-22331.13432364]\n",
      " [-61045.1890575 ]\n",
      " ...\n",
      " [ -5064.34761871]\n",
      " [-47136.0747529 ]\n",
      " [-61722.52869677]]\n",
      "Iter: 261 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02787849495385018, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-286174.22344702]\n",
      " [-276704.92345559]\n",
      " [-314338.17442937]\n",
      " ...\n",
      " [-261646.28143065]\n",
      " [-296538.67077453]\n",
      " [-316723.52968128]]\n",
      "Iter: 262 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.038311709202402094, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[  -103.7152254 ]\n",
      " [  9317.19175864]\n",
      " [-29137.17545808]\n",
      " ...\n",
      " [ 26695.87448719]\n",
      " [-15889.04841558]\n",
      " [-29581.90892024]]\n",
      "Iter: 263 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.015301862127882789, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-644489.26661641]\n",
      " [-634956.0184393 ]\n",
      " [-671580.98010286]\n",
      " ...\n",
      " [-622778.0325139 ]\n",
      " [-648115.93163237]\n",
      " [-676380.93323547]]\n",
      "Iter: 264 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.026591793104956084, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-334920.11954522]\n",
      " [-325407.23259324]\n",
      " [-363050.54743914]\n",
      " ...\n",
      " [-310662.83968342]\n",
      " [-344458.20872054]\n",
      " [-365754.88595371]]\n",
      "Iter: 265 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03747244240605559, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-36616.61726009]\n",
      " [-27141.63560212]\n",
      " [-65692.24500943]\n",
      " ...\n",
      " [ -9955.77855664]\n",
      " [-51825.8802775 ]\n",
      " [-66374.451993  ]]\n",
      "Iter: 266 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.029748117920096358, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-253871.61785237]\n",
      " [-244450.84408536]\n",
      " [-282010.36418771]\n",
      " ...\n",
      " [-229180.94112669]\n",
      " [-264846.86713039]\n",
      " [-284149.83174268]]\n",
      "Iter: 267 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03963331113811067, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ 17370.91539287]\n",
      " [ 26736.85458278]\n",
      " [-11564.41523067]\n",
      " ...\n",
      " [ 44189.60338887]\n",
      " [  1271.56711863]\n",
      " [-11863.72194629]]\n",
      "Iter: 268 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0018930495383603805, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 269 Loss: 103047.8075941864\n",
      "In model, X: (28831, 35), b: 0.013214196914448622, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-728112.45414098]\n",
      " [-718423.9862378 ]\n",
      " [-755391.7629475 ]\n",
      " ...\n",
      " [-706645.47514237]\n",
      " [-730434.58946482]\n",
      " [-760750.28260083]]\n",
      "Iter: 270 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.024518001846639666, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-418142.2879142 ]\n",
      " [-408472.62747588]\n",
      " [-446466.43004316]\n",
      " ...\n",
      " [-394121.7536706 ]\n",
      " [-426385.46120564]\n",
      " [-449726.93648575]]\n",
      "Iter: 271 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03567959873887371, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-112154.76420132]\n",
      " [-102513.66883037]\n",
      " [-141480.01294321]\n",
      " ...\n",
      " [ -85640.35504095]\n",
      " [-126230.01945941]\n",
      " [-142667.63931993]]\n",
      "Iter: 272 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0383156502112472, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-39283.56952597]\n",
      " [-29718.42345195]\n",
      " [-68631.09578441]\n",
      " ...\n",
      " [-12376.45289124]\n",
      " [-54623.67219699]\n",
      " [-69324.78977006]]\n",
      "Iter: 273 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03125381247847329, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-238017.96683525]\n",
      " [-228511.69800485]\n",
      " [-266479.94763131]\n",
      " ...\n",
      " [-212938.94302629]\n",
      " [-249470.91856159]\n",
      " [-268505.72244943]]\n",
      "Iter: 274 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.040667291025870106, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[20422.00292294]\n",
      " [29868.93477893]\n",
      " [-8776.8034917 ]\n",
      " ...\n",
      " [47508.03188355]\n",
      " [ 4099.32963447]\n",
      " [-9049.17570038]]\n",
      "Iter: 275 Loss: nan\n",
      "In model, X: (28831, 35), b: -0.0002856716599432736, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "Iter: 276 Loss: 112500.65109922666\n",
      "In model, X: (28831, 35), b: 0.01103547571614497, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-814904.48565365]\n",
      " [-805080.00212367]\n",
      " [-842296.29703791]\n",
      " ...\n",
      " [-793772.61551776]\n",
      " [-815808.27884844]\n",
      " [-848241.27693628]]\n",
      "Iter: 277 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.0223462176258949, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-504733.01346821]\n",
      " [-494926.47435471]\n",
      " [-533172.9835071 ]\n",
      " ...\n",
      " [-481043.59424211]\n",
      " [-511562.77693472]\n",
      " [-537018.76140462]]\n",
      "Iter: 278 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03358065278249717, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-196730.53969036]\n",
      " [-186948.8142962 ]\n",
      " [-226190.17843801]\n",
      " ...\n",
      " [-170520.16082006]\n",
      " [-209436.03879604]\n",
      " [-227950.08302011]]\n",
      "Iter: 279 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.040528035807713074, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[ -4511.01121417]\n",
      " [  5213.50871897]\n",
      " [-34474.66380618]\n",
      " ...\n",
      " [ 23146.41898089]\n",
      " [-20797.35988456]\n",
      " [-34936.59849948]]\n",
      "Iter: 280 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.020033069368830425, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-578629.00563852]\n",
      " [-568825.65905853]\n",
      " [-606797.77486961]\n",
      " ...\n",
      " [-555563.28911441]\n",
      " [-584052.65140056]\n",
      " [-611137.01341326]]\n",
      "Iter: 281 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03130565790200652, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-269548.84428446]\n",
      " [-259767.46864219]\n",
      " [-298749.72121232]\n",
      " ...\n",
      " [-243945.49966586]\n",
      " [-280872.49410015]\n",
      " [-300996.34449404]]\n",
      "Iter: 282 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.041277563142893076, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[  4059.42820301]\n",
      " [ 13787.22347554]\n",
      " [-25950.31988575]\n",
      " ...\n",
      " [ 31814.21486563]\n",
      " [-12435.73763726]\n",
      " [-26340.87463885]]\n",
      "Iter: 283 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.015461194394573335, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-718775.60215943]\n",
      " [-708895.93234608]\n",
      " [-746686.1741472 ]\n",
      " ...\n",
      " [-696657.8941594 ]\n",
      " [-721667.9243374 ]\n",
      " [-751965.90355135]]\n",
      "Iter: 284 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.026764999326764377, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-408805.43593265]\n",
      " [-398944.57358417]\n",
      " [-437760.84124286]\n",
      " ...\n",
      " [-384134.17268763]\n",
      " [-417618.79607822]\n",
      " [-440942.55743627]]\n",
      "Iter: 285 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.037867631909747956, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-104447.22404517]\n",
      " [ -94617.28709013]\n",
      " [-134390.74557121]\n",
      " ...\n",
      " [ -77302.22918118]\n",
      " [-119058.11628667]\n",
      " [-135510.25240213]]\n",
      "Iter: 286 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03975451527150202, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-52597.96082653]\n",
      " [-42847.64016681]\n",
      " [-82478.37760236]\n",
      " ...\n",
      " [-25245.52588905]\n",
      " [-68068.59409048]\n",
      " [-83244.34667532]]\n",
      "Iter: 287 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03531138114507593, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-178084.37944971]\n",
      " [-168408.10888899]\n",
      " [-207292.11336003]\n",
      " ...\n",
      " [-151989.42554708]\n",
      " [-191043.84605859]\n",
      " [-208895.72298184]]\n",
      "Iter: 288 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.041096820755409616, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-17122.26022899]\n",
      " [ -7497.36583288]\n",
      " [-46738.80601695]\n",
      " ...\n",
      " [ 10168.59341673]\n",
      " [-33060.8768267 ]\n",
      " [-47260.13234264]]\n",
      "Iter: 289 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.02653281119207275, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-425565.3426825 ]\n",
      " [-415932.01293452]\n",
      " [-453759.25443667]\n",
      " ...\n",
      " [-401672.23474094]\n",
      " [-433707.69249604]\n",
      " [-457032.7376881 ]]\n",
      "Iter: 290 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.037711750528204, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-119090.93898204]\n",
      " [-109485.24757742]\n",
      " [-148290.48855582]\n",
      " ...\n",
      " [ -92697.38581852]\n",
      " [-133075.88517364]\n",
      " [-149487.97715906]]\n",
      "Iter: 291 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.040826453452048335, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-32770.51656174]\n",
      " [-23237.72150639]\n",
      " [-62049.08987544]\n",
      " ...\n",
      " [ -5863.84247569]\n",
      " [-48280.38724819]\n",
      " [-62662.95857307]]\n",
      "Iter: 292 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.032061574719052285, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-279138.57926131]\n",
      " [-269652.04357482]\n",
      " [-307401.47132823]\n",
      " ...\n",
      " [-254423.72909497]\n",
      " [-289869.9588109 ]\n",
      " [-309669.48654782]]\n",
      "Iter: 293 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.042304022084735056, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[  1831.48417669]\n",
      " [ 11268.39481084]\n",
      " [-27278.67893473]\n",
      " ...\n",
      " [ 28770.90826292]\n",
      " [-14219.25960912]\n",
      " [-27641.05526384]]\n",
      "Iter: 294 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.01807799524584112, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-676579.17699433]\n",
      " [-667012.97390903]\n",
      " [-703678.71183702]\n",
      " ...\n",
      " [-654966.63694616]\n",
      " [-679844.61567354]\n",
      " [-708627.86643439]]\n",
      "Iter: 295 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.029374863200473292, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-366809.88095433]\n",
      " [-357263.28021115]\n",
      " [-394951.135381  ]\n",
      " ...\n",
      " [-342647.59071558]\n",
      " [-375991.52116757]\n",
      " [-397803.47491619]]\n",
      "Iter: 296 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.04036997263129428, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-65381.33004272]\n",
      " [-55869.24022525]\n",
      " [-94489.29536734]\n",
      " ...\n",
      " [-38779.83707372]\n",
      " [-80299.0630692 ]\n",
      " [-95298.79096636]]\n",
      "Iter: 297 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.038242229592108996, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-126054.21153962]\n",
      " [-116624.2506674 ]\n",
      " [-154693.97992247]\n",
      " ...\n",
      " [-100190.99750448]\n",
      " [-139685.106545  ]\n",
      " [-155904.62260179]]\n",
      "Iter: 298 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.04191535930739563, w: (35, 1)\n",
      "gradient shape: (35, 1)\n",
      "z: [[-24048.29074805]\n",
      " [-14686.82756733]\n",
      " [-52835.83027732]\n",
      " ...\n",
      " [  2470.74149774]\n",
      " [-39508.49545042]\n",
      " [-53358.40872848]]\n",
      "Iter: 299 Loss: nan\n",
      "In model, X: (28831, 35), b: 0.03047178583342387, w: (35, 1)\n",
      "0.8880024973119212\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = np.random.rand(X_train1.shape[1],1)  # assuming X is N-by-n. \n",
    "                                        # if X is n-by-N, use X_train.shape[0]\n",
    "y_train1 = y_train1.reshape(-1,1)\n",
    "y_test1 = y_test1.reshape(-1,1)\n",
    "print(w1.shape)\n",
    "print(X_train1.shape)\n",
    "print(y_train1.shape)\n",
    "b1 = 0\n",
    "w1, b1, loss1 = train(w1, b1, X_train1, y_train1, iter=300, lr=0.1)\n",
    "plt.figure()\n",
    "plt.plot(loss1)\n",
    "\n",
    "#training accuracy \n",
    "z1 = model(w1,b1,X_train1)\n",
    "print(accuracy(np.squeeze(y_train1), predict(z1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBqSssY6OUGJ",
    "outputId": "cbfac5ce-4ed4-474f-a878-f263cfa845c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In model, X: (12357, 35), b: 0.03047178583342387, w: (35, 1)\n",
      "0.8910738852472283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-289-eebd09f18394>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-t))\n"
     ]
    }
   ],
   "source": [
    "z1 = model(w1,b1,X_test1)\n",
    "y_test1=np.squeeze(y_test1)\n",
    "print(accuracy(y_test1, predict(z1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "fsI9xje4Oase"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
